var relearn_searchindex = [
  {
    "breadcrumb": "Internship Report \u003e Translated Blogs",
    "content": "Một giải pháp có khả năng mở rộng cao cho việc sao chép dữ liệu, sử dụng Amazon FSx for NetApp ONTAP và NetApp SnapMirror.\rTác giả: Gaurav Acharya, Jay Horne – 30/7/2025\nChủ đề: Advanced (300),Amazon FSx for NetApp ONTAP, Technical How-to\nNhững khách hàng on-premises đang sử dụng các mảng lưu trữ NetApp trong trung tâm dữ liệu của riêng họ thường áp dụng các quy tắc kiểm soát truy cập mạng và tường lửa nghiêm ngặt để bảo vệ dữ liệu của mình, tuy nhiên, kiểu bảo mật này thường đưa Network Address Translation (NAT) vào đường truyền giữa các mảng lưu trữ. ONTAP, dù được triển khai tại chỗ hay trên đám mây, đều yêu cầu các cụm lưu trữ được cấu hình với địa chỉ IP tĩnh, và giao thức SnapMirror™ được dùng để sao chép dữ liệu giữa chúng không hỗ trợ NAT. Điều này khiến việc kết nối giữa FSx for NetApp ONTAP và các phiên bản NetApp tại chỗ nằm sau tường lửa có NAT trở nên không thể. Người dùng trong những môi trường như vậy không thể dễ dàng di chuyển dữ liệu giữa hệ thống tại chỗ và Amazon FSx for NetApp ONTAP. Lý tưởng nhất, họ sẽ chọn kết nối thông qua SnapMirror qua internet công cộng, nhưng điều này là không thể trong cấu hình mặc định.\nNetApp SnapMirror™ là một tính năng thường được sử dụng cho khôi phục sau thảm họa (DR), sao lưu và sao chép dữ liệu trong hệ thống lưu trữ NetApp ONTAP, cả tại chỗ lẫn trên đám mây. Amazon FSx for NetApp ONTAP bao gồm SnapMirror như một phần của dịch vụ được quản lý toàn diện trong AWS. Tuy nhiên, vì NetApp SnapMirror không hỗ trợ NAT, và các địa chỉ IP này được xác minh đối chiếu với các hệ thống tệp được ghép cặp, nên cần triển khai một thiết bị NAT dựa trên Amazon Elastic Compute Cloud (Amazon EC2) với các Elastic IPs, để đảm bảo các tiêu đề lớp 3 (L3 headers) trùng khớp giữa mỗi hệ thống tệp và internet.\nTrong bài viết này, chúng tôi thảo luận về một kiến trúc và thiết kế nhằm hợp lý hóa và mở rộng quy mô cho thách thức di chuyển dữ liệu này trong môi trường AWS. Một lựa chọn khác có thể là thiết lập các đường hầm VPN riêng lẻ giữa trung tâm dữ liệu và AWS. Tuy nhiên, cách này sẽ rất khó quản lý khi mở rộng quy mô. Nguyên nhân khiến NetApp SnapMirror không hỗ trợ NAT là vì các siêu dữ liệu được trao đổi trong quá trình ghép cặp — chẳng hạn như các địa chỉ Logical Interface (LIF) — được xác minh đối chiếu với các hệ thống tệp đang được ghép cặp. Nếu chúng không khớp, kết nối sẽ thất bại. Từ thông tin này, ta có thể nói rằng NAT không phá vỡ SnapMirror, mà chính việc thay đổi địa chỉ IP mới là nguyên nhân. Vậy, nếu chúng ta có thể thực hiện NAT theo cách mà SnapMirror vẫn có thể xác minh các địa chỉ IP đó thì sao? Chúng ta chỉ cần đảm bảo rằng các tiêu đề lớp 3 (L3 Headers) trong các gói IP trùng khớp, và cách linh hoạt nhất để làm điều đó là sử dụng một lớp NAT thứ hai.\nTổng quan giải pháp\rĐể làm cho các headers lớp 3 (L3 headers) trùng khớp, chúng ta cần một thiết bị NAT nằm giữa mỗi hệ thống tệp và internet. Thiết bị này có thể là một máy chủ Linux, tường lửa, bộ định tuyến hoặc bất kỳ thiết bị nào trong trung tâm dữ liệu có khả năng thực hiện NAT và đáp ứng đủ băng thông cần thiết. Trong bài viết này, chúng tôi triển khai một phiên bản EC2 được tối ưu hóa cho mạng dựa trên kiến trúc Graviton trong Amazon Virtual Private Cloud (Amazon VPC). Chúng tôi sử dụng phiên bản c7gn.medium, có thông lượng mạng đạt 3.5 GB/s. Kích thước phiên bản này có thể được mở rộng tùy theo nhu cầu băng thông của bạn. Vì yêu cầu về CPU và bộ nhớ là rất nhỏ, lựa chọn này mang lại hiệu suất mạng tốt nhất so với chi phí tại thời điểm viết bài.\nĐiều kiện tiên quyết\rCác điều kiện sau là cần thiết để hoàn thành giải pháp này:\nMột thiết bị NAT mà SnapMirror có thể đi qua tại mỗi hệ thống tệp (filer). Một subnet riêng biệt cho từng hệ thống tệp. Ví dụ\rCấu hình ví dụ được triển khai từ AWS đến AWS để đảm bảo tính trực quan, như minh họa trong hình trên, nhưng một trong hai phía đều có thể được thay thế bằng bất kỳ thiết bị NAT nào thực hiện chức năng tương tự. Tương tự, ví dụ này dựa trên hệ thống tệp FSx for ONTAP trong một Single-Availability Zone (AZ). Nếu bạn đang sử dụng hệ thống tệp Multi-AZ, chúng tôi khuyến nghị triển khai hai phiên bản Amazon EC2 trong từng Availability Zone (AZ) của hệ thống tệp Multi-AZ và định tuyến lưu lượng thông qua phiên bản nằm trong từng AZ đó.\nEIPs\rTrong ví dụ của chúng tôi, chúng tôi sử dụng một địa chỉ EIP cho mỗi giao diện liên cụm (inter-cluster interface) của FSx for ONTAP. Ban đầu, chúng tôi sẽ yêu cầu bốn địa chỉ EIP chưa được gán cho bất kỳ tài nguyên nào. Việc phân bổ cổng có thể cho phép sử dụng ít địa chỉ IP hơn, nhưng điều đó nằm ngoài phạm vi của ví dụ này.\nSecurity Group\rViệc gán trực tiếp các địa chỉ EIP cho Amazon EC2 mà không có bất kỳ giới hạn nào là một thực hành bảo mật kém. Do đó, chúng tôi đã tạo một nhóm bảo mật (security group) trong mỗi VPC và cho phép toàn bộ lưu lượng đến từ bốn địa chỉ EIP này. Về mặt kỹ thuật, chỉ cần mở các cổng TCP 10000, 11104, 11105 và ICMP là đủ, nhưng bộ định tuyến của chúng tôi chỉ chuyển tiếp các cổng này. Phần sau đây tóm tắt cấu hình mạng mẫu cho hai triển khai FSx for ONTAP. Các địa chỉ IP được liệt kê chỉ nhằm mục đích minh họa — các giá trị thực tế trong môi trường của bạn sẽ khác tùy theo cấu hình mạng. Side A: VPC: 10.1.0.0/16FSx ONTAP inter-cluster endpoint 1: 10.1.0.137FSx ONTAP inter-cluster endpoint 2: 10.1.0.125inter_1 EIP: 18.190.143.162inter_2 EIP: 3.128.12.212 Side B: VPC: 10.2.0.0/16FSx ONTAP inter-cluster endpoint 1: 10.2.0.155FSx ONTAP inter-cluster endpoint 2: 10.2.0.110inter_1 EIP: 3.135.134.67inter_2 EIP: 3.146.166.253\nAmazon EC2\rĐể xử lý các NAT, hãy triển khai một phiên bản EC2 chạy RedHat 9, kèm theo nhóm bảo mật mà chúng ta đã tạo trước đó. RedHat không phải là bắt buộc, và bất kỳ bản phân phối Linux nào hỗ trợ nftables đều có thể hoạt động cho bài thực hành này. Đối với mỗi phiên bản EC2, chúng ta cần gắn kết hai địa chỉ EIP trong số các địa chỉ đã tạo. Mỗi địa chỉ EIP này phải được liên kết với một địa chỉ IP riêng (private IP) khác nhau. Cuối cùng, chúng ta phải tắt kiểm tra nguồn/đích (source/destination check) trên giao diện mạng. Điều này cho phép Amazon EC2 gửi các gói tin có địa chỉ IP nguồn không thuộc quyền sở hữu của nó.\nHình 2. Ảnh chụp màn hình của trang tổng quan mạng (network summary page) của một phiên bản EC2, trong đó địa chỉ IP riêng (private IP) và địa chỉ IP công cộng (public IP) được tô sáng (highlighted).\rnftables\rTrên mỗi phiên bản Linux này, chúng ta cần thêm một số quy tắc nftables để xử lý các kết nối. Điều này tạo ra một ánh xạ 1:1 giữa các giao diện của cụm FSx for ONTAP và một địa chỉ EIP. Đối với môi trường ví dụ của chúng ta, cấu hình nftables cho Side B sẽ như sau.\nTùy chọn 1: Chỉnh sửa trực tiếp tệp nftables.\rtable ip nat {\rchain prerouting {\rtype nat hook prerouting priority dstnat; policy accept;\rtcp dport 11104 ip daddr 10.1.0.135 dnat to 10.1.0.125\rtcp dport 11105 ip daddr 10.1.0.135 dnat to 10.1.0.125\rtcp dport 10000 ip daddr 10.1.0.135 dnat to 10.1.0.125\ricmp type { echo-reply, echo-request } ip daddr 10.1.0.135 dnat to 10.1.0.125\rtcp dport 11104 ip daddr 10.1.0.123 dnat to 10.1.0.137\rtcp dport 11105 ip daddr 10.1.0.123 dnat to 10.1.0.137\rtcp dport 10000 ip daddr 10.1.0.123 dnat to 10.1.0.137\ricmp type { echo-reply, echo-request } ip daddr 10.1.0.123 dnat to 10.1.0.137\rip daddr 10.2.0.110 dnat to 3.146.166.253\ricmp type { echo-reply, echo-request } ip daddr 10.2.0.110 dnat to 3.146.166.253\rip daddr 10.2.0.155 dnat to 3.135.134.67\ricmp type { echo-reply, echo-request } ip daddr 10.2.0.155 dnat to 3.135.134.67\r}\rchain postrouting {\rtype nat hook postrouting priority srcnat; policy accept;\rip saddr 10.1.0.125 snat to 10.1.0.135\ricmp type { echo-reply, echo-request } ip saddr 10.1.0.125 snat to 10.1.0.135\rip saddr 10.1.0.137 snat to 10.1.0.123\ricmp type { echo-reply, echo-request } ip saddr 10.1.0.137 snat to 10.1.0.123\rtcp dport 11104 ip saddr 3.146.166.253 snat to 10.2.0.110\rtcp dport 11105 ip saddr 3.146.166.253 snat to 10.2.0.110\rtcp dport 10000 ip saddr 3.146.166.253 snat to 10.2.0.110\ricmp type { echo-reply, echo-request } ip saddr 3.146.166.253 snat to 10.2.0.110\rtcp dport 11104 ip saddr 3.135.134.67 snat to 10.2.0.155\rtcp dport 11105 ip saddr 3.135.134.67 snat to 10.2.0.155\rtcp dport 10000 ip saddr 3.135.134.67 snat to 10.2.0.155\ricmp type { echo-reply, echo-request } ip saddr 3.135.134.67 snat to 10.2.0.155\r}\r}\rTùy chọn 2: Script cấu hình nftables CLI\r#!/bin/bash\r# Install nftables and enable ip forwarding in the kernel\rdnf install -y\recho 1 \u003e /proc/sys/net/ipv4/ip_forward\recho \"net.ipv4.ip_forward = 1\" \u003e\u003e /etc/sysctl.conf\r# Create the pre-routing and postrouting chains in nftables.\rnft add table ip nat\rnft -- add chain ip nat prerouting { type nat hook prerouting priority -100 \\; }\rnft add chain ip nat postrouting { type nat hook postrouting priority 100 \\; }\r# Unmap any incoming packets from the internet to the local fsx interface\r# Map packets destined to 3.146.166.253(10.2.0.70) -\u003e 10.2.0.110\rnft add rule ip nat prerouting tcp dport 11104 ip daddr 10.2.0.70 dnat to 10.2.0.110\rnft add rule ip nat prerouting tcp dport 11105 ip daddr 10.2.0.70 dnat to 10.2.0.110\rnft add rule ip nat prerouting tcp dport 10000 ip daddr 10.2.0.70 dnat to 10.2.0.110\rnft add rule ip nat prerouting icmp type { echo-request, echo-reply } ip daddr 10.2.0.70 dnat to 10.2.0.110\r# Map packets destined to 3.135.134.67(10.2.0.186) -\u003e 10.2.0.155\rnft add rule ip nat prerouting tcp dport 11104 ip daddr 10.2.0.186 dnat to 10.2.0.155\rnft add rule ip nat prerouting tcp dport 11105 ip daddr 10.2.0.186 dnat to 10.2.0.155\rnft add rule ip nat prerouting tcp dport 10000 ip daddr 10.2.0.186 dnat to 10.2.0.155\rnft add rule ip nat prerouting icmp type { echo-request, echo-reply } ip daddr 10.2.0.186 dnat to 10.2.0.155\r# Map any outgoing packets from the local fsx interface to its respective public IP\r# 10.2.0.110 -\u003e 3.146.166.253(10.2.0.70)\rnft add rule ip nat postrouting ip saddr 10.2.0.110 snat to 10.2.0.70\rnft add rule ip nat postrouting icmp type { echo-request, echo-reply } ip saddr 10.2.0.110 snat to 10.2.0.70\r# 10.2.0.155 -\u003e 3.135.134.67(10.2.0.186)\rnft add rule ip nat postrouting ip saddr 10.2.0.155 snat to 10.2.0.186\rnft add rule ip nat postrouting icmp type { echo-request, echo-reply } ip saddr 10.2.0.155 snat to 10.2.0.186\r# Unmap any incoming packets for the remote EIPs to the originating FSX internal IP\r# 3.128.12.212 -\u003e 10.1.0.125\rnft add rule ip nat postrouting tcp dport 11104 ip saddr 3.128.12.212 snat to 10.1.0.125\rnft add rule ip nat postrouting tcp dport 11105 ip saddr 3.128.12.212 snat to 10.1.0.125\rnft add rule ip nat postrouting tcp dport 10000 ip saddr 3.128.12.212 snat to 10.1.0.125\rnft add rule ip nat postrouting icmp type { echo-request, echo-reply } ip saddr 3.128.12.212 snat to 10.1.0.125\r# 18.190.143.162 -\u003e 10.1.0.137\rnft add rule ip nat postrouting tcp dport 11104 ip saddr 18.190.143.162 snat to 10.1.0.137\rnft add rule ip nat postrouting tcp dport 11105 ip saddr 18.190.143.162 snat to 10.1.0.137\rnft add rule ip nat postrouting tcp dport 10000 ip saddr 18.190.143.162 snat to 10.1.0.137\rnft add rule ip nat postrouting icmp type { echo-request, echo-reply } ip saddr 18.190.143.162 snat to 10.1.0.137\r# Map any outgoing packets destined to a remote fsx interface to their respective public IP\r# 10.1.0.125 -\u003e 3.128.12.212\rnft add rule ip nat prerouting ip daddr 10.1.0.125 dnat to 3.128.12.212\rnft add rule ip nat prerouting icmp type { echo-request, echo-reply } ip daddr 10.1.0.125 dnat to 3.128.12.212\r# 10.1.0.137 -\u003e 18.190.143.162\rnft add rule ip nat prerouting ip daddr 10.1.0.137 dnat to 18.190.143.162\rnft add rule ip nat prerouting icmp type { echo-request, echo-reply } ip daddr 10.1.0.137 dnat to 18.190.143.162\r# Persist the config\rnft list ruleset \u003e /etc/sysconfig/nftables.conf\rBảng định tuyến\rKhi cả hai bộ định tuyến đã được cấu hình, chúng ta cần đảm bảo rằng lưu lượng SnapMirror sẽ đi qua chúng. Để thực hiện điều này, chúng ta cập nhật bảng định tuyến (route table) được liên kết với FSx for ONTAP để gửi lưu lượng từ VPC ở xa đến giao diện mạng (network interface) của các phiên bản EC2. Ví dụ, ở Side B, chúng ta thêm một tuyến (route) trỏ 10.1.0.0/16 đến Elastic Network Interface của phiên bản EC2. Ở Side A, chúng ta sẽ làm ngược lại: trỏ 10.2.0.0/16 đến EC2 instance tương ứng.\nHình 3. Bảng định tuyến (Route table) được liên kết với FSx for ONTAP.\rFSx cho ONTAP security group\rLà bước thiết lập cuối cùng, chúng ta cần cho phép mạng VPC từ xa được kết nối đến các giao diện của FSx for ONTAP. Để làm điều này, chúng ta đã thêm dải địa chỉ 10.0.0.0/8 vào nhóm bảo mật (security group) trên cả hai phiên bản FSx for ONTAP.\nKết nối ngang hàng giữa các hệ thống tệp\rKhi các kết nối mạng đã được thiết lập, việc còn lại là kết nối ngang hàng (peer) giữa các hệ thống tệp FSx for ONTAP. Trước tiên, chúng ta đăng nhập vào Side A và bắt đầu yêu cầu peering.\nSau đó, chúng ta đăng nhập vào Side B và thực thi cùng một lệnh, nhưng không sử dụng tùy chọn generate passphrase và dùng các địa chỉ IP từ Side A. Thao tác này được thực hiện từ phía Side B của hệ thống tệp FSx for ONTAP.\nTừ đây, các SVM (Storage Virtual Machine) có thể được kết nối ngang hàng (peered) và một mối quan hệ SnapMirror có thể được tạo ra.\nDọn dẹp\rViệc chạy các phiên bản EC2 và hệ thống tệp FSx for ONTAP sẽ phát sinh chi phí. Hãy nhớ xóa và chấm dứt (terminate) các tài nguyên này nếu chúng không còn cần thiết. Để xóa một hệ thống tệp, hãy làm theo hướng dẫn trong tài liệu hướng dẫn người dùng FSx for NetApp ONTAP. Để chấm dứt các phiên bản EC2, hãy truy cập phần Terminate Your Instance trong tài liệu hướng dẫn người dùng Amazon EC2.\nLink bài viết gốc: (https://aws.amazon.com/blogs/storage/highly-scalable-solution-design-to-replicate-data-using-amazon-fsx-for-netapp-ontap-and-snapmirror/)",
    "description": "Một giải pháp có khả năng mở rộng cao cho việc sao chép dữ liệu, sử dụng Amazon FSx for NetApp ONTAP và NetApp SnapMirror.\rTác giả: Gaurav Acharya, Jay Horne – 30/7/2025\nChủ đề: Advanced (300),Amazon FSx for NetApp ONTAP, Technical How-to\nNhững khách hàng on-premises đang sử dụng các mảng lưu trữ NetApp trong trung tâm dữ liệu của riêng họ thường áp dụng các quy tắc kiểm soát truy cập mạng và tường lửa nghiêm ngặt để bảo vệ dữ liệu của mình, tuy nhiên, kiểu bảo mật này thường đưa Network Address Translation (NAT) vào đường truyền giữa các mảng lưu trữ. ONTAP, dù được triển khai tại chỗ hay trên đám mây, đều yêu cầu các cụm lưu trữ được cấu hình với địa chỉ IP tĩnh, và giao thức SnapMirror™ được dùng để sao chép dữ liệu giữa chúng không hỗ trợ NAT. Điều này khiến việc kết nối giữa FSx for NetApp ONTAP và các phiên bản NetApp tại chỗ nằm sau tường lửa có NAT trở nên không thể. Người dùng trong những môi trường như vậy không thể dễ dàng di chuyển dữ liệu giữa hệ thống tại chỗ và Amazon FSx for NetApp ONTAP. Lý tưởng nhất, họ sẽ chọn kết nối thông qua SnapMirror qua internet công cộng, nhưng điều này là không thể trong cấu hình mặc định.",
    "tags": [],
    "title": "Blog 1",
    "uri": "/en/3-translated_blogs/blog_1/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Events Participated",
    "content": "Summary Report: “Kick-off AWS First Cloud Journey Workforce – OJT FALL 2025”\rEvent Objectives\rOfficially kick off the AWS First Cloud Journey Workforce OJT Fall 2025 program Introduce students to the objectives, structure, and learning paths of the program Provide insights into career opportunities in Cloud Computing, AI/ML, DevOps, and Security Connect students with AWS Study Group community and partner companies Inspire students to build practical skills and prepare for a career in cloud technology Speakers \u0026 Key Presenters\rNguyen Gia Hung – Head of Solutions Architect, AWS Vietnam Shared insights on AWS First Cloud Journey and future career opportunities in cloud computing\nDo Huy Thang – DevOps Lead, VNG Shared experiences in DevOps and career guidance\nDanh Hoang Hieu Nghi – GenAI Engineer, Renova Discussed the pathway from First Cloud Journey to becoming a GenAI Engineer\nBui Ho Linh Nhi – AI Engineer, SoftwareOne Shared her experience as a female engineer in tech and journey with First Cloud Journey\nPham Nguyen Hai Anh – Cloud Engineer, G-Asia Pacific Shared a day in the life of a Cloud Engineer\nNguyen Dong Thanh Hiep – Principal Cloud Engineer, G-Asia Pacific Shared his journey joining the First Cloud Journey program\nKey Highlights\rProgram overview: history, impact, and alumni success stories Key objectives: build high-quality AWS Builders for Vietnam and equip students with practical skills in Cloud, DevOps, AI/ML, Security, Data \u0026 Analytics Networking sessions and alumni sharing Career guidance and insights into cloud-related roles Inspiration for students to develop technical skills and prepare for real-world cloud careers Event Experience\rAttended hands-on sessions and keynote presentations from industry experts Learned about the AWS First Cloud Journey structure, opportunities, and community Gained insights into career paths, practical skills, and networking strategies in the cloud industry Motivated to develop skills for roles such as Cloud Engineer, DevOps Engineer, and AI/ML Engineer Participating in the Kick-off AWS First Cloud Journey Workforce – OJT FALL 2025 event provided a comprehensive introduction to the program, real-world career insights, and opportunities to connect with AWS experts and peers, setting a strong foundation for future growth in cloud technology.",
    "description": "Summary Report: “Kick-off AWS First Cloud Journey Workforce – OJT FALL 2025”\rEvent Objectives\rOfficially kick off the AWS First Cloud Journey Workforce OJT Fall 2025 program Introduce students to the objectives, structure, and learning paths of the program Provide insights into career opportunities in Cloud Computing, AI/ML, DevOps, and Security Connect students with AWS Study Group community and partner companies Inspire students to build practical skills and prepare for a career in cloud technology Speakers \u0026 Key Presenters\rNguyen Gia Hung – Head of Solutions Architect, AWS Vietnam Shared insights on AWS First Cloud Journey and future career opportunities in cloud computing",
    "tags": [],
    "title": "Event 1",
    "uri": "/en/4-events_participated/4.1-event-1/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Workshop \u003e Deploy Flow",
    "content": "Architectural model\rDomain: Route 53 (DNS) + ACM (SSL Certificate).\nCDN: CloudFront (Global Edge Network).\nStorage (Primary): S3 Singapore (ap-southeast-1).\nStorage (Failover): S3 N. Virginia (us-east-1).\nReplication: Automatically copy code from Singapore -\u003e Virginia.\nSecurity: OAC (Origin Access Control) - Private Bucket.\nTable of Contents\rPrerequisites\nS3 and Replication\nRoute 53 and ACM\nClouFront and Failover\nS3 Policy\nDNS Record\nDeploy and Test",
    "description": "Architectural model\rDomain: Route 53 (DNS) + ACM (SSL Certificate).\nCDN: CloudFront (Global Edge Network).\nStorage (Primary): S3 Singapore (ap-southeast-1).\nStorage (Failover): S3 N. Virginia (us-east-1).\nReplication: Automatically copy code from Singapore -\u003e Virginia.\nSecurity: OAC (Origin Access Control) - Private Bucket.\nTable of Contents\rPrerequisites",
    "tags": [],
    "title": "Frontend Deploy",
    "uri": "/en/5-workshop/5.3-deploy_flow/5.3.1-frontend-deploy/index.html"
  },
  {
    "breadcrumb": "",
    "content": "Student Information\rFull Name: Dương Bình Minh\nPhone Number: 0355578938\nEmail: duongbinhminh10032004@gmail.com\nUniversity: Sai Gon University\nMajor: Information Technology\nClass: DCT1226\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 29/09/2025 to 22/11/2025\nReport Content\rWorklog Proposal Translated Blogs Events Participated Workshop Self-awareness Sharing and Feedback",
    "description": "Student Information\rFull Name: Dương Bình Minh\nPhone Number: 0355578938\nEmail: duongbinhminh10032004@gmail.com\nUniversity: Sai Gon University\nMajor: Information Technology\nClass: DCT1226\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 29/09/2025 to 22/11/2025",
    "tags": [],
    "title": "Internship Report",
    "uri": "/en/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Workshop \u003e Deploy Flow \u003e Backend Deploy",
    "content": "This phase establishes the foundational network infrastructure and security boundaries for the microservices deployment.\nVPC and Subnet Configuration\rStep 1: Create VPC Infrastructure\nNavigate to VPC Console → Create VPC Select VPC and more option Configure VPC parameters: Parameter Value Rationale Name tag auto-generation SGU-Microservices Naming convention IPv4 CIDR block 10.0.0.0/16 Standard private network range Number of Availability Zones 2 (ap-southeast-1a, ap-southeast-1b) High availability across AZs Number of public subnets 2 For internet-facing resources Number of private subnets 2 For data layer isolation NAT gateways None Cost optimization (~$30/month savings) VPC endpoints None Cost optimization DNS options Enable DNS hostnames + Enable DNS resolution Required for service discovery Click Create VPC Network Architecture Result:\nVPC: 10.0.0.0/16\r├── Public Subnet 1: 10.0.0.0/20 (ap-southeast-1a)\r├── Public Subnet 2: 10.0.16.0/20 (ap-southeast-1b)\r├── Private Subnet 1: 10.0.128.0/20 (ap-southeast-1a)\r└── Private Subnet 2: 10.0.144.0/20 (ap-southeast-1b)\rSecurity Groups Configuration\rSecurity groups act as virtual firewalls controlling inbound and outbound traffic for AWS resources.\nStep 2: Create Security Groups\nNavigate to VPC → Security Groups → Create security group. Create four security groups with the following specifications:\nSecurity Group 1: public-alb-sg (Application Load Balancer)\rParameter Value Name public-alb-sg Description Security group for SGUTODOLIST ALB VPC SGU-Microservices-VPC Inbound Rules:\nType Protocol Port Source Purpose HTTPS TCP 443 0.0.0.0/0 Public HTTPS access HTTP TCP 80 0.0.0.0/0 Public HTTP access (redirect to HTTPS) Security Group 2: ecs-app-sg (ECS Application Containers)\rParameter Value Name ecs-app-sg Description Security group for SGUTODOLIST Service Container VPC SGU-Microservices-VPC Inbound Rules - Phase 1 (ALB to Services):\nType Protocol Port Source Purpose Custom TCP TCP 8080 public-alb-sg ALB to API Gateway Custom TCP TCP 8081 public-alb-sg ALB to User Service Custom TCP TCP 8082 public-alb-sg ALB to Taskflow Service Custom TCP TCP 9998 public-alb-sg ALB to Notification Service Custom TCP TCP 9999 public-alb-sg ALB to Auth Service Custom TCP TCP 9092 public-alb-sg Services call to Kafka Custom TCP TCP 9997 public-alb-sg ALB to AI Service Important: Click Create security group before proceeding to Phase 2.\nInbound Rules - Phase 2 (Inter-service Communication):\nSelect the newly created ecs-app-sg Edit inbound rules → Add rule Configure self-referencing rule: Type: All TCP Port range: 0-65535 Source: Select ecs-app-sg (self-reference) Purpose: Allow containers to communicate with each other Security Group 3: private-db-sg (Data Layer)\rParameter Value Name private-db-sg Description Security group for SGUTODOLIST RDS \u0026 Redis \u0026 Kafka VPC SGU-Microservices-VPC Inbound Rules:\nType Protocol Port Source Purpose MySQL/Aurora TCP 3306 ecs-app-sg RDS database access Custom TCP TCP 6379 ecs-app-sg Redis cache access Custom TCP TCP 9092 ecs-app-sg Kafka broker access MySQL/Aurora TCP 3306 bastion-sg Access Database from the Bastion Host (Admin/Debug) Custom TCP TCP 6379 bastion-sg Access Redis from the Bastion Host (Admin/Debug) MySQL/Aurora TCP 3306 14.186.212.182/32 Direct Access from a fixed IP address (Personal/Debug) Security Group 4: bastion-sg (SSH Jump Host)\rParameter Value Name bastion-sg Description Security group for SGUTODOLIST bastion VPC SGU-Microservices-VPC Inbound Rules:\nType Protocol Port Source Purpose SSH TCP 22 My IP Secure shell access from operator’s IP Security Note: Replace “My IP” with your actual public IP address for enhanced security.\nNetwork Validation Checklist\rBefore proceeding to the next phase, verify:\nVPC created with CIDR 10.0.0.0/16 2 Public subnets across different AZs 2 Private subnets across different AZs DNS hostnames and resolution enabled All 4 security groups created with correct rules ecs-app-sg includes self-referencing rule STEP 2: Infrastructure \u0026 ALB Setup ➡",
    "description": "This phase establishes the foundational network infrastructure and security boundaries for the microservices deployment.\nVPC and Subnet Configuration\rStep 1: Create VPC Infrastructure\nNavigate to VPC Console → Create VPC Select VPC and more option Configure VPC parameters: Parameter Value Rationale Name tag auto-generation SGU-Microservices Naming convention IPv4 CIDR block 10.0.0.0/16 Standard private network range Number of Availability Zones 2 (ap-southeast-1a, ap-southeast-1b) High availability across AZs Number of public subnets 2 For internet-facing resources Number of private subnets 2 For data layer isolation NAT gateways None Cost optimization (~$30/month savings) VPC endpoints None Cost optimization DNS options Enable DNS hostnames + Enable DNS resolution Required for service discovery Click Create VPC Network Architecture Result:",
    "tags": [],
    "title": "Network \u0026 Security Preparation (VPC, SG)",
    "uri": "/en/5-workshop/5.3-deploy_flow/5.3.2-backend-deploy/5.3.2.1-network--security-preparation-vpc-sg/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Workshop \u003e Deploy Flow \u003e Frontend Deploy",
    "content": "Preparation Requirements\rA registered domain name (e.g. sgutodolist.com)\nWith AWS Free Tier accounts, Route 53 domain registration is not supported.\nTherefore, in this project, the domain is registered with a third-party domain provider and then hosted on Amazon Route 53. An AWS account\nA successfully built ReactJS project\nAWS CLI (optional, for testing and validation)\nNode.js and npm installed on the local machine\nRequired Knowledge\rBasic understanding of Amazon S3, CloudFront, and Route 53\nFamiliarity with using the AWS Management Console\nAbility to build a ReactJS project\nSTEP 2: S3 and Replication ➡",
    "description": "Preparation Requirements\rA registered domain name (e.g. sgutodolist.com)\nWith AWS Free Tier accounts, Route 53 domain registration is not supported.\nTherefore, in this project, the domain is registered with a third-party domain provider and then hosted on Amazon Route 53. An AWS account\nA successfully built ReactJS project\nAWS CLI (optional, for testing and validation)",
    "tags": [],
    "title": "Prerequisites",
    "uri": "/en/5-workshop/5.3-deploy_flow/5.3.1-frontend-deploy/5.3.1.1-prerequisites/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Worklog",
    "content": "Week 1 Objectives\rLearn the regulations Connect with other FCJ members Form team and decide team project Understand and practice basic AWS services Tasks to be carried out this week\rDay Task Start Date Completion Date Reference Material 1 - Get acquainted with FCJ members 08/09/2025 08/09/2025 - Read and take note of internship unit rules and regulations Rules and regulations - Learn and practice making a static website with Hugo Hugo quickstart, Learn Hugo Theme - Create AWS account. Practice creating Users group, Users (IAM user) Create AWS account; Create user, group user 2 - Learn and practice creating Budget (Template \u0026 Custom: Cost, Usage, Saving plans, Reservation) 09/09/2025 09/09/2025 Create budget - Update worklog Sample worklog 3 - Learn about support packages, types of support request, how to create a support 10/09/2025 10/09/2025 Support packages 4 - Update worklog (UI) 11/09/2025 11/09/2025 5 - Learn theory about VPC (Subnet, Route Table, Internet Gateway, NAT Gateway) 12/09/2025 12/09/2025 AWS Virtual Private Cloud - Update worklog",
    "description": "Week 1 Objectives\rLearn the regulations Connect with other FCJ members Form team and decide team project Understand and practice basic AWS services Tasks to be carried out this week\rDay Task Start Date Completion Date Reference Material 1 - Get acquainted with FCJ members 08/09/2025 08/09/2025 - Read and take note of internship unit rules and regulations Rules and regulations - Learn and practice making a static website with Hugo Hugo quickstart, Learn Hugo Theme - Create AWS account. Practice creating Users group, Users (IAM user) Create AWS account; Create user, group user 2 - Learn and practice creating Budget (Template \u0026 Custom: Cost, Usage, Saving plans, Reservation) 09/09/2025 09/09/2025 Create budget - Update worklog Sample worklog 3 - Learn about support packages, types of support request, how to create a support 10/09/2025 10/09/2025 Support packages 4 - Update worklog (UI) 11/09/2025 11/09/2025 5 - Learn theory about VPC (Subnet, Route Table, Internet Gateway, NAT Gateway) 12/09/2025 12/09/2025 AWS Virtual Private Cloud - Update worklog",
    "tags": [],
    "title": "Week 1 Worklog",
    "uri": "/en/1-worklog/1.1-week_1/index.html"
  },
  {
    "breadcrumb": "Internship Report",
    "content": "Week 1: Orientation, team formation, and building a foundational understanding of AWS services and tools required for the program\nWeek 2: Explore and practice core VPC and EC2 concepts through hands-on configuration, foundational networking setup, and initial project planning activities.\nWeek 3: Study and apply Route 53 and CloudFormation concepts while progressing solution architecture design and setting up foundational backend, frontend, and database components.\nWeek 4: Develop backend authentication and frontend foundations while exploring S3 and API Gateway services, defining required APIs, translating technical blogs, and establishing core project architecture and UI structure.\nWeek 5: Continue full-stack development while exploring Auto Scaling, CloudShell, and improving frontend–backend integration alongside technical blog translation.\nWeek 6: Extend application development by integrating caching, databases, messaging, and real-time communication features.\nWeek 7: Deploy and scale system components on AWS while integrating load balancing and advancing core business flows.\nWeek 8: Enhance system quality through testing, performance optimization, accessibility improvements, and deployment experimentation.\nWeek 9: Refine authentication and notification flows by integrating OAuth, stabilizing WebSocket communication, and fixing architectural and logic issues.\nWeek 10: Perform internal user acceptance testing, resolve UI and backend issues, and refine system stability based on feedback.\nWeek 11: Identify and resolve remaining defects while finalizing and validating the overall solution architecture.\nWeek 12: Adjust deployment architecture to cost-optimized single-region design and complete infrastructure deployment experiments.\nWeek 13: Implement full cloud infrastructure and containerized deployment using AWS networking, ALB, ECS Fargate, and managed databases.\nWeek 14: Finalize system deployment, perform validation testing, and complete documentation and workshop deliverables.",
    "description": "Week 1: Orientation, team formation, and building a foundational understanding of AWS services and tools required for the program\nWeek 2: Explore and practice core VPC and EC2 concepts through hands-on configuration, foundational networking setup, and initial project planning activities.\nWeek 3: Study and apply Route 53 and CloudFormation concepts while progressing solution architecture design and setting up foundational backend, frontend, and database components.\nWeek 4: Develop backend authentication and frontend foundations while exploring S3 and API Gateway services, defining required APIs, translating technical blogs, and establishing core project architecture and UI structure.",
    "tags": [],
    "title": "Worklog",
    "uri": "/en/1-worklog/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Workshop",
    "content": "Project Introduction\rSGU TodoList is a task management application built using a Microservices architecture on the AWS Cloud platform. The project was initially designed to be deployed as a Multi-Region SaaS model to ensure high availability and disaster recovery. However, due to budget constraints and AWS Free Tier limitations, the team optimized the architecture for a Single-Region Deployment with a Cross-Region Failover mechanism for the frontend.\nOverall architecture\rBackend Architecture (Single-Region: Singapore)\r┌──────────────────────────────────────────────────────┐\r│ Internet Users │\r└───────────────────────┬──────────────────────────────┘\r│\r▼\r┌────────────────┐\r│ Route 53 │ ◄── DNS Management\r│ (Global DNS) │\r└───────┬────────┘\r│\r▼\r┌──────────────────────┐\r│ Application Load │\r│ Balancer (ALB) │ ◄── SSL/TLS Termination\r│ + ACM Certificate │ (sgutodolist.com)\r└──────────┬───────────┘\r│\r┌──────────────┼──────────────┐\r│ │ │\r▼ ▼ ▼\r┌─────────┐ ┌─────────┐ ┌──────────┐\r│ Auth │ │ User │ │Taskflow │ ◄── ECS Fargate\r│ Service │ │ Service │ │ Service │ Microservices\r└────┬────┘ └────┬────┘ └────┬─────┘ (Public Subnets)\r│ │ │\r└────────────┼─────────────┘\r│\r┌────────┴────────┐\r│ API Gateway │ ◄── Central Entry Point\r│ (Port 8080) │ + CORS + Rate Limiting\r└────────┬────────┘\r│\r┌───────────┼─────────────┐\r│ │ │\r▼ ▼ ▼\r┌────────────┐ ┌─────────┐ ┌──────────┐\r│Notification│ │ Kafka │ │ AI Model │\r│ Service │ │ Cluster │ │ Service │\r└──────┬─────┘ └────┬────┘ └────┬─────┘\r│ │ │\r└────────────┼────────────┘\r│\r┌──────────┴──────────┐\r│ │\r▼ ▼\r┌─────────┐ ┌───────────┐\r│ RDS │ │ Redis │ ◄── Data Layer\r│ MySQL │ │ElastiCache│ (Private Subnets)\r└─────────┘ └───────────┘\rFrontend Architecture (Multi-Region Failover)\r┌─────────────────────────────────────────────────────────┐\r│ Internet Users │\r└──────────────────────────┬──────────────────────────────┘\r│\r▼\r┌──────────────┐\r│ Route 53 │ ◄── DNS: sgutodolist.com\r└────────┬─────┘\r│\r▼\r┌────────────────┐\r│ CloudFront │ ◄── Global CDN\r│ Distribution │ SSL Certificate\r└────────┬───────┘ Custom Domain\r│\r┌─────────────┴─────────────┐\r│ Origin Group (HA) │\r│ ┌─────────────────┐ │\r│ │ Primary Origin │ │\r│ │ S3 Singapore │ ◄───┼── Main Region\r│ └─────────────────┘ │\r│ │ │\r│ ┌───────▼───────────┐ │\r│ │ Failover Origin │ │\r│ │ S3 N.Virginia │ ◄─┼── Backup Region\r│ └───────────────────┘ │\r└───────────────────────────┘\r│\r┌──────────┴──────────┐\r│ S3 Cross-Region │\r│ Replication (CRR) │ ◄── Auto Sync\r└─────────────────────┘\rMain components\r1. Backend Services (ECS Fargate)\rService Port Chức năng Dependencies API Gateway 8080 - Điều phối routing\n- CORS handling\n- Rate limiting Redis, All Services Auth Service 9999 - Xác thực JWT\n- OAuth2 (Google)\n- Token management RDS, Redis, Kafka User Service 8081 - Quản lý user profile\n- User CRUD RDS, Redis, Kafka Taskflow Service 8082 - Quản lý tasks\n- Workflows RDS, Redis, Kafka Notification Service 9998 - WebSocket real-time\n- Push notifications RDS, Redis, Kafka AI Model Service 9997 - Task prioritization\n- Smart suggestions Python Flask 2. Infrastructure Components\rNetworking\rVPC: 10.0.0.0/16 at Singapore (ap-southeast-1) Public Subnets (2 AZs): ECS Tasks, ALB, Bastion Private Subnets (2 AZs): RDS, Redis (data) Security Strategy: Eliminated NAT to save cost (~$30/month) Data Storage\rRDS MySQL: Primary database, Free Tier (db.t3.micro) ElastiCache Redis: Caching + Rate limiting (cache.t3.micro) Kafka (ECS): Event streaming, Service Discovery (kafka.sgu.local) Load Balancing \u0026 SSL\rApplication Load Balancer: HTTPS termination, path-based routing ACM Certificate: sgutodolist.com Target Groups: Each service has their onw target group with health checks Service Discovery\rAWS Cloud Map: Namespace sgu.local Internal DNS: auth.sgu.local:9999 user.sgu.local:8081 taskflow.sgu.local:8082 notification.sgu.local:9998 ai-model.sgu.local:9997 kafka.sgu.local:9092 Cost \u0026 Optimization\rImportant Architectural Decisions\rFactors Initial Choice Final Choice Savings NAT Gateway 2 NAT (HA) ❌ Not used ~$60/month ECS Compute EC2 Instances ✅ Fargate Spot (0.5 vCPU) ~$40/month RDS Multi-AZ ✅ Single-AZ Free Tier ~$30/month Redis Cluster Mode ✅ Single Node ~$20/month Backend Multi-Region Active-Active ✅ Single Region ~$200/month Frontend HA Multi-Region Active ✅ Failover Only ~$50/month Public Subnet Strategy for ECS\rInstead of using expensive NAT Gateways, all ECS Tasks are deployed on Public Subnets with Public IP enabled. This allows:\n✅ Load Docker images from ECR over the Internet ✅ Outbound connectivity to AWS services ✅ Save 100% on NAT Gateway costs ⚠️ Trade-off: Need to manage Security Groups carefully Domain \u0026 SSL Strategy\rProduction Domains\rFrontend: https://sgutodolist.com (CloudFront + ACM us-east-1) Backend API: https://sgutodolist.com (ALB + ACM ap-southeast-1) SSL Certificates\rCertificate 1 (us-east-1): For CloudFront (Must be in Virginia) Domain: sgutodolist.com, *.sgutodolist.com Certificate 2 (ap-southeast-1): For ALB Singapore Domain: sgutodolist.com Security Architecture\rSecurity Groups Matrix\rSG Name Purpose Inbound Rules public-alb-sg ALB public facing 0.0.0.0/0:443, 0.0.0.0/0:80 ecs-app-sg ECS Tasks ALB:8080-8082, 9998-9999; Self: all ports (inter-service) private-db-sg RDS + Redis + Kafka ecs-app-sg:3306, 6379, 9092 bastion-sg SSH Jump Host My IP:22 Authentication Flow\rUser → CloudFront → API Gateway → Auth Service\r↓\rJWT + Redis Session\r↓\rGoogle OAuth2 (Optional)\rTeam Information\rProject: SGU TodoList - Task Management System Architecture: Single-Region Microservices (Cost-Optimized) Primary Region: Asia Pacific (Singapore) - ap-southeast-1 Failover Region: US East (N. Virginia) - us-east-1 (Frontend only) Deployment Model: AWS Free Tier Optimized",
    "description": "Project Introduction\rSGU TodoList is a task management application built using a Microservices architecture on the AWS Cloud platform. The project was initially designed to be deployed as a Multi-Region SaaS model to ensure high availability and disaster recovery. However, due to budget constraints and AWS Free Tier limitations, the team optimized the architecture for a Single-Region Deployment with a Cross-Region Failover mechanism for the frontend.",
    "tags": [],
    "title": "Workshop Overview",
    "uri": "/en/5-workshop/5.1-workshop_overview/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Workshop \u003e Deploy Flow",
    "content": "Architecture Model\rThis model illustrates the core services used for high-availability, low-latency content delivery and security:\nDomain \u0026 SSL: Route 53 (DNS) is used for domain management, and ACM (SSL Certificate) secures the traffic.\nCDN: CloudFront acts as the Global Edge Network for content delivery.\nStorage (Primary): S3 in Singapore (ap-southeast-1) holds the main static assets.\nStorage (Failover): S3 in N. Virginia (us-east-1) serves as the secondary storage location.\nReplication: Automated replication copies files from Singapore to Virginia for redundancy.\nSecurity: OAC (Origin Access Control) is implemented to keep the S3 buckets private and ensure traffic only flows via CloudFront.\nTable of Contents\rNetwork \u0026 Security Preparation (VPC, SG)\nInfrastructure \u0026 ALB Setup (RDS, Redis, Cloud Map, ALB Routing)\nCode Update \u0026 Image Build (Create new Docker Image)\nTask Definitions Creation (Configure settings, FIX environment variables)\nServices Deployment Completion \u0026 Verification (Route 53, Google Console, Final Test)",
    "description": "Architecture Model\rThis model illustrates the core services used for high-availability, low-latency content delivery and security:\nDomain \u0026 SSL: Route 53 (DNS) is used for domain management, and ACM (SSL Certificate) secures the traffic.\nCDN: CloudFront acts as the Global Edge Network for content delivery.\nStorage (Primary): S3 in Singapore (ap-southeast-1) holds the main static assets.",
    "tags": [],
    "title": "Backend Deploy",
    "uri": "/en/5-workshop/5.3-deploy_flow/5.3.2-backend-deploy/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Translated Blogs",
    "content": "Tăng tốc Đổi mới Hàng không Vũ trụ: High Performance Computing (HPC) trên Amazon Web Services (AWS)\rTác giả: Gabe Kafity – 29/7/2025\nChủ đề: Best Practices, High Performance Computing\nTrong ngành hàng không vũ trụ đang phát triển nhanh chóng ngày nay, khả năng đổi mới nhanh chóng và hiệu quả không chỉ là một lợi thế – mà đó là một điều cần thiết. Khi các công nghệ như UAVs (Unmanned Aerial Vehicle) tự hành, các chùm vệ tinh (satellite constellations), tên lửa tái sử dụng và thực tế tăng cường/ảo (augmented/virtual reality) tiến bộ, khả năng đổi mới nhanh chóng mang lại cho các tổ chức hàng không vũ trụ lợi thế cạnh tranh. High Performance Computing (HPC) rất quan trọng đối với đổi mới hàng không vũ trụ và đã trở thành nền tảng của sự tiến bộ trong ngành hàng không vũ trụ. Bất kể quy mô, tuổi đời hay tốc độ lặp lại (iteration speed) của một tổ chức, Amazon Web Services (AWS) luôn sẵn sàng giúp thúc đẩy các sứ mệnh hàng không vũ trụ của họ tiến lên.\nTrong bài đăng này, chúng ta sẽ khám phá lý do tại sao, cách thức và những gì khách hàng hàng không vũ trụ thường làm với HPC trong AWS.\nHiện Trạng của HPC\rCơ sở hạ tầng HPC on-premises truyền thống thường đòi hỏi đầu tư vốn đáng kể và có thể mất hàng tháng, hoặc thậm chí hàng năm, để mua sắm và triển khai. Sau khi được triển khai, các cluster thường chạy ở mức hoặc gần 100% tỷ lệ sử dụng (utilization). Tỷ lệ sử dụng cơ sở hạ tầng cao này dẫn đến thời gian chờ đợi lâu cho các HPC job mới đi vào hàng đợi (queue). Các nhà khoa học nghiên cứu và kỹ sư phải chờ đợi (thường là hàng tuần) để job của họ đi qua hàng đợi và chạy, trước khi họ có thể phân tích kết quả và lặp lại sự đổi mới của họ. Ngoài ra, chu kỳ khấu hao (depreciation cycle) của cơ sở hạ tầng HPC on-premises thường là 5-8 năm. Điều này có nghĩa là trong khi cơ sở hạ tầng HPC ngày càng tốt hơn mỗi năm, các cluster on-premises bị mắc kẹt với việc sử dụng cơ sở hạ tầng kém hiệu quả hơn cho đến khi đến lúc làm mới phần cứng (hardware refresh), lúc đó chu kỳ cơ sở hạ tầng cũ (legacy infrastructure) lại bắt đầu lại.\nNgược lại, AWS cung cấp cho các tổ chức quyền truy cập tức thì vào các tài nguyên tính toán gần như không giới hạn, cho phép họ tăng tốc đổi mới trong khi kiểm soát chi phí. Việc triển khai diễn ra chỉ trong vài phút và khách hàng chỉ trả tiền cho những gì họ sử dụng. Tận dụng các khả năng của cloud, các HPC cluster trong AWS mở rộng quy mô (scale out) để đáp ứng nhu cầu, xử lý job thành công và thu hẹp quy mô (scale back in) khi hàng đợi trống. Tính đàn hồi (elasticity) này làm giảm đáng kể thời gian chờ đợi cho các kỹ sư và nhà khoa học, trong khi chỉ phải trả tiền cho các tài nguyên khi chúng đang chạy. Ngoài ra, AWS cải thiện cơ sở hạ tầng HPC của chúng tôi với tốc độ của phần mềm. Điều này có nghĩa là thay vì chờ đợi nhiều năm để làm mới phần cứng nhằm hiện đại hóa cơ sở hạ tầng HPC, khách hàng của AWS liên tục có quyền truy cập vào cơ sở hạ tầng HPC mới nhất, hiệu suất/giá cả tốt nhất từ Amazon và các đối tác của chúng tôi (NVIDIA, Intel, AMD, v.v.).\nHình 1: Đối lập giữa việc chạy các HPC workload on-premises (trái) so với trong AWS (phải). Bên trái bị giới hạn bởi dung lượng trung tâm dữ liệu cố định, nơi thời gian chờ đợi trong hàng đợi dài và cơ sở hạ tầng nhanh chóng trở nên lỗi thời. Bên phải có dung lượng đàn hồi (elastic capacity) có thể mở rộng theo nhu cầu, rút ngắn thời gian chờ đợi trong hàng đợi trong khi chạy trên cơ sở hạ tầng hiện đại hơn.\rCác HPC Workload Chủ chốt trong Hàng không Vũ trụ\rComputational Fluid Dynamics (CFD)\nCác tổ chức hàng không vũ trụ đang tận dụng các tài nguyên tính toán mạnh mẽ của AWS để thực hiện các mô phỏng CFD phức tạp nhằm tối ưu hóa thiết kế máy bay và phân tích hệ thống đẩy (propulsion systems). Sử dụng các dịch vụ HPC của AWS, các tổ chức có thể chạy các workload như Siemens STAR-CCM+, Ansys Fluent hoặc mô phỏng OpenFOAM với hàng nghìn core, giảm thời gian mô phỏng từ hàng tuần xuống hàng giờ.\nPhân tích Cấu trúc (Structural Analysis)\nNhu cầu của thiết kế hàng không vũ trụ hiện đại đòi hỏi phân tích cấu trúc chuyên sâu đối với những thứ như độ bền sản phẩm, độ rung và âm học (acoustics). Cho dù đó là thử nghiệm vật liệu composite mới hay thực hiện phân tích độ mỏi (fatigue analysis) trên các thành phần quan trọng, khả năng HPC của AWS cho phép khách hàng chạy nhiều mô phỏng đồng thời bằng cách sử dụng phần mềm như Dassault Systèmes Abaqus hoặc Simcenter Nastran, đẩy nhanh quá trình lặp lại thiết kế (design iteration process).\nLập kế hoạch Sứ mệnh và Hoạt động Không gian (Mission Planning and Space Operations)\nKhi ngành hàng không vũ trụ phát triển và đổi mới, các tổ chức đang sử dụng các dịch vụ HPC của AWS để mô phỏng cơ học quỹ đạo phức tạp (orbital mechanics), tối ưu hóa việc triển khai các chùm vệ tinh (satellite constellation deployments) và quản lý các cửa sổ phóng (launch windows) một cách hiệu quả. Các mô phỏng này đòi hỏi số lượng lớn các compute cluster, cơ sở hạ tầng mạng và lưu trữ thế hệ tiếp theo, có thể dễ dàng triển khai và tự động mở rộng quy mô dựa trên nhu cầu.\nHình 2: Ví dụ về các hình ảnh trực quan (visualizations) của các HPC workload dành cho khách hàng hàng không vũ trụ.\rMỗi loại workload mô phỏng đều có các yêu cầu riêng về loại cơ sở hạ tầng mà nó chạy trên. AWS cho phép khách hàng tối ưu hóa cấu hình cơ sở hạ tầng, cluster và hàng đợi của họ để chạy hiệu quả workload mô hình hóa hoặc mô phỏng đang thực hiện.\nBộ Công Cụ HPC của AWS\rHigh performance computing đòi hỏi cơ sở hạ tầng hiệu quả ở mọi lớp của stack. Điều này bao gồm các công cụ tính toán (compute), lưu trữ (storage), mạng (networking) và điều phối (orchestration) cho phép các tổ chức hàng không vũ trụ đổi mới nhanh chóng. Trong phần này, chúng ta sẽ xem xét một số công cụ mà khách hàng hàng không vũ trụ sử dụng trên AWS cho các HPC workload.\nAmazon Elastic Compute Cloud (Amazon EC2) cung cấp nền tảng tính toán rộng nhất và sâu nhất, với hơn 850 instance. Amazon EC2 có nhiều loại instance type hiệu suất cao được tối ưu hóa cho Accelerated Computing và HPC. AWS Nitro System được giới thiệu vào năm 2017 và được xây dựng dựa trên sự kết hợp giữa phần cứng, phần mềm và firmware được xây dựng có mục đích. Nó cung cấp cơ sở hạ tầng ảo hóa cơ bản cho các EC2 instance. Theo truyền thống, các hypervisor bảo vệ phần cứng vật lý và BIOS, ảo hóa CPU, lưu trữ, mạng và cung cấp một bộ khả năng quản lý phong phú. Với Nitro System, chúng tôi tách rời các chức năng đó, chuyển chúng sang phần cứng và phần mềm chuyên dụng, đồng thời giảm chi phí bằng cách cung cấp thực tế tất cả các tài nguyên của một server cho các instance của bạn. Điều này làm giảm thiểu chi phí ảo hóa (virtualization overhead).\nHình 3: Nitro System làm giảm thiểu chi phí hypervisor overhead để các instance của khách hàng có thể chạy ở mức ~100% dung lượng bare metal. Vùng màu nhạt hơn cho thấy các hoạt động kỹ thuật mà Nitro đảm nhiệm, trong khi vùng màu đậm hơn cho thấy các instance của khách hàng chạy trên Nitro.\rDịch vụ được quản lý (managed service) mới nhất của AWS giúp đơn giản hóa HPC trên AWS là AWS Parallel Computing Service (AWS PCS). AWS PCS giúp khách hàng dễ dàng chạy và mở rộng quy mô các HPC workload cũng như xây dựng các mô hình khoa học và kỹ thuật trên AWS bằng cách sử dụng Slurm làm trình quản lý workload. Dịch vụ được quản lý này cho phép bạn xây dựng các HPC cluster hoàn chỉnh tích hợp các tài nguyên tính toán (compute), lưu trữ (storage), mạng (networking) và hình ảnh trực quan (visualization), và mở rộng quy mô liền mạch từ 0 đến hàng nghìn instance. Thay vào đó, khách hàng có thể sử dụng AWS ParallelCluster, đây là một công cụ quản lý cluster mã nguồn mở (open-source), giàu tính năng, giúp dễ dàng cấu hình, triển khai và quản lý các HPC cluster trên AWS. Công cụ này được sử dụng thông qua các mẫu cơ sở hạ tầng dưới dạng mã (infrastructure as code templates), và có giao diện đồ họa dựa trên web tùy chọn. AWS ParallelCluster không phải là một dịch vụ được quản lý (managed service) và do đó yêu cầu khách hàng phải tự triển khai.\nAWS Batch giúp bạn chạy các batch computing workload trên AWS Cloud. Batch computing là một cách phổ biến để các nhà phát triển, nhà khoa học và kỹ sư truy cập vào một lượng lớn tài nguyên tính toán. AWS Batch loại bỏ công việc nặng nhọc không tạo ra sự khác biệt trong việc cấu hình và quản lý cơ sở hạ tầng cần thiết, giống như phần mềm batch computing truyền thống. Dịch vụ này có thể cấp phát tài nguyên hiệu quả để phản hồi các job đã gửi nhằm loại bỏ các hạn chế về dung lượng (capacity constraints), giảm chi phí tính toán và cung cấp kết quả nhanh chóng.\nCho đến nay, chúng ta đã thảo luận về các tài nguyên tính toán và công cụ điều phối (orchestration tooling) cho phép các HPC workload chạy trên AWS. Có những thành phần khác quan trọng đối với cơ sở hạ tầng HPC, chẳng hạn như mạng kết nối các compute node và lưu trữ hiệu suất cao (high-performance storage). Trước tiên, hãy xem xét về mạng.\nElastic Fabric Adapter (EFA) là một giao diện mạng cho các Amazon EC2 instance cho phép khách hàng chạy các ứng dụng đòi hỏi mức độ giao tiếp giữa các node (inter-node communications) cao trên quy mô lớn trên AWS. Giao diện phần cứng bỏ qua hệ điều hành (OS bypass hardware interface) được xây dựng tùy chỉnh của nó giúp tăng cường hiệu suất giao tiếp giữa các instance (inter-instance communications), điều này rất quan trọng để mở rộng quy mô các HPC workload có độ trễ thấp (low latency).\nHình 4: Cho thấy network infrastructure stack của EFA, trực quan hóa cách kernel được bỏ qua để tăng tốc hiệu suất.\rAWS cung cấp nhiều dịch vụ lưu trữ, chẳng hạn như Amazon Simple Storage Service (Amazon S3), Amazon Elastic Block Storage (Amazon EBS), cùng nhiều dịch vụ khác. Tất cả các dịch vụ lưu trữ này có thể được sử dụng khi xây dựng các HPC cluster trong AWS. Tuy nhiên, nhiều HPC workload được hưởng lợi rất nhiều từ bộ lưu trữ chuyên biệt, chẳng hạn như Lustre – một hệ thống tệp phân tán, song song, mã nguồn mở được thiết kế cho HPC và lưu trữ dữ liệu quy mô lớn. Amazon đã giải quyết nhu cầu này cho các HPC và AI/ML workload bằng cách cung cấp Amazon FSx for Lustre.\nAmazon FSx for Lustre là dịch vụ lưu trữ chia sẻ được quản lý hoàn toàn (fully managed shared storage service), được xây dựng trên hệ thống tệp song song, hiệu suất cao phổ biến nhất thế giới. Nó cho phép khách hàng tăng tốc các compute workload với bộ lưu trữ chia sẻ cung cấp độ trễ dưới mili giây (sub-millisecond latencies), thông lượng (throughput) lên đến hàng trăm GB/s, và hàng triệu IOPS, tất cả đều được quản lý hoàn toàn và có thể triển khai trong vài phút, mà không gặp khó khăn trong việc thiết lập và quản trị.\nMột Ngày của một HPC Job trên AWS\rGiờ đây chúng ta đã hiểu rõ hơn về các trường hợp sử dụng HPC và các dịch vụ mà khách hàng hàng không vũ trụ đang tận dụng trên AWS, hãy tổng hợp tất cả lại thành một quy trình làm việc (workflow) chức năng. Sơ đồ dưới đây minh họa một khách hàng đang chạy các HPC workload trong môi trường hybrid cloud của họ, giữa trung tâm dữ liệu on-premises và AWS. Người dùng cuối từ bên trong ranh giới mạng của khách hàng kết nối với Login Nodes thông qua SSH. Từ Login Nodes, các HPC job được gửi đi và thêm vào hàng đợi job. Điều này kích hoạt việc phân bổ các compute node, nơi các EC2 instance được mở rộng quy mô để đáp ứng nhu cầu hàng đợi và chạy các job. Các EC2 này có khả năng kết nối với các AWS services, chạy cho đến khi HPC job hoàn thành, và sau đó tự động thu hẹp quy mô trở lại (scale back down).\nHình 5: Ví dụ về quy trình làm việc triển khai các HPC job tận dụng AWS Parallel Computing Service.\rChúng ta đã đề cập đến một số trường hợp sử dụng, dịch vụ và quy trình làm việc mà khách hàng hàng không vũ trụ tận dụng trên AWS. Bước hợp lý tiếp theo là nghe từ chính khách hàng\nCác Câu Chuyện Thành Công trong Hàng không Vũ trụ từ Thực tế\rHypersonix Launch Systems đã giảm 92% thời gian CFD simulation pipeline của họ, từ 3 tháng xuống còn 1 tuần, bằng cách di chuyển sang AWS. Họ đã chạy các STAR-CCM+ workload on-premises, trong một HPC cluster bị sử dụng quá mức và lỗi thời. Thời gian chờ đợi trong hàng đợi kéo dài khiến các nhà nghiên cứu và kỹ sư của họ thường xuyên phải ngồi không. AWS đã trả lại thời gian cho các đội ngũ kỹ thuật này, để họ có thể đổi mới và đưa sản phẩm ra thị trường nhanh hơn. “Tôi tin rằng chúng tôi có thể nổi bật so với các công ty lớn hơn vì chúng tôi có khả năng và tài nguyên cloud mà chúng tôi cần trên AWS.”, Tiến sĩ Stephen Hall, Trưởng phòng Mô phỏng Cấu trúc Nhiệt CFD Tiên tiến tại Hypersonix Launch Systems, cho biết.\nBoom Supersonic sử dụng AWS để tăng tốc thiết kế và xây dựng máy bay siêu thanh của họ. Họ có thể chạy hàng nghìn mô phỏng tiên tiến đồng thời trên AWS, dẫn đến năng suất tăng ước tính gấp 6 lần so với môi trường on-prem của họ. Boom đã sử dụng hơn 53 triệu compute hours trên AWS để hoàn thành máy bay chở khách Overture của họ. “AWS, nhà cung cấp cloud hàng đầu thế giới, sẽ giúp chúng tôi liên tục tinh chỉnh các thiết kế của mình.”, Blake Scholl, Người sáng lập và CEO của Boom Supersonic, cho biết.\nĐể biết thêm thông tin về các câu chuyện thành công của khách hàng, vui lòng truy cập: (https://aws.amazon.com/solutions/case-studies/)\nKết Luận\rHPC dựa trên Cloud đang cách mạng hóa cách các tổ chức hàng không vũ trụ đổi mới. AWS cung cấp khả năng mở rộng (scalability), hiệu suất và bảo mật cần thiết cho các HPC workload hàng không vũ trụ khắt khe nhất. Khi ngành công nghiệp tiếp tục phát triển, cam kết của chúng tôi trong việc hỗ trợ đổi mới hàng không vũ trụ vẫn mạnh mẽ hơn bao giờ hết.\nLink bài viết gốc: (https://aws.amazon.com/blogs/hpc/accelerating-aerospace-innovation-high-performance-computing-hpc-on-amazon-web-services-aws/)",
    "description": "Tăng tốc Đổi mới Hàng không Vũ trụ: High Performance Computing (HPC) trên Amazon Web Services (AWS)\rTác giả: Gabe Kafity – 29/7/2025\nChủ đề: Best Practices, High Performance Computing\nTrong ngành hàng không vũ trụ đang phát triển nhanh chóng ngày nay, khả năng đổi mới nhanh chóng và hiệu quả không chỉ là một lợi thế – mà đó là một điều cần thiết. Khi các công nghệ như UAVs (Unmanned Aerial Vehicle) tự hành, các chùm vệ tinh (satellite constellations), tên lửa tái sử dụng và thực tế tăng cường/ảo (augmented/virtual reality) tiến bộ, khả năng đổi mới nhanh chóng mang lại cho các tổ chức hàng không vũ trụ lợi thế cạnh tranh. High Performance Computing (HPC) rất quan trọng đối với đổi mới hàng không vũ trụ và đã trở thành nền tảng của sự tiến bộ trong ngành hàng không vũ trụ. Bất kể quy mô, tuổi đời hay tốc độ lặp lại (iteration speed) của một tổ chức, Amazon Web Services (AWS) luôn sẵn sàng giúp thúc đẩy các sứ mệnh hàng không vũ trụ của họ tiến lên.",
    "tags": [],
    "title": "Blog 2",
    "uri": "/en/3-translated_blogs/blog_2/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Events Participated",
    "content": "AWS Cloud Mastery Series #3 – Security Pillar Workshop\rThis workshop focused on helping participants understand how to design and operate secure AWS systems based on the Security Pillar of the AWS Well-Architected Framework, combining theory with practical demos and real-world scenarios.\nWhat I Learned\rSecurity Foundation\rThe session started with an overview of the Security Pillar and its role in cloud architecture. Key security principles such as Least Privilege, Zero Trust, and Defense in Depth were explained in a practical context.\nThe speaker also clarified the AWS Shared Responsibility Model, highlighting common misunderstandings and real security risks seen in cloud environments in Vietnam.\nIdentity \u0026 Access Management (IAM)\rThis part covered modern IAM design and best practices, including:\nHow to use IAM Roles instead of long-term credentials Managing access with IAM Users, Roles, and Policies Using IAM Identity Center for SSO and permission sets Applying Service Control Policies (SCP) and permission boundaries in multi-account setups Enforcing security with MFA, credential rotation, and Access Analyzer A short demo demonstrated how to validate IAM policies and simulate access, showing how misconfigurations can easily lead to over-permission.\nDetection \u0026 Monitoring\rThe workshop then moved on to detection and monitoring, where I learned how AWS provides visibility into security events:\nUsing CloudTrail, GuardDuty, and Security Hub for continuous detection Implementing logging at multiple layers (VPC Flow Logs, ALB logs, S3 logs) Automating alerts and responses with EventBridge Applying the concept of Detection-as-Code to make detection consistent and repeatable Infrastructure Protection\rThis section focused on securing the network and compute layer:\nDesigning VPCs with proper segmentation and public/private placement Understanding the difference between Security Groups and Network ACLs Protecting workloads with WAF, Shield, and Network Firewall Basic security practices for EC2 and container workloads (ECS/EKS) The examples helped connect architectural decisions to real security outcomes.\nData Protection\rData security was covered through practical encryption and access strategies:\nManaging encryption keys with AWS KMS (policies, grants, rotation) Encrypting data at rest and in transit across services like S3, EBS, RDS, and DynamoDB Storing and rotating secrets using Secrets Manager and Parameter Store Applying data classification and guardrails to reduce the risk of data leakage Incident Response\rThe final technical session focused on incident response:\nAWS-recommended IR lifecycle Playbooks for real-world cases such as: Compromised IAM credentials Public S3 bucket exposure Malware detected on EC2 Techniques for isolation, snapshotting, and evidence collection Automating responses with Lambda and Step Functions This section highlighted the importance of preparing response workflows before incidents occur.\nKey Takeaways\rI gained a clear understanding of how the five Security Pillars work together in AWS. Security is not just about tools, but about design, automation, and continuous monitoring. Proper IAM design is critical and often the weakest point in cloud environments. Incident response should be automated and tested, not handled manually under pressure. Overall, the workshop provided practical insights into securing AWS environments and helped strengthen my confidence in designing and operating cloud systems with security in mind.",
    "description": "AWS Cloud Mastery Series #3 – Security Pillar Workshop\rThis workshop focused on helping participants understand how to design and operate secure AWS systems based on the Security Pillar of the AWS Well-Architected Framework, combining theory with practical demos and real-world scenarios.\nWhat I Learned\rSecurity Foundation\rThe session started with an overview of the Security Pillar and its role in cloud architecture. Key security principles such as Least Privilege, Zero Trust, and Defense in Depth were explained in a practical context.\nThe speaker also clarified the AWS Shared Responsibility Model, highlighting common misunderstandings and real security risks seen in cloud environments in Vietnam.",
    "tags": [],
    "title": "Event 2",
    "uri": "/en/4-events_participated/4.2-event-2/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Workshop \u003e Deploy Flow \u003e Backend Deploy",
    "content": "This phase provisions the data layer components, service discovery mechanism, and load balancing infrastructure.\nSSL Certificate Provisioning\rRequest ACM Certificate\nSwitch region to Singapore (ap-southeast-1) Navigate to Certificate Manager (ACM) Select Request a certificate → Request a public certificate Configure certificate request: Domain name: sgutodolist.com Validation method: DNS validation Click Request Access the certificate details → Under Domains section → Click Create records in Route 53 Wait for status to change to Issued (typically 5-10 minutes) RDS MYSQL (Database)\rGoal: Create MySQL 8.0 Database located in Private Subnet.\nGo to RDS \u003e Create database.\nChoose a database creation method: Full Configuration (To customize).\nEngine options: Select MySQL.\nEngine Version: Select 8.0.x (For example 8.0.35 or 8.0.39) to match Docker Compose.\nTemplates: Select Free tier.\nSettings:\nDB Instance identifier: sgu-todolist-db\nMaster username: root\nMaster password: 12345678 (Example).\nInstance configuration: Select db.t3.micro (or t2.micro if the account is old).\nConnectivity (IMPORTANT):\nCompute resource: Don’t connect to an EC2 compute resource.\nVPC: Select SGU-Microservices-vpc.\nDB Subnet group: Select Create new (or select the existing one pointing to 2 Private Subnets).\nPublic access: NO (Because the DB is in the Private area).\nVPC security group: Select private-db-sg (Uncheck default).\nAvailability Zone: Select ap-southeast-1a.\nAdditional configuration: Initial database name: aws_todolist_database (Enter it here to avoid having to run the CREATE DATABASE command manually, if desired). However, it’s best to leave it blank and use the Bastion created later to be sure.\nUncheck Enable automated backups (to save storage space if only testing).\nClick Create database. 👉 After creating (Status: Available), copy Endpoint.\nELASTICACHE REDIS\rGoal: Create a simple, cheap Redis t3.micro, without using Cluster mode.\nGo to ElastiCache \u003e Redis OSS caches \u003e Create Redis OSS cache.\nCluster settings:\nEngine: Redis OSS.\nDeployment option: Node-based cluster (This must be selected to adjust the configuration).\nCreation method: Cluster cache.\nCluster info: Cluster mode: Disabled.\nName: sgu-redis.\nCache settings: Node type: cache.t3.micro.\nNumber of replicas: 0.\nConnectivity: Subnet groups: Select Create new (if not already there).\nName: sgu-redis-subnet-group.\nVPC: SGU-Microservices-vpc.\nSubnets: Select 2 Private Subnets.\nVPC security groups: Select private-db-sg.\nAvailability Zone placements: Select Specify Availability Zones -\u003e ap-southeast-1a.\nClick Next.\nAdvanced settings:\nEnable automatic backups: Uncheck (Save money).\nLogs: Disable all.\nScroll to the bottom and click Create. After creating, copy Primary Endpoint.\nAWS CLOUD MAP (Service Discovery)\rGoal: Create an internal .local domain for Kafka and AI Service to talk to each other without a Load Balancer.\nGo to Cloud Map service.\nClick Create namespace.\nNamespace name: sgu.local.\nDescription: SGU Internal Network.\nInstance discovery: Select API calls and DNS queries in VPCs.\nVPC: Select SGU-Microservices-vpc.\nClick Create namespace.\nALB ROUTING (Target Groups \u0026 Load Balancer)\rThis is the most important routing part for users to access the system.\n4.1 Create 5 Target Groups (Repeat 5 times)\rGo to EC2 \u003e Target Groups \u003e Create target group.\nCommon configuration for all 5:\nTarget type: IP addresses (Required for ECS Fargate).\nProtocol: HTTP.\nIP address type: IPv4.\nVPC: SGU-Microservices-vpc.\nHealth check path: /actuator/health.\nNote: In the next “Register targets” step, DO NOT enter any IP, click Create.\nList of 5 Target Groups to create:\nName Port Health Check Path auth-tg 9999 /api/auth/actuator/health user-tg 8081 /api/user/actuator/health task-tg 8082 /api/taskflow/actuator/health noti-tg 9998 /api/notification/actuator/health 4.2 Create Application Load Balancer (ALB)\rGo to EC2 \u003e Load Balancers \u003e Create load balancer \u003e Application Load Balancer.\nBasic configuration: Name: sgu-alb.\nScheme: Internet-facing.\nIP address type: IPv4.\nNetwork mapping: VPC: SGU-Microservices-vpc.\nMappings: Select 2 Public Subnets (For example ...public1... and ...public2...). ⚠️ If you get this wrong, the whole thing will collapse.\nSecurity groups: Select public-alb-sg. Listeners and routing (Create 2): HTTP:80: Forward to api-gateway-tg.\nHTTPS:443 (Click Add listener):\nDefault action: Forward to api-gateway-tg.\nSecure listener settings: Select ACM certificate sgutodolist.com\nImportant: Để mọi service phải thông qua api-gateway thì phải set Listener rules của Protocol:Port HTTPS:443 là: /api/auth/* (vì auth thì có thể đi 1 luồng riêng) và Default\nEC2 \u003e Load balancers \u003e sgu-alb \u003e HTTPS:443 listener Ở mục Listener rules xóa các Rule có điều kiện Path là /api/\u003cservice\u003e/* ⬅ STEP 1: Network \u0026 Security Preparation\rSTEP 3: Code Update \u0026 Image Build ➡",
    "description": "This phase provisions the data layer components, service discovery mechanism, and load balancing infrastructure.\nSSL Certificate Provisioning\rRequest ACM Certificate\nSwitch region to Singapore (ap-southeast-1) Navigate to Certificate Manager (ACM) Select Request a certificate → Request a public certificate Configure certificate request: Domain name: sgutodolist.com Validation method: DNS validation Click Request Access the certificate details → Under Domains section → Click Create records in Route 53 Wait for status to change to Issued (typically 5-10 minutes) RDS MYSQL (Database)\rGoal: Create MySQL 8.0 Database located in Private Subnet.",
    "tags": [],
    "title": "Infrastructure \u0026 ALB Setup (RDS, Redis, Cloud Map, ALB Routing)",
    "uri": "/en/5-workshop/5.3-deploy_flow/5.3.2-backend-deploy/5.3.2.2-infrastructure--alb-setup-rds-redis-cloud-map-alb-routing/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Workshop",
    "content": "Preamble: Prerequisites for Backend Deployment\rBefore initiating the deployment steps outlined in the Table of Contents, the following foundational tools, configurations, and AWS service setups must be completed to ensure a smooth, secure, and cost-optimized deployment into the ap-southeast-1 (Singapore) region.\nStep Requirement Purpose 1 AWS Account \u0026 CLI Access and command-line management. 2 Domain \u0026 SSL Securing traffic and global content delivery. 3 Source Code \u0026 Artifacts Building the deployable microservice images. 4 AWS Network Foundation Setting up the isolated, highly available VPC structure. I. Tools \u0026 Credentials\rAWS Account Setup: A valid AWS Account is required, and initial billing alerts (AWS Budgets) must be configured to monitor Free Tier usage.\nAWS CLI: AWS Command Line Interface must be installed and configured with appropriate IAM User credentials granting necessary permissions (EC2, RDS, S3, CloudFront, Route 53, IAM, CloudWatch).\nDocker: Docker Engine must be installed locally to build the container images for the Spring Boot microservices.\nJava/Maven: Local environment must be set up to compile and package the Spring Boot application code.\nII. Domain, SSL, and DNS Preparation\rRegistered Domain: A domain name must be registered (e.g., via Route 53 or an external registrar).\nRoute 53 Hosted Zone: The domain must be managed within an Amazon Route 53 Hosted Zone.\nACM Certificate: An AWS Certificate Manager (ACM) certificate must be provisioned for the domain (e.g., *.taskmanager.com) in two regions:\nus-east-1 (N. Virginia): Required for CloudFront (Global Service).\nap-southeast-1 (Singapore): Required for the Application Load Balancer (ALB).\nIII. Network Foundation (VPC \u0026 Subnets)\rA VPC must be created in ap-southeast-1 with the following structure for High Availability (HA) across at least two Availability Zones (AZs):\nVPC: One primary VPC (e.g., CIDR 10.0.0.0/16).\nPublic Subnets (Multi-AZ): To host the Application Load Balancer (ALB) and Internet Gateway (IGW).\nPrivate Subnets (Multi-AZ): To host the application instances (EC2/ECS), RDS, and ElastiCache.\nNAT Gateway: Deployed in at least one Public Subnet to allow resources in the Private Subnets (EC2 instances) to securely access the internet for updates and communication with AWS services (like S3/ECR).\nSecurity Groups (SG): Initial Security Groups must be defined following the principle of least privilege:\nALB SG: Allows inbound traffic on HTTP (80) and HTTPS (443) from the internet.\nEC2/ECS SG: Allows inbound traffic only from the ALB SG on the application port (e.g., 8080).\nRDS/ElastiCache SG: Allows inbound traffic only from the EC2/ECS SG on their respective ports (e.g., MySQL 3306, Redis 6379).\nIV. Data \u0026 Storage Pre-Configuration\rS3 Primary Bucket (ap-southeast-1): An S3 bucket must be created in ap-southeast-1 to hold static assets and application deployment artifacts (JARs/Docker Images).\nS3 DR Bucket (us-east-1): A secondary S3 bucket must be created in us-east-1 (N. Virginia) to serve as the Disaster Recovery target.\nCross-Region Replication (CRR): S3 CRR must be configured to automatically replicate objects from the primary bucket in ap-southeast-1 to the DR bucket in us-east-1.\nV. Compute \u0026 Service Preparation\rDocker Images: The Spring Boot microservices must be built into production-ready Docker images.\nECR Repository (Optional): If using ECS, a repository must be prepared in ECR (Elastic Container Registry) to store the Docker images.\nIAM Roles: Necessary IAM Roles must be created for:\nEC2/ECS Tasks: Grants permissions to access S3, CloudWatch, and the RDS database.\nALB: Allows the load balancer to perform its function.",
    "description": "Preamble: Prerequisites for Backend Deployment\rBefore initiating the deployment steps outlined in the Table of Contents, the following foundational tools, configurations, and AWS service setups must be completed to ensure a smooth, secure, and cost-optimized deployment into the ap-southeast-1 (Singapore) region.\nStep Requirement Purpose 1 AWS Account \u0026 CLI Access and command-line management. 2 Domain \u0026 SSL Securing traffic and global content delivery. 3 Source Code \u0026 Artifacts Building the deployable microservice images. 4 AWS Network Foundation Setting up the isolated, highly available VPC structure. I. Tools \u0026 Credentials\rAWS Account Setup: A valid AWS Account is required, and initial billing alerts (AWS Budgets) must be configured to monitor Free Tier usage.",
    "tags": [],
    "title": "Prerequisite",
    "uri": "/en/5-workshop/5.2-prerequisite/index.html"
  },
  {
    "breadcrumb": "Internship Report",
    "content": "Cost-Optimized SaaS Task Management Platform\rSingle-Region High Availability with ECS Fargate\r1. Executive Summary\rThe SaaS Task Management Platform is designed to deliver a Todoist-like collaborative experience with High Availability (HA) and Cost Efficiency, specifically optimized for AWS Free Tier constraints.\nInitially, the project targeted a Multi-Region deployment to achieve the high levels of global performance, disaster recovery (DR), and uptime. However, due to the cost limitations and the constraints of the AWS Free Tier (particularly concerning cross-region data transfer and running multiple primary/replica database instances), the architecture was strategically consolidated into a Single AWS Region (ap-southeast-1 - Singapore).\nBuilt on Spring Boot microservices deployed as ECS Fargate containers, the platform now leverages Multi-AZ deployment within Singapore for resilience while eliminating server management overhead.\nKey Architecture Highlights:\nServerless Compute: ECS Fargate eliminates EC2 instance management\nRegional High Availability: Multi-AZ deployment for compute (ECS), database (RDS), and caching (ElastiCache)\nLow Latency in SEA: Optimized performance for Southeast Asian users\nGlobal Content Delivery: CloudFront CDN for worldwide static asset distribution\nCost Predictability: Architecture designed to maximize AWS Free Tier benefits\nDisaster Recovery: S3 Cross-Region Replication to us-east-1 for data backup\nThe result is a production-ready, cost-effective platform that balances performance, availability, and operational simplicity—ideal for MVP development and regional deployment, while maintaining a clear roadmap for future Multi-Region expansion.\n2. Problem Statement \u0026 Solution\r2.1. The Challenge: Balancing Performance, Availability, and Cost\rTraditional SaaS platforms face critical trade-offs when operating on limited budgets:\nOperational Complexity:\nEC2-based deployments require constant server management (patching, monitoring, capacity planning)\nManual scaling decisions lead to over-provisioning (wasted money) or under-provisioning (poor performance)\nComplex deployment processes prone to human error and downtime\nCost Challenges:\nMulti-region architectures introduce expensive cross-region data transfer\nRunning RDS Read Replicas in multiple regions quickly exhausts Free Tier limits\nNAT Gateway charges ($35+/month) consume significant portions of small budgets\nAlways-on EC2 instances waste capacity during off-peak hours\nAvailability vs. Budget:\nSingle-region deployments risk total outage during regional failures\nMulti-AZ configurations increase costs but are essential for production workloads\nBalancing HA requirements with Free Tier constraints requires careful architecture\n2.2. The Single-Region ECS Solution\rThis project adopts a Single-Region (ap-southeast-1) High Availability architecture using ECS Fargate for serverless compute, optimizing for cost while maintaining production-grade reliability.\nCore Architecture Decisions:\nCompute Layer (Serverless):\nECS Fargate for containerized Spring Boot microservices (no EC2 management)\nApplication Load Balancer for traffic distribution and health checks\nAWS Cloud Map for service discovery between microservices\nAuto Scaling based on CPU/memory metrics\nData Layer (Regional + Multi-AZ):\nRDS MySQL (db.t3.micro) with Multi-AZ for automated failover\nElastiCache Redis (cache.t3.micro) for session management and caching\nS3 with Cross-Region Replication to us-east-1 for disaster recovery\nNetwork Layer (Global + Regional):\nVPC with public/private subnets across 2 Availability Zones\nCloudFront CDN for global static asset delivery\nRoute 53 for DNS management and domain hosting\nVPC Endpoints to eliminate NAT Gateway costs ($35/month savings)\nSecurity \u0026 Observability:\nSecurity Groups for network-level access control\nIAM Roles for service-to-service authentication (no hardcoded credentials)\nOrigin Access Control (OAC) to secure S3 buckets\nCloudWatch for centralized logging, metrics, and alarms\nKey Advantages Over Traditional Architecture:\nAspect Traditional EC2 ECS Fargate (This Project) Server Management Manual patching, monitoring, SSH access Fully managed, no server access needed Scaling Manual ASG configuration Automatic based on metrics Deployment Complex, multi-step process Rolling updates with zero downtime Cost Model Pay for running instances (24/7) Pay only for container runtime Free Tier 750 EC2 hours/month 20 GB-hours + 50 GB data transfer Startup Time 2-5 minutes (AMI boot) 30-60 seconds (container start) 2.3. Benefits and ROI\rTechnical Benefits:\n80% reduction in operational overhead (no server management)\n99.5%+ uptime with Multi-AZ deployment for RDS and ECS\n\u003c100ms API response time for cached requests\n2-5 minute recovery time for AZ failures (automatic)\nCost Benefits:\n~$25-30/month operational cost (vs $50-90 with EC2)\nVPC Endpoints eliminate $35/month NAT Gateway charges\nPay-per-use pricing—only charged for actual container runtime\nFree Tier optimized to stay within budget constraints\nLearning \u0026 Career Value:\nHands-on experience with modern cloud-native architecture\nContainer orchestration expertise (Docker, ECS Fargate)\nDevOps practices: IaC, CI/CD, monitoring, incident response\nPortfolio project demonstrating AWS Solutions Architect competencies\n3. Solution Architecture\rFigure 1. Single-Region ECS Fargate Architecture\r3.1. Architecture Overview\rPrimary Deployment Region: ap-southeast-1 (Singapore)\nCompute \u0026 Orchestration:\nECS Fargate Cluster running 5 containerized Spring Boot microservices\nApplication Load Balancer distributing traffic across ECS tasks (Multi-AZ)\nAWS Cloud Map for internal service discovery (private DNS namespace)\nAuto Scaling policies targeting 70% CPU, 80% memory utilization\nData \u0026 Caching:\nRDS MySQL (db.t3.micro) with Multi-AZ deployment for automatic failover\nElastiCache Redis (cache.t3.micro) for session storage and hot data caching\nS3 bucket for user files, attachments, and static assets\nNetwork Infrastructure:\nVPC (10.0.0.0/16) with public and private subnets across 2 Availability Zones\nInternet Gateway for public subnet (ALB) internet connectivity\nVPC Endpoints for S3, ECR, CloudWatch (eliminates NAT Gateway cost)\nSecurity Groups controlling all traffic flows between components\nGlobal Services:\nContent Delivery:\nCloudFront CDN with global edge locations for static asset distribution\nOrigin: Primary S3 bucket in Singapore, failover to us-east-1\nOrigin Access Control (OAC) ensuring S3 is only accessible via CloudFront\nDisaster Recovery:\nS3 Cross-Region Replication (CRR) to us-east-1 (N. Virginia)\nRDS Automated Snapshots (7-day retention, daily backups)\nDNS \u0026 Security:\nRoute 53 for domain hosting and DNS management\nACM (AWS Certificate Manager) for free SSL/TLS certificates\nCloudWatch for centralized logs, metrics, and alarms\n3.2. Microservices Architecture\rThe platform implements 5 core Spring Boot microservices deployed as ECS Fargate tasks:\nService Port Responsibilities Container Resources API Gateway 8080 API Traffic Routing, Request Aggregation/Validation 0.5 vCPU, 1GB RAM User Service 8081 User profiles, settings, avatar management 0.5 vCPU, 1GB RAM Taskflow Service 8082 Board/workspace creation, Task CRUD, assignments, comments, attachments 0.5 vCPU, 1GB RAM Notification Service 9998 Real-time notifications, email (SES), WebSockets 0.5 vCPU, 1GB RAM Auth Service 9999 User authentication, JWT tokens, OAuth2 (Google) 0.5 vCPU, 1GB RAM Service Communication Patterns:\nExternal Traffic (Users):\nUser → Route 53 → CloudFront (static) OR ALB (API)\r→ ALB Path-Based Routing:\r/api/* → API Gateway Target Group (Port 8080)\rAPI Gateway Internal Routing (Port-based, via Cloud Map):\r/auth/* → Auth Service (Port 9999)\r/users/* → User Service (Port 8081)\r/taskflow/* → Taskflow Service (Port 8082)\r/notifications/* → Notification Service (Port 9998)\rInternal Service-to-Service:\nService A → AWS Cloud Map (service-b.local:PORT)\r→ Direct container-to-container communication\r→ Response cached in Redis (5-minute TTL)\rContainer Configuration:\nBase Image: eclipse-temurin:21-jre-alpine (lightweight JRE)\nHealth Check: Spring Actuator /actuator/health/liveness endpoint (used by ALB)\nLogging: CloudWatch Logs with 7-day retention\nEnvironment Variables: Database credentials, Redis endpoint, S3 bucket name\nIAM Task Role: Permissions for S3, SES, CloudWatch access\n3.3. AWS Services Used\rCategory Services Purpose Cost Optimization Compute ECS Fargate, ALB Serverless container orchestration, load balancing Free Tier: 750 ALB hours, 20 GB Fargate hours Database RDS MySQL (db.t3.micro) Primary database with Multi-AZ HA Fixed cost: ~$15/month (exceeds Free Tier) Caching ElastiCache Redis (cache.t3.micro) Session store, API response caching Free Tier: 750 node hours Storage S3 Standard + CRR Object storage with DR replication Free Tier: 5GB storage, 20k GET, 2k PUT CDN CloudFront Global content delivery Free Tier: 1TB data transfer, 10M requests Container Registry ECR Docker image storage Free Tier: 500MB storage Service Discovery AWS Cloud Map Microservices service registry $0.50/namespace + $0.10/instance/month Networking VPC, VPC Endpoints Network isolation, private AWS service access VPC free, Endpoints ~$7/month (replaces $35 NAT) DNS Route 53 Domain hosting $0.50/hosted zone Security ACM, IAM, Security Groups SSL certificates, access control Free Observability CloudWatch Logs, Metrics, Alarms Monitoring, logging, alerting Free Tier: 5GB logs, 10 custom metrics 4. Service Roles Overview\rAWS Service Role in Architecture Configuration Details Route 53 DNS hosting for custom domain, SSL certificate validation Hosted zone: sgutodolist.com, Health checks for ALB CloudFront Global CDN serving static assets from edge locations Origin: S3 Singapore (primary), S3 Virginia (failover) ACM Free SSL/TLS certificates for HTTPS Wildcard cert: *.sgutodolist.com, auto-renewal enabled VPC Isolated network for all resources CIDR: 10.0.0.0/16, 2 public + 2 private subnets across 2 AZs Internet Gateway Enables ALB to receive traffic from internet Attached to public subnets only VPC Endpoints Private connections to AWS services (no NAT Gateway) S3 Gateway Endpoint, Interface Endpoints for ECR/CloudWatch Application Load Balancer Layer 7 routing, SSL termination, health checks Multi-AZ, single routing rule to API Gateway Target Group ECS Fargate Serverless container runtime (no EC2 management) Runs Spring Boot containers with varying ports ECS Cluster Logical grouping of ECS services and tasks Single cluster: sgutodolist-cluster ECS Service Maintains desired task count, integrates with ALB Desired count: 2 tasks/service, rolling deployment strategy ECS Task Definition Blueprint for containers (image, resources, environment) 5 task definitions (one per microservice) AWS Cloud Map Service discovery via private DNS Namespace: local, services: api-gateway.local, auth-service.local, etc. RDS MySQL Primary relational database with Multi-AZ db.t3.micro, 20GB storage, automated backups enabled ElastiCache Redis In-memory cache for sessions and API responses cache.t3.micro, cluster mode disabled S3 (Primary) User files, attachments, static frontend assets Bucket: sgutodolist-assets-sg (ap-southeast-1) S3 (Secondary) Disaster recovery replica Bucket: sgutodolist-assets-us (us-east-1) S3 CRR Automated async replication Singapore → Virginia Replication rule for all objects, versioning enabled Origin Access Control Restricts S3 access to CloudFront only Blocks direct public access to S3 buckets Security Groups Virtual firewall for ALB, ECS tasks, RDS, Redis Least-privilege rules, source-based restrictions IAM Task Execution Role Allows ECS to pull images from ECR, write logs Permissions: ECR pull, CloudWatch Logs write IAM Task Role Allows containers to access S3, SES, etc. Permissions: S3 read/write, SES send, CloudWatch metrics CloudWatch Logs Centralized application logs from ECS containers 7-day retention, 5 log groups (one per service) CloudWatch Metrics Performance metrics (CPU, memory, request count) Custom metrics for business KPIs CloudWatch Alarms Alerting for high CPU, failed tasks, budget thresholds SNS integration for email notifications 5. Service Flow\r5.1. User Request Flow\rStatic Assets (Frontend - HTML, CSS, JS, Images):\nUser accesses https://sgutodolist.com\nRoute 53 resolves DNS to CloudFront distribution\nCloudFront checks nearest edge location cache:\nCache HIT: Returns asset immediately (latency \u003c20ms)\nCache MISS: Fetches from S3 origin (Singapore), caches for 24 hours\nBrowser loads frontend application\nStatic assets served with low latency globally via edge locations\nAPI Requests (Backend Microservices):\nFrontend makes API call: https://sgutodolist.com/api/task\nRoute 53 resolves to Application Load Balancer in Singapore\nALB performs SSL termination and routes all /api/* traffic to the API Gateway Target Group (Port 8080).\nAPI Gateway (running on ECS) receives the request and internally routes via AWS Cloud Map based on the path:\nRequests to /api/auth/** are routed to Auth Service (Port 9999).\nRequests to /api/user/** are routed to User Service (Port 8081).\nRequests to /api/taskflow/** are routed to Taskflow Service (Port 8082).\nRequests to /api/notifications/** are routed to Notification Service (Port 9998).\nThe target microservice processes the request.\nData Access Pattern (90% Reads, 10% Writes):\nRead Operations:\nSpring Boot service receives request\nCheck ElastiCache Redis for cached data:\nCache HIT: Return immediately (latency \u003c5ms)\nCache MISS: Query RDS MySQL (Multi-AZ), cache result with TTL\nResponse returned through ALB to user\nTotal latency: 5-10ms (cached) or 20-50ms (database query)\nWrite Operations:\nSpring Boot service validates request\nWrite to RDS MySQL primary database\nRDS synchronously replicates to standby instance (same region, different AZ)\nInvalidate/update related cache entries in Redis\nAsync: Trigger notifications, update search index\nSuccess response to user\nTotal latency: 50-100ms\nService-to-Service Communication:\nExample: API Gateway needs to validate a JWT token via Auth Service\nAPI Gateway queries AWS Cloud Map: http://auth-service.local:9999/validate\nCloud Map resolves to healthy Auth Service task private IP\nDirect container-to-container communication within VPC (no ALB overhead)\nResponse cached in Redis for 5 minutes to reduce repeated calls\nLatency: 10-20ms for first call, \u003c5ms for subsequent cached calls\n5.2. Developer Deployment Flow\rCI/CD Pipeline (Manual Process, Automatable with CodePipeline):\nPhase 1: Local Development\nDeveloper updates Spring Boot microservice code\nRun unit tests: ./mvnw test\nRun integration tests with Docker Compose (local MySQL + Redis)\nCommit changes to Git repository\nPhase 2: Container Build\nBuild Spring Boot JAR: ./mvnw clean package\nBuild Docker image:\nFROM eclipse-temurin:21-jre-alpineWORKDIR /appCOPY target/task-service.jar app.jarEXPOSE 8084HEALTHCHECK CMD curl -f http://localhost:8084/actuator/health || exit 1ENTRYPOINT [\"java\", \"-jar\", \"app.jar\"]\rTag image: task-service:v1.2.3\nPhase 3: Push to ECR\n# Authenticate to ECR\raws ecr get-login-password --region ap-southeast-1 |\\\rdocker login --username AWS --password-stdin {account-id}.dkr.ecr.ap-southeast-1.amazonaws.com\r# Tag and push\rdocker tag task-service:v1.2.3\\\r{account-id}.dkr.ecr.ap-southeast-1.amazonaws.com/task-service:v1.2.3\rdocker push {account-id}.dkr.ecr.ap-southeast-1.amazonaws.com/task-service:v1.2.3\rPhase 4: Update ECS Task Definition\nNavigate to ECS console → Task Definitions → task-service\nCreate new revision:\nUpdate container image to :v1.2.3\nReview environment variables (DB_HOST, REDIS_HOST, etc.)\nVerify resource allocation (0.5 vCPU, 1GB RAM)\nRegister new task definition revision\nPhase 5: Deploy to ECS Service (Rolling Update)\nNavigate to ECS Service → task-service\nUpdate service to use new task definition revision\nECS performs automatic rolling update:\nLaunch 2 new tasks with updated image\nWait for tasks to pass ALB health checks (3 consecutive passes)\nALB starts routing 50% traffic to new tasks\nDrain connections from old tasks (30-second timeout)\nStop old tasks once fully drained\nTotal deployment time: 3-5 minutes (zero downtime)\nPhase 6: Verification \u0026 Monitoring\nCheck ECS Service events: aws ecs describe-services\nMonitor CloudWatch Logs for errors: /ecs/task-service\nTest API endpoints via ALB: curl https://sgutodolist.com/api/task/health\nCheck CloudWatch Metrics: CPU, memory, request count, error rate\nMonitor ALB Target Health in console\nRollback Procedure (If Issues Detected):\nIdentify last known good task definition revision (e.g., v1.2.2)\nUpdate ECS Service to use previous revision\nECS performs automatic rollback deployment (3-5 minutes)\nVerify health checks pass and logs show no errors\nInvestigate issue in lower environments before next deployment\nFuture Automation (Phase 2 Roadmap):\nGitHub Actions: Trigger builds on push to main branch\nAWS CodePipeline: Source (GitHub) → Build (CodeBuild) → Deploy (ECS)\nBlue/Green Deployments: Use CodeDeploy for safer production rollouts\nAutomated Testing: Integration tests run before deployment\n5.3. Data Replication \u0026 Storage Strategy\rData Storage \u0026 Backup Matrix:\nData Type Primary Storage Replication Method Backup RPO RTO Transactional Data (users, boards, tasks) RDS MySQL Multi-AZ Synchronous (within AZs) Automated snapshots (daily, 7-day retention) \u003c1 minute 10-20 minutes (AZ failure), 2-4 hours (region failure) Session Tokens ElastiCache Redis None (ephemeral) None (regenerate on failure) N/A Immediate (users re-authenticate) User Files (attachments, avatars) S3 Singapore CRR to S3 Virginia (async) Versioning (30-day retention) 5-15 minutes Immediate (CloudFront failover) Static Assets (frontend code) S3 Singapore CRR to S3 Virginia (async) Versioning + git repository 5-15 minutes Immediate (CloudFront failover) Application Logs CloudWatch Logs Regional replication (managed by AWS) 7-day retention N/A N/A Docker Images ECR Singapore Manual push to other regions if needed Image versioning N/A Minutes (pull from ECR) S3 Cross-Region Replication Configuration:\nSource Bucket: sgutodolist-assets-sg (ap-southeast-1)\nDestination Bucket: sgutodolist-assets-us (us-east-1)\nReplication Rules:\nReplicate all objects (prefix: /)\nReplicate metadata, tags, and object ACLs\nDo NOT replicate delete markers (prevent accidental data loss)\nPriority: 1 (highest)\nVersioning: Enabled on both source and destination\nExpected Latency: 5-15 minutes for most objects (\u003c1MB)\nUse Case: Disaster recovery + CloudFront origin failover\n5.4. High Availability \u0026 Disaster Recovery\rFailure Scenarios \u0026 Automated Response:\nScenario 1: Single ECS Task Failure\nDetection: ALB health check fails (3 consecutive failures at 10-second intervals)\nAutomatic Response:\nALB stops routing new requests to unhealthy task\nExisting connections drained (30-second timeout)\nECS Service Controller launches replacement task automatically\nImpact: Zero user-facing downtime (other tasks handle load)\nRecovery Time: 2-3 minutes (container start + health check pass)\nUser Experience: No interruption\nScenario 2: Complete Service Failure (All Tasks Unhealthy)\nDetection: All 2 tasks for a service fail health checks, ALB returns 503 errors\nAutomatic Response: ECS attempts to launch replacement tasks\nManual Intervention Required:\nCheck CloudWatch Logs for root cause (database connection timeout, memory leak, bad deployment)\nIf bad deployment: Rollback to previous task definition revision\nIf infrastructure issue: Check RDS/Redis connectivity, Security Groups\nImpact: Service unavailable until new tasks healthy or issue resolved\nRecovery Time: 5-15 minutes (depends on issue complexity)\nMitigation: Implement circuit breakers, graceful degradation\nScenario 3: Availability Zone Failure (e.g., AZ-1a Goes Down)\nDetection: All tasks in AZ-1a become unreachable, ALB marks them unhealthy\nAutomatic Response:\nALB immediately routes all traffic to tasks in healthy AZ (AZ-1b)\nECS Service launches replacement tasks in healthy AZs to restore desired count\nRDS Multi-AZ: If primary DB in failed AZ, RDS automatically fails over to standby (60-120 seconds)\nImpact:\n30-50% capacity reduction for 5-10 minutes (temporary latency increase)\nPossible 60-120 second database unavailability during RDS failover\nRecovery Time: 5-10 minutes for full capacity restoration\nUser Experience: Slight performance degradation, no data loss\nScenario 4: RDS Primary Database Failure\nDetection:\nRDS health checks fail\nApplication logs show connection timeouts to database\nCloudWatch alarm triggers: DatabaseConnections metric drops to 0\nAutomatic Response:\nRDS Multi-AZ automatically promotes standby instance to primary\nDNS endpoint (sgutodolist-db.xxxxx.ap-southeast-1.rds.amazonaws.com) updated to point to new primary\nApplication connection pools automatically reconnect (Spring Boot retry logic)\nImpact: 60-120 seconds of database write unavailability\nRecovery Time: 1-2 minutes (fully automated)\nData Loss: Zero (synchronous replication to standby)\nScenario 5: Complete Regional Failure (Singapore Outage)\nDetection:\nRoute 53 health checks fail for Singapore ALB\nCloudWatch alarms trigger: All ECS tasks unreachable\nManual verification: AWS Service Health Dashboard confirms regional issue\nManual DR Process:\nOption A: Read-Only Mode (15-30 minutes)\nUpdate CloudFront origin to use S3 Virginia bucket (static assets still work)\nDisplay maintenance page to users: “Service temporarily unavailable”\nWait for AWS to restore Singapore region\nOption B: Full Recovery to New Region (2-4 hours)\nRestore latest RDS snapshot to new region (us-east-1):\naws rds restore-db-instance-from-db-snapshot \\ --db-instance-identifier sgutodolist-dr \\ --db-snapshot-identifier rds:sgutodolist-db-2024-12-07-00-00 \\ --db-instance-class db.t3.micro \\ --availability-zone us-east-1a\rCreate new ECS Fargate cluster in us-east-1\nDeploy all 5 microservices using existing task definitions (update DB endpoint)\nCreate new ALB in us-east-1, configure target groups\nUpdate Route 53 to point to new ALB\nUpdate CloudFront origin to use new ALB for API calls\nImpact: Full service outage during recovery\nRecovery Time: 2-4 hours (manual process)\nData Loss: Last 5-60 minutes (RDS automated backups every 5 minutes, snapshots hourly)\nCost: Additional resources in us-east-1 (~$30/month if kept running)\nDisaster Recovery Metrics:\nRPO (Recovery Point Objective): \u003c1 hour (RDS automated backups)\nRTO (Recovery Time Objective):\nAZ failure: 5-10 minutes (automatic)\nRegional failure: 2-4 hours (manual)\nAvailability Target: 99.5% (43 minutes downtime/month allowance)\n6. Budget Estimation\r6.1. Monthly Cost Breakdown (Free Tier Optimized)\rAWS Service Specification Free Tier Allocation Expected Usage Cost/Month ECS Fargate 5 services x 2 tasks x 0.5 vCPU, 1GB RAM 20 GB-hours/month 10 tasks x 24h x 30d = 7,200 GB-hours $0 (Month 1), ~$36 after Free Tier RDS MySQL db.t3.micro, Multi-AZ, 20GB storage 750 hours Single-AZ only 744 hours Multi-AZ $15.00 (fixed cost) ElastiCache Redis cache.t3.micro, single node 750 node hours 744 hours $0 (Free Tier) Application Load Balancer Standard ALB, minimal LCUs 750 hours + 15 LCUs 744 hours, 10 LCUs $0 (Free Tier) S3 Storage Standard class + CRR 5GB storage, 20k GET, 2k PUT 10GB storage, 30k GET, 5k PUT, CRR $2.00 CloudFront Global edge locations 1TB data transfer, 10M requests 50GB data transfer, 2M requests $0 (Free Tier) Route 53 Hosted zone + queries First 25 zones discounted 1 hosted zone, 1M queries $0.50 VPC Endpoints S3 Gateway + ECR/CloudWatch Interface None 3 endpoints x 24h x 30d $7.00 ECR Docker image storage 500MB storage/month 2GB storage (5 images x 400MB) $0.20 AWS Cloud Map Service discovery namespace None 1 namespace + 5 service instances $1.00 CloudWatch Logs, metrics, alarms 5GB logs, 10 custom metrics 3GB logs (7-day retention), 15 metrics, 5 alarms $3.00 Data Transfer Inter-AZ, internet egress 100GB out to internet 30GB out (API responses, ALB traffic) $3.00 VPC, Security Groups, IAM, ACM Networking and security Free N/A $0.00 Month 1 Total: ~$31.70 Month 2+ Total: ~$67.70 6.2. Cost Optimization Strategies\rImmediate Savings (Implemented):\n✅ VPC Endpoints Instead of NAT Gateway (-$28/month)\nBefore: NAT Gateway ($32/month) + data processing ($3/month) = $35/month\nAfter: VPC Endpoints ($7/month for 3 endpoints)\nSavings: $28/month ($336/year)\nTrade-off: None—endpoints are more secure and faster\n✅ Single-Region Deployment (-$50+/month)\nBefore: Multi-region (Singapore + Sydney) with cross-region data transfer\nAfter: Single region with S3 CRR for DR only\nSavings: No second RDS instance ($15), no second ElastiCache ($12), no second ALB ($16), no cross-region data transfer ($10-20)\nTrade-off: Users outside SEA experience higher API latency (acceptable for MVP)\n✅ ECS Fargate with Minimal Resources (-$10-15/month vs larger EC2)\nConfiguration: 0.5 vCPU, 1GB RAM per task (Fargate minimum)\nSavings: Efficient resource allocation, pay only for what you use\nTrade-off: Monitor performance, scale up if needed\nAdditional Optimizations (Consider if Budget Exceeded):\nReduce ECS Task Count During Off-Peak Hours\nCurrent: 2 tasks per service (10 total) running 24/7\nOptimization: Scale down to 1 task per service during 12am-8am SGT\nPotential Savings: ~$6/month\nRisk: Reduced redundancy during off-hours\nOptimize CloudWatch Log Retention\nCurrent: 7-day retention (3GB logs)\nOptimization: Reduce to 3-day retention\nPotential Savings: ~$1-2/month\nTrade-off: Shorter log history for debugging\nUse S3 Intelligent-Tiering\nAutomatically moves infrequently accessed objects to cheaper storage tiers\nPotential Savings: $0.50-1/month\nTrade-off: Minimal retrieval delay for cold objects\nDisable RDS Multi-AZ Temporarily (NOT RECOMMENDED)\nSavings: ~$7.50/month (50% reduction)\nCRITICAL RISK: No automatic failover, accept downtime during DB failures\nUse Case: Only for development/testing environments\nRevised Budget Scenarios:\nScenario Monthly Cost Annual Cost Use Case Current (Free Tier - Month 1) $31.70 - Initial launch with Free Tier benefits Standard (Post Free Tier) $67.70 $812/year Production after Free Tier expires Optimized (Off-peak scaling) $61.70 $740/year Budget-conscious production Development (No Multi-AZ) $60.20 $722/year Testing environment only 6.3. Free Tier Monitoring \u0026 Budget Alerts\rCritical Free Tier Limits to Track:\nService Free Tier Limit Monthly Allowance Alert Threshold Monitoring Method ECS Fargate 20 GB-hours First month only 16 GB-hours (80%) CloudWatch custom metric + AWS Budgets RDS 750 hours Single-AZ only (Multi-AZ NOT covered) N/A (always paid) N/A ElastiCache 750 node-hours Monthly (cache.t2.micro or t3.micro) 700 hours (93%) CloudWatch billing metric ALB 750 hours + 15 LCUs Monthly 700 hours (93%) AWS Cost Explorer S3 5GB storage, 20k GET, 2k PUT Monthly 4GB, 18k GET, 1.8k PUT S3 Storage Lens CloudFront 1TB data transfer, 10M requests Monthly 900GB, 9M requests CloudFront usage reports Data Transfer 100GB out to internet Monthly 90GB (90%) CloudWatch billing alarm AWS Budgets Configuration:\nBudget 1: Overall Monthly Budget\nName: “Production Infrastructure Budget”\nAmount: $70/month\nAlert Thresholds:\n50% ($35) - Email to team\n80% ($56) - Email to admin + Slack notification\n100% ($70) - Email + SMS to admin\n110% ($77) - Email + trigger cost reduction script\nBudget 2: ECS Fargate Specific\nName: “ECS Fargate Monitor”\nAmount: $40/month\nFiltered by: Service = ECS\nAlert Thresholds: 80%, 100%\nBudget 3: Data Transfer Watch\nName: “Data Transfer Overage Prevention”\nAmount: $10/month\nFiltered by: Usage Type Group = Data Transfer\nAlert Thresholds: 50%, 80%, 100%\nDaily Monitoring Routine (5 minutes):\nCheck AWS Cost Explorer → Daily spend trend\nReview ECS Service metrics → Task count hasn’t scaled unexpectedly\nVerify S3 data transfer → No unusual spikes from CRR\nCheck CloudWatch alarms → No billing alerts triggered\nReview top 5 cost drivers → Identify any anomalies\nWeekly Cost Review (15 minutes):\nGenerate cost report by service (last 7 days)\nCompare to previous week → Identify trends\nReview CloudFront data transfer → Validate CDN efficiency\nCheck RDS Performance Insights → Optimize slow queries\nUpdate cost forecast for end of month\n7. Risk Assessment\r7.1. Risk Matrix\rRisk Category Specific Risk Impact Probability Priority Mitigation Strategy Cost Free Tier exhaustion before month end High High CRITICAL Daily monitoring, budget alarms at 50%/80%/100%, auto-scaling limits Cost ECS Fargate over-scaling during traffic spike High Medium HIGH Set maximum task count to 3 per service, configure target tracking conservatively Cost Unexpected data transfer charges Medium Medium MEDIUM VPC Endpoints eliminate most charges, monitor S3 CRR costs Availability RDS primary failure during peak hours High Low MEDIUM Multi-AZ enabled, automatic failover in 60-120 seconds, test monthly Availability Complete regional failure (Singapore) Critical Very Low HIGH Documented DR runbook, quarterly DR drills, maintain S3 CRR to us-east-1 Availability All ECS tasks fail after bad deployment High Medium HIGH Implement blue/green deployments, automated rollback, thorough testing in staging Performance ElastiCache eviction under load Medium Medium MEDIUM Monitor hit rate (target \u003e80%), increase instance size if needed Performance Database connection pool exhaustion Medium Medium MEDIUM Set max connections to 20 per task, monitor with Performance Insights Security Exposed RDS endpoint to internet Critical Low CRITICAL Security Group restricts to ECS tasks only, no public access, quarterly audit Security Leaked IAM credentials in code Critical Low CRITICAL Use IAM roles exclusively, never hardcode secrets, automated scanning with git-secrets Security S3 bucket misconfiguration (public access) High Low HIGH OAC enforced, S3 Block Public Access enabled, quarterly review Data Accidental database deletion High Very Low MEDIUM Enable RDS deletion protection, require MFA for destructive operations Data Data loss during regional failure Medium Very Low LOW RDS automated backups (7-day retention), test restore process monthly Operational Failed deployment with no rollback plan Medium Medium MEDIUM Document rollback procedure, keep previous 3 task definition revisions 7.2. Detailed Mitigation Plans\rCost Control Measures:\nProactive Monitoring:\nDaily Checks:\r- AWS Cost Explorer: Current spend vs forecast\r- ECS Service: Task count = expected (2 per service)\r- CloudWatch Billing: No unexpected alarms\r- S3 metrics: CRR data transfer within normal range\rWeekly Reviews:\r- Top 5 cost drivers analysis\r- Comparison to previous week\r- Free Tier usage tracking\r- Forecast adjustment for end of month\rMonthly Actions:\r- Generate detailed cost report\r- Review and optimize resource allocation\r- Update budget alerts for next month\r- Document lessons learned\rEmergency Cost Reduction Plan (Execute if \u003e100% budget):\nAction Time to Execute Monthly Savings Impact 1. Reduce ECS tasks to 1 per service 5 minutes $18 Reduced redundancy, acceptable for emergency 2. Stop ElastiCache cluster temporarily 2 minutes $12 Slower performance, users may notice 3. Disable S3 CRR temporarily 5 minutes $1 No new DR backups, existing data safe 4. Reduce CloudWatch log retention to 1 day 2 minutes $2 Limited debugging history 5. Scale down to db.t3.micro Single-AZ (risky) 10 minutes $7.50 High risk of downtime, emergency only Total Potential Savings: 24 minutes $40.50 Acceptable for 1-2 weeks while investigating Security Hardening:\nNetwork Security Configuration:\nSecurity Group: ALB-SG\rInbound:\r- Port 443 (HTTPS) from 0.0.0.0/0\r- Port 80 (HTTP) from 0.0.0.0/0 (redirect to 443)\rOutbound:\r- All traffic to ECS-Tasks-SG\rSecurity Group: ECS-Tasks-SG\rInbound:\r- Ports 8080, 8081, 8082, 9998, 9999 from ALB-SG only\r- Ports 8080, 8081, 8082, 9998, 9999 from ECS-Tasks-SG (inter-service communication)\rOutbound:\r- Port 3306 to RDS-SG\r- Port 6379 to Redis-SG\r- Port 443 to VPC Endpoints (S3, ECR, CloudWatch)\rSecurity Group: RDS-SG\rInbound:\r- Port 3306 from ECS-Tasks-SG only\rOutbound:\r- None (no outbound connections)\rSecurity Group: Redis-SG\rInbound:\r- Port 6379 from ECS-Tasks-SG only\rOutbound:\r- None\rIAM Roles Configuration:\nTask Execution Role (ECS infrastructure):\rPermissions:\r- ecr:GetDownloadUrlForLayer\r- ecr:BatchGetImage\r- logs:CreateLogStream\r- logs:PutLogEvents\rTask Role (Application permissions):\rPermissions:\r- s3:GetObject on sgutodolist-assets-sg/*\r- s3:PutObject on sgutodolist-assets-sg/uploads/*\r- ses:SendEmail for notification service\r- cloudwatch:PutMetricData for custom metrics\rSecurity Audit Checklist (Monthly):\nReview all Security Group rules for unnecessary open ports\nVerify RDS has no public accessibility\nConfirm S3 Block Public Access is enabled\nCheck IAM roles for over-permissive policies\nReview CloudTrail logs for suspicious API calls\nVerify all data at rest encryption (RDS, S3)\nConfirm SSL/TLS for all data in transit\nRotate RDS master password (quarterly)\nReview and update security group descriptions\nPerformance Optimization:\nCaching Strategy:\nData Type Cache Location TTL Eviction Policy Monitoring Metric User sessions Redis 30 minutes TTL-based Session count, hit rate \u003e90% User profiles Redis 5 minutes LRU Hit rate \u003e85% Board metadata Redis 10 minutes LRU Hit rate \u003e80% Task lists Redis 2 minutes LRU Hit rate \u003e75% Static reference data Redis 1 hour TTL-based Hit rate \u003e95% Database Query Optimization:\n-- Add indexes for common queries\rCREATE INDEX idx_tasks_board_id ON tasks(board_id);\rCREATE INDEX idx_tasks_assignee_id ON tasks(assignee_id);\rCREATE INDEX idx_tasks_status ON tasks(status);\rCREATE INDEX idx_boards_user_id ON boards(user_id);\rCREATE INDEX idx_board_members_user_id ON board_members(user_id);\r-- Monitor slow queries (\u003e500ms)\rSET GLOBAL slow_query_log = 'ON';\rSET GLOBAL long_query_time = 0.5;\rApplication-Level Optimizations:\nPagination: Max 100 items per page, default 20\nLazy Loading: Use JPA fetch = FetchType.LAZY for relationships\nConnection Pooling: HikariCP with max 20 connections per task\nResponse Compression: gzip enabled on ALB (automatic)\nAPI Rate Limiting: 100 requests/minute per user (prevent abuse)\nDisaster Recovery Testing:\nMonthly DR Drill (30 minutes):\nTest Procedure Expected Result Pass/Fail 1. ECS Task Failure Manually stop one task ALB routes traffic to healthy task, new task launches in 2-3 min 2. Cache Failure Restart Redis cluster App reconnects automatically, regenerates cache 3. Deployment Rollback Deploy bad task definition, then rollback Service returns to previous version in 3-5 min 4. RDS Snapshot Restore Restore snapshot to new instance Database accessible, data integrity verified Quarterly Full DR Exercise (2 hours):\nSimulate Complete Regional Failure\nMark all Singapore resources as “unavailable”\nDocument current RTO/RPO baselines\nExecute DR Runbook:\nStep 1: Restore RDS snapshot to us-east-1 (30 minutes)\rStep 2: Create ECS cluster in us-east-1 (10 minutes)\rStep 3: Deploy all 5 services with updated DB endpoint (20 minutes)\rStep 4: Create and configure ALB in us-east-1 (15 minutes)\rStep 5: Update Route 53 to point to new ALB (5 minutes)\rStep 6: Update CloudFront origin for API calls (10 minutes)\rStep 7: End-to-end testing (20 minutes)\rVerify Complete Functionality:\nUser can log in and access their boards\nTasks can be created, updated, deleted\nFile uploads work correctly\nNotifications are sent\nDocument Findings:\nActual RTO achieved vs target (2-4 hours)\nIssues encountered and resolutions\nUpdates needed to DR runbook\nCost of maintaining DR environment\n8. Expected Outcomes\r8.1. Technical Achievements\rPerformance Metrics:\nMetric Target Measurement Method Baseline API Response Time (p95) \u003c100ms CloudWatch custom metrics 80-120ms API Response Time (p99) \u003c300ms CloudWatch custom metrics 250-400ms Page Load Time \u003c2 seconds CloudFront + browser metrics 1.5-2.5s Cache Hit Rate \u003e80% Redis INFO stats 75-85% Database Query Time (p95) \u003c50ms RDS Performance Insights 30-70ms Availability (Monthly) 99.5% CloudWatch uptime monitoring 99.4-99.7% ECS Task Health \u003e95% ALB target health checks 97-99% Scalability Capabilities:\nCurrent Capacity: 100-500 concurrent users with 10 ECS tasks (2 per service)\nMaximum Capacity (Same Cost): 1,000 users with optimized caching and query performance\nScaling Path: Add tasks linearly (3-4 per service for 2,000+ users, ~$30/month additional)\nOperational Improvements:\nBefore (Traditional EC2) After (ECS Fargate) Improvement 4-6 hours/week server management 30 min/week monitoring 85% time savings Manual scaling decisions Automatic target tracking Zero manual intervention 5-10 minute deployments 3-5 minute rolling updates 50% faster SSH access required No server access needed Improved security Complex AMI management Simple Docker images Easier version control 8.2. Long-term Value\rSkills Development:\nCloud Architecture:\nHands-on experience with AWS core services (ECS, Fargate, RDS, ElastiCache, VPC)\nUnderstanding of High Availability patterns (Multi-AZ, health checks, auto-scaling)\nCost optimization strategies for cloud infrastructure\nTrade-off analysis: Performance vs Cost vs Availability\nContainer \u0026 Microservices:\nDocker containerization best practices\nMicroservices communication patterns (service discovery, API gateways)\nContainer orchestration with ECS Fargate\nService mesh concepts (Cloud Map for DNS-based discovery)\nDevOps Practices:\nCI/CD pipeline design and implementation\nInfrastructure monitoring and observability\nIncident response and disaster recovery\nRolling deployments and zero-downtime updates\nSecurity:\nLeast-privilege IAM policies\nNetwork segmentation with Security Groups\nEncryption at rest (RDS, S3) and in transit (TLS/SSL)\nSecurity audit and compliance practices\nPortfolio Project Value:\nDemonstrates to Employers:\n✅ Production-ready system design (not just tutorials)\n✅ Cost consciousness and budget management\n✅ Modern cloud-native architecture patterns\n✅ End-to-end project delivery (planning → implementation → testing → documentation)\n✅ Ability to work within constraints (budget, technology, time)\nInterview Talking Points:\n“Reduced operational costs by 40% while maintaining 99.5% uptime using ECS Fargate”\n“Implemented Multi-AZ disaster recovery with \u003c2 minute RTO for AZ failures”\n“Designed microservices architecture serving 500+ concurrent users on $30/month budget”\n“Achieved \u003c100ms API response times through strategic caching and query optimization”\nBusiness Foundation:\nMonetization Potential:\nCurrent architecture supports 100-500 users at $30/month\nRevenue model: $5/user/month = $500-2,500/month potential\nBreak-even: 6-10 paying users\nProfit margin: 97-99% after break-even\nScaling Roadmap:\nUsers Monthly Cost Revenue ($5/user) Profit Required Changes 100 $30 $500 $470 None (current architecture) 500 $50 $2,500 $2,450 Increase tasks to 3 per service 1,000 $80 $5,000 $4,920 Upgrade RDS to db.t3.small, add CloudWatch dashboards 5,000 $300 $25,000 $24,700 Multi-region (add ap-southeast-2), Aurora RDS, managed Redis 10,000+ $800+ $50,000+ $49,200+ Multi-region active-active, Aurora Global DB, ECS auto-scaling Career Impact:\nAWS Certification Alignment:\nCloud Practitioner: Covers 70% of services used (EC2, RDS, S3, CloudFront)\nSolutions Architect Associate: Direct experience with 80% of exam topics (VPC, IAM, HA patterns)\nSysOps Administrator: Hands-on with monitoring, scaling, cost optimization\nOpen Source Contribution:\nTemplate repository for “Single-Region HA SaaS on AWS Free Tier”\nBlog series: “Building a Production SaaS for $30/month”\nConference talk: “Cost-Optimized Cloud Architecture Patterns”\nNext Project Ideas (Building on This Foundation):\nAdd real-time collaboration with WebSockets (AWS API Gateway WebSocket)\nImplement full-text search with Amazon OpenSearch\nBuild AI-powered task recommendations with Amazon Bedrock\nAdd mobile app with AWS Amplify and GraphQL API\n9. Future Development Roadmap\rThe current Single-Region HA architecture provides a cost-optimized, production-ready foundation. As the platform grows in users and revenue, the following enhancements can be implemented incrementally.\n9.1. Phase 2: Multi-Region Expansion (6-12 months)\rWhen to Consider:\nUser base exceeds 1,000 active users\nSignificant user population outside Southeast Asia (\u003e20%)\nMonthly revenue justifies additional infrastructure cost ($300+)\nBusiness requires \u003c50ms API latency globally\nTarget Architecture:\nComponent Current (Phase 1) Future (Phase 2) Regions ap-southeast-1 (Singapore) + ap-southeast-2 (Sydney), us-west-2 (Oregon) Routing Route 53 (single endpoint) Route 53 Latency-Based Routing with health checks Database RDS Multi-AZ (single region) Amazon Aurora Global Database (multi-region) Compute ECS Fargate (Singapore only) ECS Fargate clusters in each region Caching ElastiCache (Singapore) Regional ElastiCache clusters + Global Datastore Storage S3 CRR (passive DR) S3 Multi-Region Access Points (active-active) Estimated Cost $30-70/month $200-300/month Benefits:\n✅ True global low latency (\u003c50ms for 95% of users)\n✅ Enhanced disaster recovery (automatic failover between regions)\n✅ Geographic compliance (data residency for EU, APAC, US)\n✅ Improved read performance (local read replicas)\n9.2. Phase 3: Enterprise Features (12-18 months)\rAdvanced Security:\nAWS Cognito for SSO/SAML integration\nAWS WAF for API protection against attacks\nAWS Shield Standard for DDoS protection\nAWS GuardDuty for threat detection\nObservability \u0026 Analytics:\nAWS X-Ray for distributed tracing\nAmazon OpenSearch for log analytics\nCustom CloudWatch Dashboards for business metrics\nAWS Cost Explorer API for automated cost reporting\nPerformance Optimization:\nAmazon ElastiCache for Redis with Cluster Mode (horizontal scaling)\nAmazon Aurora Serverless v2 (auto-scaling database)\nAWS Global Accelerator for improved network performance\nCloudFront Functions for edge computing\nOperational Excellence:\nAWS Systems Manager for parameter management\nAWS Secrets Manager for credential rotation\nInfrastructure as Code with Terraform or CDK\nAutomated DR testing with AWS Backup\n9.3. Phase 4: AI/ML Integration (18-24 months)\rAmazon Bedrock for AI-powered task recommendations\nAmazon SageMaker for predictive analytics (task completion time)\nAmazon Comprehend for sentiment analysis on comments\nAmazon Rekognition for smart image tagging in attachments\n10. Conclusion\rThis Cost-Optimized SaaS Task Management Platform demonstrates that production-grade applications can be built within AWS Free Tier constraints without sacrificing quality or reliability.\nKey Achievements:\n✅ 99.5% availability with Multi-AZ deployment\n✅ \u003c$30/month operational cost in first month\n✅ Zero server management with ECS Fargate\n✅ \u003c100ms API response times with strategic caching\n✅ Disaster recovery with S3 Cross-Region Replication\nCompetitive Advantages:\nModern cloud-native architecture ready for enterprise scale\nComprehensive documentation and operational runbooks\nClear scaling path from 100 to 100,000+ users\nFoundation for future AI/ML features\nNext Steps:\nComplete Phase 1 implementation (4-6 weeks)\nDeploy to production and gather metrics (2-3 months)\nDocument lessons learned and optimize based on real usage\nEvaluate Phase 2 expansion based on user growth and feedback\nThis architecture provides a solid foundation for learning, career development, and potential monetization while maintaining strict cost discipline and operational excellence.",
    "description": "Cost-Optimized SaaS Task Management Platform\rSingle-Region High Availability with ECS Fargate\r1. Executive Summary\rThe SaaS Task Management Platform is designed to deliver a Todoist-like collaborative experience with High Availability (HA) and Cost Efficiency, specifically optimized for AWS Free Tier constraints.",
    "tags": [],
    "title": "Proposal",
    "uri": "/en/2-proposal/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Workshop \u003e Deploy Flow \u003e Frontend Deploy",
    "content": "PHASE 1: STORAGE PREPARATION (S3 \u0026 REPLICATION)\rThis phase focuses on creating a durable storage layer for frontend resources and setting up an automated cross-region synchronization mechanism.\nStep 1.1: Create the Primary Bucket (Singapore)\rOpen the Amazon S3 Console and select the Asia Pacific (Singapore) region.\nClick Create bucket.\nBucket name: sgutodolist-frontend-sg Object Ownership: ACLs disabled (Recommended) Block Public Access: ✅ Block all public access\n(The bucket must remain private because CloudFront OAC will be used.) Bucket Versioning: ✅ Enable\n(Required for cross-region replication.) Default encryption: Server-side encryption with Amazon S3 managed keys (SSE-S3) Click Create bucket.\nStep 1.2: Create the Secondary Bucket (N. Virginia)\rSwitch the region to US East (N. Virginia).\nClick Create bucket.\nBucket name: sgutodolist-frontend-us Block Public Access: ✅ Block all public access Bucket Versioning: ✅ Enable Click Create bucket.\nAfter completing Step 1.1 and Step 1.2, two S3 buckets should be available:\nPrimary bucket: Singapore (ap-southeast-1) Secondary bucket: N. Virginia (us-east-1) Step 1.3: Configure Replication (Automatic Sync)\rGo back to the Singapore bucket (sgutodolist-frontend-sg).\nNavigate to Management → Replication rules → Click Create replication rule.\nRule name: SyncToUS Status: Enabled Source bucket: Apply to all objects in the bucket Destination: Choose a bucket in this account\n→ Select sgutodolist-frontend-us\n(Ensure that the region us-east-1 is selected so the bucket is visible.) IAM Role: Select Create new role\n(AWS will automatically generate the required permissions.) Click Save.\nWhen prompted with “Replicate existing objects?”, select No\n(The bucket is currently empty.)\n⬅ STEP 1: Prerequisites\rSTEP 3: Route 53 and ACM ➡",
    "description": "PHASE 1: STORAGE PREPARATION (S3 \u0026 REPLICATION)\rThis phase focuses on creating a durable storage layer for frontend resources and setting up an automated cross-region synchronization mechanism.\nStep 1.1: Create the Primary Bucket (Singapore)\rOpen the Amazon S3 Console and select the Asia Pacific (Singapore) region.",
    "tags": [],
    "title": "S3 and Replication",
    "uri": "/en/5-workshop/5.3-deploy_flow/5.3.1-frontend-deploy/5.3.1.2-s3-and-replication/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Worklog",
    "content": "Week 2 Objectives\rStudy Module 1 practicing VPC and EC2 Learn and practice configure VPC, EC2 and their related features Assign tasks for team project Update worklog structure Tasks to be carried out this week\rDay Task Start Date Completion Date Reference Material 1 - Learn about VPC and its feature 15/09/2025 15/09/2025 VPC, Youtube Lesson 2 - Practice creating VPC, Subnets, Internet Gateway 16/09/2025 16/09/2025 Create VPC, Create Subnets, Create Internet Gateway - Update worklog 3 - Practice creating Route table, Security groups 17/09/2025 17/09/2025 Create Route Table, Create Security Groups - Decide project scopes, assgign tasks 4 - Learn about Deploying Amazon EC2 Instances 18/09/2025 18/09/2025 Deploying Amazon EC2 Instances, Youtube Tutorial - Practice creating EC2 server, Checking connection, creating NAT Gateway Create EC2 instance, Check connection, Create NAT Gateway 5 - Practice using Reachability Analyzer 19/09/2025 19/09/2025 Create Reachability Analyzer - Team meeting to decide proposal project - Update worklog",
    "description": "Week 2 Objectives\rStudy Module 1 practicing VPC and EC2 Learn and practice configure VPC, EC2 and their related features Assign tasks for team project Update worklog structure Tasks to be carried out this week\rDay Task Start Date Completion Date Reference Material 1 - Learn about VPC and its feature 15/09/2025 15/09/2025 VPC, Youtube Lesson 2 - Practice creating VPC, Subnets, Internet Gateway 16/09/2025 16/09/2025 Create VPC, Create Subnets, Create Internet Gateway - Update worklog 3 - Practice creating Route table, Security groups 17/09/2025 17/09/2025 Create Route Table, Create Security Groups - Decide project scopes, assgign tasks 4 - Learn about Deploying Amazon EC2 Instances 18/09/2025 18/09/2025 Deploying Amazon EC2 Instances, Youtube Tutorial - Practice creating EC2 server, Checking connection, creating NAT Gateway Create EC2 instance, Check connection, Create NAT Gateway 5 - Practice using Reachability Analyzer 19/09/2025 19/09/2025 Create Reachability Analyzer - Team meeting to decide proposal project - Update worklog",
    "tags": [],
    "title": "Week 2 Worklog",
    "uri": "/en/1-worklog/1.2-week_2/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Translated Blogs",
    "content": "Khắc phục sự cố Môi trường Elastic Beanstalk bằng Amazon Q Developer CLI\rTác giả: Adarsh Suresh, Chandu Utlapalli – 29/7/2025\nChủ đề: Amazon Q Developer, AWS Elastic Beanstalk, Technical How-to\nGiới thiệu\rCác nhà phát triển làm việc với AWS nhận thấy AWS Elastic Beanstalk là một dịch vụ vô giá giúp việc triển khai và chạy các ứng dụng web trở nên đơn giản mà không cần phải lo lắng về cơ sở hạ tầng nền tảng (underlying infrastructure). Bạn chỉ cần tải mã ứng dụng của mình lên, và Elastic Beanstalk sẽ tự động xử lý các chi tiết về cấp phát dung lượng (capacity provisioning), cân bằng tải (load balancing), điều chỉnh quy mô (scaling), và giám sát (monitoring), cho phép bạn tập trung vào việc viết code.\nVới việc phát hành CLI agent mới được tăng cường của Amazon Q Developer, chúng ta đã thấy cách Q CLI có thể được sử dụng để chuyển đổi phương pháp tiếp cận quy trình phát triển phần mềm.\nNgoài phát triển phần mềm, các nhà phát triển và đội ngũ DevOps có thể dành phần lớn thời gian của họ cho các tác vụ vận hành (operational tasks) như triển khai và kiểm thử mã của họ trên nhiều môi trường, bao gồm cả việc khắc phục sự cố các lỗi liên quan đến triển khai (deployment related failures) hoặc các vấn đề về tình trạng ứng dụng (application health issues). Các tính năng dựa trên tác nhân (agentic features) mới của Q CLI có thể được sử dụng để đơn giản hóa đáng kể quy trình này bằng cách giúp bạn xác định và giải quyết các vấn đề vận hành theo cách hiệu quả hơn.\nKhi khắc phục sự cố môi trường Elastic Beanstalk, Q CLI trở thành người bạn đồng hành không thể thiếu. Khi các môi trường hiển thị tình trạng sức khỏe bị suy giảm (degraded health) hoặc lỗi triển khai (deployment failures), các nhà phát triển có thể sử dụng Q CLI để nhanh chóng điều tra mà không cần phải điều hướng qua nhiều trang AWS console hoặc phân tích nhiều logs thủ công.\nVí dụ, khi đối mặt với lỗi triển khai, bạn có thể chạy q chat để bắt đầu một cuộc trò chuyện mới và mô tả vấn đề. Q CLI có thể giúp phân tích instance logs, kiểm tra cấu hình môi trường (environment configurations), và xác định các cấu hình sai (misconfigurations) trong ứng dụng. Nó có thể lấy các thông báo lỗi liên quan từ Elastic Beanstalk logs và đề xuất các biện pháp khắc phục cụ thể dựa trên các mẫu lỗi mà nó nhận ra.\nKhi giải quyết các vấn đề về tình trạng sức khỏe (health issues), các nhà phát triển có thể yêu cầu Q CLI kiểm tra trạng thái môi trường, mức sử dụng tài nguyên (resource utilization) và các sự kiện gần đây. Nó có thể xác định xem một ứng dụng có đang gặp vấn đề thiếu bộ nhớ (out of memory problems), vấn đề kết nối (connectivity issues), hay lỗi liên quan đến dependency hay không. Q CLI cũng có thể kiểm tra application logs để tìm các lỗi lặp lại có thể gây ra suy giảm tình trạng sức khỏe (health degradation).\nĐiều mà các nhà phát triển đánh giá cao nhất là cách Q CLI kết nối các điểm giữa các dịch vụ AWS khác nhau. Nếu một môi trường Elastic Beanstalk gặp sự cố do vấn đề cấu hình Amazon VPC cơ bản hoặc vấn đề quyền Amazon S3, Q CLI có thể xác định các kết nối này và cung cấp các giải pháp toàn diện (holistic solutions).\nViệc tiết kiệm thời gian là rất đáng kể – những gì trước đây mất hàng giờ điều tra trên nhiều trang AWS console giờ đây chỉ mất vài phút với các truy vấn Q CLI có mục tiêu. Điều này đã cải thiện đáng kể khả năng của các nhà phát triển trong việc duy trì các môi trường khỏe mạnh và nhanh chóng giải quyết các vấn đề khi chúng phát sinh.\nDưới đây, chúng tôi sẽ hướng dẫn bạn một số ví dụ về cách bạn có thể sử dụng Q CLI để khắc phục một số sự cố mà bạn có thể gặp phải khi quản lý môi trường Elastic Beanstalk.\nHướng dẫn Giải pháp\rĐiều kiện Tiên quyết\nNếu bạn muốn làm theo trên máy tính của riêng mình, vui lòng đảm bảo bạn hoàn thành các điều kiện tiên quyết sau:\nMột tài khoản AWS có quyền truy cập Elastic Beanstalk Sự quen thuộc cơ bản với các khái niệm Elastic Beanstalk (environments, applications, deployments) AWS CLI được cài đặt và cấu hình với các quyền thích hợp để truy cập tài nguyên Elastic Beanstalk, và thu thập logs AWS Q Developer CLI được cài đặt và thiết lập EB CLI được cài đặt và thiết lập (tùy chọn) Các môi trường web server Elastic Beanstalk đã được tạo để khắc phục sự cố Bây giờ chúng ta hãy đi sâu vào việc khắc phục các sự cố Elastic Beanstalk cụ thể với Q CLI. Tất cả các kịch bản dưới đây đều được kiểm tra với Amazon Q Developer CLI bằng gói đăng ký Pro tier vì nó cung cấp giới hạn yêu cầu cao hơn (higher request limits), nhưng điều này không bắt buộc cho mục đích của bản demo này. Khắc phục sự cố tình trạng môi trường\nHãy xem xét một môi trường Elastic Beanstalk đang chạy Node.js 22 trên Amazon Linux 2023, nơi chúng ta sẽ triển khai một phiên bản ứng dụng mới. Sau khi triển khai một phiên bản ứng dụng mới vào môi trường Elastic Beanstalk dựa trên Node.js của chúng ta, chúng tôi nhận thấy rằng tình trạng sức khỏe (health status) của nó đã chuyển sang trạng thái “Warning” (Cảnh báo) với thông báo sau hiển thị trong các sự kiện môi trường (environment events):\n100% of requests failing with HTTP 5xx errors\rHình 1. Bảng điều khiển EB hiển thị trạng thái sức khỏe Cảnh báo, cùng với lý do cho trạng thái sức khỏe\rThông báo sự kiện này có thể là kết quả của một số vấn đề, bao gồm nhưng không giới hạn ở lỗi ứng dụng Node.js, sự cố cấu hình reverse proxy, vấn đề sử dụng tài nguyên (resource utilization issues) v.v..\nHãy sử dụng Q CLI để giúp chúng ta điều tra thêm. Chúng ta sẽ bắt đầu một cuộc trò chuyện mới với agent bằng cách chạy q chat, và hỏi câu hỏi sau:\nWhy is my beanstalk environment nodejs-app in us-east-1 unhealthy? Check the logs if required, and recommend steps to resolve the issue (Tại sao môi trường beanstalk nodejs-app của tôi ở us-east-1 không khỏe mạnh? Kiểm tra các logs nếu cần, và đề xuất các bước để giải quyết vấn đề)",
    "description": "Khắc phục sự cố Môi trường Elastic Beanstalk bằng Amazon Q Developer CLI\rTác giả: Adarsh Suresh, Chandu Utlapalli – 29/7/2025\nChủ đề: Amazon Q Developer, AWS Elastic Beanstalk, Technical How-to\nGiới thiệu\rCác nhà phát triển làm việc với AWS nhận thấy AWS Elastic Beanstalk là một dịch vụ vô giá giúp việc triển khai và chạy các ứng dụng web trở nên đơn giản mà không cần phải lo lắng về cơ sở hạ tầng nền tảng (underlying infrastructure). Bạn chỉ cần tải mã ứng dụng của mình lên, và Elastic Beanstalk sẽ tự động xử lý các chi tiết về cấp phát dung lượng (capacity provisioning), cân bằng tải (load balancing), điều chỉnh quy mô (scaling), và giám sát (monitoring), cho phép bạn tập trung vào việc viết code.",
    "tags": [],
    "title": "Blog 3",
    "uri": "/en/3-translated_blogs/blog_3/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Workshop \u003e Deploy Flow \u003e Backend Deploy",
    "content": "This phase focuses on adapting the application source code for the cloud environment. We will configure Cross-Origin Resource Sharing (CORS) for the reactive API Gateway, refactor application configurations to use dynamic environment variables, and finally build and push all microservice Docker images to the Amazon Elastic Container Registry (ECR).\nObjectives:\nConfigure the code to work seamlessly in both Local (Docker Compose) and Cloud (AWS ECS) environments.\nCreate Repositories on AWS ECR.\nBuild and Push all 6 Docker images to AWS.\n1. CREATE REPOSITORIES ON AWS ECR\rBefore pushing images, you must create a “repository” for each service. Run the following commands in your Terminal (PowerShell or Git Bash):\n# Auth Service aws ecr create-repository --repository-name auth-service --region ap-southeast-1 # User Service aws ecr create-repository --repository-name user-service --region ap-southeast-1 # Taskflow Service aws ecr create-repository --repository-name taskflow-service --region ap-southeast-1 # Notification Service aws ecr create-repository --repository-name notification-service --region ap-southeast-1 # API Gateway aws ecr create-repository --repository-name api-gateway --region ap-southeast-1 # AI Model Service aws ecr create-repository --repository-name ai-model-service --region ap-southeast-1\r2. BUILD AND PUSH IMAGES\rExecute these commands from the root directory of your project (todolist-backend).\n1. Login Docker to AWS: (Replace 031133710884 with your AWS Account ID if it’s different)\naws ecr get-login-password --region ap-southeast-1 | docker login --username AWS --password-stdin 031133710884.dkr.ecr.ap-southeast-1.amazonaws.com\r2. Build \u0026 Push each of the 6 Services:\nService 1: API Gateway\ncd api-gateway docker build -t api-gateway:latest . docker tag api-gateway:latest 031133710884.dkr.ecr.ap-southeast-1.amazonaws.com/api-gateway:latest docker push 031133710884.dkr.ecr.ap-southeast-1.amazonaws.com/api-gateway:latest cd ..\rService 2: Auth Service\ncd auth-service docker build -t auth-service:latest . docker tag auth-service:latest 031133710884.dkr.ecr.ap-southeast-1.amazonaws.com/auth-service:latest docker push 031133710884.dkr.ecr.ap-southeast-1.amazonaws.com/auth-service:latest cd ..\rService 3: User Service\ncd user-service docker build -t user-service:latest . docker tag user-service:latest 031133710884.dkr.ecr.ap-southeast-1.amazonaws.com/user-service:latest docker push 031133710884.dkr.ecr.ap-southeast-1.amazonaws.com/user-service:latest cd ..\rService 4: Taskflow Service\ncd taskflow-service docker build -t taskflow-service:latest . docker tag taskflow-service:latest 031133710884.dkr.ecr.ap-southeast-1.amazonaws.com/taskflow-service:latest docker push 031133710884.dkr.ecr.ap-southeast-1.amazonaws.com/taskflow-service:latest cd ..\rService 5: Notification Service\ncd notification-service docker build -t notification-service:latest . docker tag notification-service:latest 031133710884.dkr.ecr.ap-southeast-1.amazonaws.com/notification-service:latest docker push 031133710884.dkr.ecr.ap-southeast-1.amazonaws.com/notification-service:latest cd ..\rService 6: AI Model Service (Note: The folder name is model)\ncd model docker build -t ai-model-service:latest . docker tag ai-model-service:latest 031133710884.dkr.ecr.ap-southeast-1.amazonaws.com/ai-model-service:latest docker push 031133710884.dkr.ecr.ap-southeast-1.amazonaws.com/ai-model-service:latest cd ..\rEXPECTED RESULT\rAfter running all the commands, you can verify by running:\naws ecr describe-repositories --region ap-southeast-1\rIf you see a list of 6 repositories and no errors during the push process, you have successfully completed this phase.\nYou can now proceed to creating the Task Definitions.\n⬅ STEP 2: Infrastructure \u0026 ALB Setup\rSTEP 4: Task Definitions Creation ➡",
    "description": "This phase focuses on adapting the application source code for the cloud environment. We will configure Cross-Origin Resource Sharing (CORS) for the reactive API Gateway, refactor application configurations to use dynamic environment variables, and finally build and push all microservice Docker images to the Amazon Elastic Container Registry (ECR).\nObjectives:\nConfigure the code to work seamlessly in both Local (Docker Compose) and Cloud (AWS ECS) environments.\nCreate Repositories on AWS ECR.",
    "tags": [],
    "title": "Code Update \u0026 Image Build (Create new Docker Image)",
    "uri": "/en/5-workshop/5.3-deploy_flow/5.3.2-backend-deploy/5.3.2.3-code-update--image-build-create-new-docker-image/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Workshop",
    "content": "Table of Contents\rThis section provides step-by-step guidance for deploying both Frontend and Backend components onto the cloud infrastructure.\nIt covers the complete deployment workflow, from build artifacts preparation to production-ready release on AWS services.\n5.3.1. Frontend Deploy\nGuidelines for building, configuring, and deploying the frontend to Amazon S3 + CloudFront with cross-region failover.\n5.3.2. Backend Deploy\nInstructions for deploying backend microservices to AWS ECS/Fargate, including network configuration, load balancing, and service integration.\nBy the end of this section, the application will be fully deployed on AWS with a secure, scalable, and cost-optimized architecture.",
    "description": "Table of Contents\rThis section provides step-by-step guidance for deploying both Frontend and Backend components onto the cloud infrastructure.\nIt covers the complete deployment workflow, from build artifacts preparation to production-ready release on AWS services.\n5.3.1. Frontend Deploy\nGuidelines for building, configuring, and deploying the frontend to Amazon S3 + CloudFront with cross-region failover.\n5.3.2. Backend Deploy\nInstructions for deploying backend microservices to AWS ECS/Fargate, including network configuration, load balancing, and service integration.",
    "tags": [],
    "title": "Deploy Flow",
    "uri": "/en/5-workshop/5.3-deploy_flow/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Workshop \u003e Deploy Flow \u003e Frontend Deploy",
    "content": "PHASE 2: DOMAIN \u0026 SECURITY PREPARATION (ROUTE 53 \u0026 ACM)\rBefore creating the CloudFront distribution, the domain name system (DNS) and SSL certificates must be properly prepared.\nStep 2.1: Create a Hosted Zone (If Not Exists)\rNavigate to Amazon Route 53 → Hosted zones.\nIf the domain sgutodolist.com does not exist in the list:\nClick Create hosted zone. Domain name: sgutodolist.com Type: Public hosted zone Click Create. Update Name Servers at the domain provider\n(This project uses a domain purchased from a third-party registrar.)\nAfter opening the newly created Hosted Zone, you will see four Name Servers in the following format: ns-1538.awsdns-00.co.uk. ns-1374.awsdns-43.org. ns-172.awsdns-21.com. ns-547.awsdns-04.net.\nyaml Copy code\nCopy all four Name Servers and update them in the domain management panel of your domain registrar. Step 2.2: Request an SSL Certificate (ACM) – IMPORTANT\r⚠️ IMPORTANT NOTE:\nSSL certificates used with Amazon CloudFront MUST be created in the US East (N. Virginia) region.\nSwitch the AWS Console region to US East (N. Virginia).\nGo to AWS Certificate Manager (ACM) → Request certificate.\nSelect Request a public certificate → Next.\nDomain names:\nsgutodolist.com *.sgutodolist.com\n(Wildcard domain for subdomains such as www.) Validation method: DNS validation (Recommended).\nClick Request.\nIn the Certificates list, click the newly created certificate\n(Status: Pending validation).\nUnder the Domains section, click Create records in Route 53.\nClick Create records.\nWait a few minutes until the certificate status changes to Issued (Green).\n⬅ STEP 2: S3 and Replication\rSTEP 4: CloudFront and Failover ➡",
    "description": "PHASE 2: DOMAIN \u0026 SECURITY PREPARATION (ROUTE 53 \u0026 ACM)\rBefore creating the CloudFront distribution, the domain name system (DNS) and SSL certificates must be properly prepared.\nStep 2.1: Create a Hosted Zone (If Not Exists)\rNavigate to Amazon Route 53 → Hosted zones.",
    "tags": [],
    "title": "Route 53 and ACM",
    "uri": "/en/5-workshop/5.3-deploy_flow/5.3.1-frontend-deploy/5.3.1.3-route-53-and-acm/index.html"
  },
  {
    "breadcrumb": "Internship Report",
    "content": "Blog 1: Một giải pháp có khả năng mở rộng cao cho việc sao chép dữ liệu, sử dụng Amazon FSx for NetApp ONTAP và NetApp SnapMirror.\nBlog 2: Tăng tốc Đổi mới Hàng không Vũ trụ: High Performance Computing (HPC) trên Amazon Web Services (AWS)\nBlog 3: Khắc phục sự cố Môi trường Elastic Beanstalk bằng Amazon Q Developer CLI\nBlog 4: Triển khai liên tục dựa trên phương pháp GitOps với ArgoCD và EKS bằng cách sử dụng ngôn ngữ tự nhiên\nBlog 5: Ra Mắt Strands Agents 1.0: Việc điều phối Multi-Agent cho môi trường production đã được đơn giản hóa\nBlog 6: Kích hoạt phân tích dữ liệu Genomic và Multiomic nhanh chóng với Illumina DRAGEN™ v4.4 trên các instance Amazon EC2 F2\nBlog 7: Những hiểu biết sâu sắc và bài học kinh nghiệm từ Amazon Q trong tích hợp trình thu thập thông tin web Connect\nBlog 8: Xây dựng hệ thống đa tenant resilient với hàng đợi công bằng Amazon SQS\nBlog 9: Amazon Braket ra mắt bộ xử lý lượng tử siêu dẫn 54-qubit IQM Emerald\nBlog 10: Empower đã mở rộng quy mô đảm bảo chất lượng trung tâm liên hệ như thế nào với Amazon Connect và Amazon Bedrock\nBlog 11: Làm thế nào để quản lý Bot AI bằng AWS WAF và tăng cường bảo mật\nBlog 12: Hỗ trợ tùy chỉnh trên quy mô lớn: Biến một KB (Cơ sở kiến thức) Salesforce hợp nhất thành các tác tử AI tập trung vào LOB (Ngành kinh doanh)",
    "description": "Blog 1: Một giải pháp có khả năng mở rộng cao cho việc sao chép dữ liệu, sử dụng Amazon FSx for NetApp ONTAP và NetApp SnapMirror.\nBlog 2: Tăng tốc Đổi mới Hàng không Vũ trụ: High Performance Computing (HPC) trên Amazon Web Services (AWS)\nBlog 3: Khắc phục sự cố Môi trường Elastic Beanstalk bằng Amazon Q Developer CLI\nBlog 4: Triển khai liên tục dựa trên phương pháp GitOps với ArgoCD và EKS bằng cách sử dụng ngôn ngữ tự nhiên",
    "tags": [],
    "title": "Translated Blogs",
    "uri": "/en/3-translated_blogs/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Worklog",
    "content": "Week 3 Objectives\rStudy Module 2 in the First Cloud Journey Bootcamp – 2025 playlist Learn and get familiar with Route 53, CloudFormation and their related features Design the database and set up Backend \u0026 Frontend Update worklog structure Tasks to be carried out this week\rDay Task Start Date Completion Date Reference Material 1 - Learn about Route 53 and its feature 22/09/2025 22/09/2025 Route 53 2 - Practice creating Key pair, CloudFormation Template, Configure Security Group 23/09/2025 23/09/2025 Create Key pair, Initialize CloudFormation Template 3 - Practice creating Key pair, CloudFormation Template, Configure Security Group 24/09/2025 24/09/2025 Create Key pair, Initialize CloudFormation Template, Configure Security Group - Team meeting and discuss about Database design, Backend, Frontend 4 - Practice connecting to Remote Desktop Gateway (RDGW) by Remote Desktop Protocol (RDP) 25/09/2025 25/09/2025 Connecting to RDGW, Youtube tutorial - Design solution architecture - Team meeting to discuss about workshop project 5 - Practice deploy Microsoft Active Directory 26/09/2025 26/09/2025 Deploy Microsoft Active Directory - Practice deploy Microsoft Active Directory Deploy Microsoft Active Directory - Pratice configure DNS Configure DNS - Update worklog",
    "description": "Week 3 Objectives\rStudy Module 2 in the First Cloud Journey Bootcamp – 2025 playlist Learn and get familiar with Route 53, CloudFormation and their related features Design the database and set up Backend \u0026 Frontend Update worklog structure Tasks to be carried out this week\rDay Task Start Date Completion Date Reference Material 1 - Learn about Route 53 and its feature 22/09/2025 22/09/2025 Route 53 2 - Practice creating Key pair, CloudFormation Template, Configure Security Group 23/09/2025 23/09/2025 Create Key pair, Initialize CloudFormation Template 3 - Practice creating Key pair, CloudFormation Template, Configure Security Group 24/09/2025 24/09/2025 Create Key pair, Initialize CloudFormation Template, Configure Security Group - Team meeting and discuss about Database design, Backend, Frontend 4 - Practice connecting to Remote Desktop Gateway (RDGW) by Remote Desktop Protocol (RDP) 25/09/2025 25/09/2025 Connecting to RDGW, Youtube tutorial - Design solution architecture - Team meeting to discuss about workshop project 5 - Practice deploy Microsoft Active Directory 26/09/2025 26/09/2025 Deploy Microsoft Active Directory - Practice deploy Microsoft Active Directory Deploy Microsoft Active Directory - Pratice configure DNS Configure DNS - Update worklog",
    "tags": [],
    "title": "Week 3 Worklog",
    "uri": "/en/1-worklog/1.3-week_3/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Translated Blogs",
    "content": "Triển khai liên tục dựa trên phương pháp GitOps với ArgoCD và EKS bằng cách sử dụng ngôn ngữ tự nhiên\rJagdish Komakula, Aditya Ambati và Anand Krishna Varanasi | 17/07/2025 | Amazon Elastic Kubernetes Service, Amazon Q, Amazon Q Developer, Developer Tools, Technical How-to | Permalink\nGiới thiệu\rArgoCD là một bộ công cụ GitOps hàng đầu giúp các nhóm quản lý việc triển khai Kubernets một cách khai báo, sử dụng Git là nguồn thông tin đáng tin cậy duy nhất. Bộ tính năng mạnh mẽ của nó bao gồm hệ thống đồng bộ tự động, hỗ trợ khôi phục, phát hiện sai lệch, chiến lược triển khai nâng cao, Tích hợp RBAC, và hỗ trợ đa cụm (multi-cluster), khiến nó trở thành giải pháp được ưa chuộng cho việc triển khai ứng dụng trên Kubernetes. Tuy nhiên, khi các tổ chức mở rộng quy mô, một vài điểm khó khăn và thử thách vận hành bắt đầu xuất hiện.\nĐiểm khó khăn khi sử dụng ArgoCD theo phương pháp truyền thống\rGiao diện của ArgoCD và CLI được thiết kế cho người dùng có nền tảng công nghệ kỹ thuật chuyên sâu. Việc tích hợp tương tác với manifests YAML, hiểu mối quan hệ giữa các tài nguyên Kubernets, và việc khắc phục lỗi đồng bộ đòi hỏi kiến thức chuyên môn cao. Điều này hạn chế khả năng tiếp cập vào quy trình GitOps đối với các bên liên quan ít am hiểu công nghệ và làm tăng sự phụ thuộc vào các kỹ sư DevOps.\nViệc quản lý ArgoCD trên nhiều cụm (clusters) hoặc nhiều môi trường (environments) (sử dụng mô hình hub-spoke, per-cluster, hoặc grouped) tạo ra sự phức tạp đáng kể trong việc vận hành. Các nhóm phải xử lý nhiều instance ArgoCD, duy trì cấu hình nhất quán, và điều phối các triển khai, điều này có thể trở thành một nút thắt khi các quy mô dịch vụ ngày càng mở rộng.\nArgoCD vượt trội trong việc đồng bộ và giám sát các tài nguyên Kubernetes nhưng thiếu cơ chế tích hợp sẵn cho các tác vụ tiền triển khai (ví dụ: quét hình ảnh) và hậu triển khai (ví dụ: việc kiểm tra tải). Điều này khiến các nhóm phải dựa vào các bộ công cụ bên ngoài hoặc các script tùy chỉnh, khiến quy trình triển khai bị phân mảnh và tăng thêm gánh nặng bảo trì.\nViệc chuyển ứng dụng giữa các môi trường (Dev -\u003e Test -\u003e Prod) không được tinh gọn một cách tự nhiên. Các nhóm phải tự mình điều phối thủ công hoặc viết script cho các quy trình chuyển đổi này, việc này làm chậm việc triển khai các bản vá lỗi khẩn cấp và làm phức tạp hóa quá trình phát hành.\nKhi các tổ chức áp dụng các chiến lược đa cụm (multi-cluster), việc quản lý quyền truy cập, RBAC và khả năng hiển thị tài nguyên của ArgoCD trên nhiều môi trường trở nên công kềnh, thường dẫn tới việc làm phân mảnh quy trình làm việc và tạo ra các lỗ hổng bảo mật tiềm ẩn.\nCách ArgoCD MCP Server cùng với Amazon Q CLI giải quyết các vấn đề trên:\rViệc tích hợp máy chủ ArgoCD MCP với Amazon Q CLI về cơ bản đã thay đổi trải nghiệm người dùng bằng cách giới thiệu tương tác bằng ngôn ngữ tự nhiên cho các thao tác GitOps.\nVới MCP, người dùng có thể quản lý việc triển khai, giám sát trạng thái ứng dụng, và thực hiện thao tác đồng bộ hóa và khôi phục việc vận hành bằng cách sử dụng sử dụng ngôn ngữ giao tiếp thông thường thay vì các câu lệnh kỹ thuật (commands) hoặc YAML. Ví dụ, một người dùng có thể hỏi đơn giản là: “Những ứng dụng nào đang không đồng bộ ở môi trường production?” hay “Đồng bộ ứng dụng api-service,” và hệ thống sẽ thực hiện ngầm các lệnh gọi API ArgoCD phù hợp.\nĐiều này giúp dân chủ hóa quyền truy cập vào GitOps, cho phép các thành viên trong nhóm ít chuyên kỹ thuật (như QA, các người quản lý sản phẩm hoặc kỹ sư hỗ trợ) có thể tương tác an toàn với các quy trình triển khai.\nGiao diện ngôn ngữ tự nhiên giúp loại bỏ độ phức tạp của việc quản lý đa cụm (multi-cluster) và đa môi trường (multi-environment). Người dùng có thể truy vấn hoặc thực hiện hành động trên các tài nguyên giữa các cụm mà không cần ghi nhớ tên tài nguyên, namespace hoặc endpoint API.\nMCP Server xử lý việc xác thực, quản lý session và xử lý lỗi mạnh mẽ, giảm nhu cầu khắc phục sự cố thủ công và viết script tùy chỉnh.\nViệc tích hợp cung cấp các phản hồi chi tiết, xử lý các endpoint thông minh, và các thông báo lỗi toàn diện, giúp cho việc chẩn đoán và giải quyết vấn đề trở nên dễ dàng hơn. Việc kiểm tra kiểu tĩnh hoàn toàn và cấu hình theo môi trường còn nâng cao thêm độ tin cậy và khả năng bảo trì.\nBằng cách tận dụng khả năng mở rộng của Amazon Q CLI, người dùng được tiếp cận với các tích hợp sẵn và gợi ý theo ngữ cảnh, từ đó đẩy nhanh quy trình phát triển và triển khai.\nMCP Server cho phép các trợ lý AI và mô hình ngôn ngữ tự động hóa các tác vụ định kỳ, đề xuất hành động và thậm chí gỡ lỗi (debug) các vấn đề, hoạt động như một kỹ sư DevOps ảo. Điều này có thể giảm đáng kể công sức thủ công và tăng tốc độ phản hồi sự cố.\nSo sánh ArgoCD truyền thống với ArgoCD MCP Server và Amazon Q CLI\rTính năng/Thách thức ArgoCD truyền thống MPC Server + Amazon Q CLI Giao diện người dùng Giao diện kỹ thuật (UI/CLI), yêu cầu thao tác với YAML Ngôn ngữ tự nhiên, tương tác hội thoại Khả năng truy cập cho người không phải kỹ sư Hạn chế Mở rộng, dân chủ hóa Quản lý đa cụm Phức tạp, thủ công Đơn giản hóa, được trừu tượng hóa Tác vụ Tiền/Hậu Triển khai Cần công cụ bên ngoài/script Vẫn dùng công cụ bên ngoài, nhưng dễ gọi hơn Chuyển đổi ứng dụng Thủ công hoặc dùng script Ngôn ngữ tự nhiên, điều phối dễ dàng hơn Khắc phục sự cố Mang tính kỹ thuật, dễ xảy ra lỗi Được hướng dẫn, có hỗ trợ AI, phản hồi chi tiết Tự động hóa Yêu cầu viết script Do AI/agent điều khiển, chủ động Bạn có thể thực hiện các thao tác sau bằng ngôn ngữ tự nhiên nhờ tích hợp Amazon Q CLI với ArgoCD MCP server:\nQuản lý ứng dụng: Liệt kê, khởi tạo, cập nhật và xóa các ứng dụng ArgoCD.\nThao tác đồng bộ: Kích hoạt đồng bộ và theo dõi trạng thái\nTrực quan hóa cây tài nguyên: Xem cấu trúc phân cấp của các tài nguyên do ứng dụng quản lý\nGiám sát trạng thái sức khỏe: Kiểm tra trạng thái sức khỏe của các ứng dụng và các tài nguyên của chúng.\nTheo dõi sự kiện: Xem các sự kiện liên quan đến ứng dụng và tài nguyên.\nTruy cập log: Truy xuất log từ các workload của ứng dụng.\nThực hiện hành động trên tài nguyên: Gọi các thao tác trên tài nguyên được quản lý bởi ứng dụng\nThiết lập môi trường của bạn\rCác điều kiện tiên quyết\rSau đây là các điều kiện tiên quyết cho việc thiết lập môi trường EKS của bạn, sau đó sẽ được quản lý bởi ArgoCD thông qua Amazon CLI.\nMột tài khoản AWS với quyền thích hợp\nAWS CLI phiên bản 2.13.0 hoặc mới hơn\nNode.js phiên bản 18.0.0 hoặc mới hơn\nnpm phiên bản 9.0.0 hoặc mới hơn\nAmazon Q CLI phiên bản 1.0.0 hoặc mới hơn (npm install -g @aws/amazon-q-cli)\nMột cụm EKS (phiên bản 1.27 hoặc mới hơn) đã được cài ArgoCD phiên bản 2.8 hoặc mới hơn\nKết nối với cụm EKS của bạn\rSử dụng AWS CLI để cập nhật kubeconfig của bạn aws eks update-kubeconfig --name \u003ccluster_name\u003e --region \u003cregion\u003e --role-arn \u003ciam_role_arn\u003e\nXác minh các pod ArgoCD đang chạy đúng cách trong namespace argocd kubectl get pods -n argocd\nTruy cập giao diện người dùng ArgoCD server trên máy cục bộ bằng lệnh chuyển tiếp port kubectl port-forward svc/blueprints-addon-argocd-server -n argocd 8080:443\nTạo ArgoCD API token\rTruy cập vào ArgoCD UI tại https://localhost:8080 Đăng nhập bằng thông tin đăng nhập của tài khoản admin Điều hướng đến User Settings \u003e API Tokens Nhấp vào “Generate New” để tạo một token mới Tạo một tệp cấu hình Amazon Q CLI MCP tại đường dẫn .amazonq/mcp.json và cập nhật các giá trị ARGOCD_BASE_URL và ARGOCD_API_TOKEN sao cho phù hợp với thiết lập môi trường của bạn. Tích hợp với Amazon Q CLI\r{ \"mcpServers\": { \"argocd-mcp-stdio\": { \"type\": \"stdio\", \"command\": \"npx\", \"args\": [ \"argocd-mcp@latest\", \"stdio\" ], \"env\": { \"ARGOCD_BASE_URL\": \"\u003cARGOCD_BASE_URL\u003e\", \"ARGOCD_API_TOKEN\": \"\u003cARGOCD_API_TOKEN\u003e\", \"NODE_TLS_REJECT_UNAUTHORIZED\": \"0\" } } } }\rKhi cấu hình xong, bạn có thể bắt đầu sử dụng câu lệnh bằng ngôn ngữ tự nhiên với Amazon Q CLI để tương tác với các ứng dụng ArgoCD của mình.\nQuản lý các ứng dụng bằng cách sử dụng ngôn ngữ tự nhiên\rDưới đây là một số câu lệnh (prompts) để tương tác với các ứng dụng ArgoCD trong cụm EKS của bạn\nLiệt kê ứng dụng Argo\nCâu lệnh: List all ArgoCD applications in my cluster\nAmazon Q sẽ sử dụng máy chủ ArgoCD MCP để truy xuất và hiển thị tất cả các ứng dụng\nTạo ứng dụng ArgoCD mới\nCâu lệnh: Create new argocd application using App name: game-2048 Repo: https://github.com/aws-ia/terraform-aws-eks-blueprints Path: patterns/gitops/getting-started-argocd/k8s. Branch: main Namespace: argocd\nAmazon Q sẽ tạo một ứng dụng mới từ thông tin GitRepo cung cấp\nXem trạng thái triển khai\nCâu lệnh: Show me the resource tree for team-carmen app\nAmazon Q sẽ hiển thị cấu trúc phân cấp của các tài nguyên Kubernetes do ứng dụng quản lý.\nĐồng bộ hóa ứng dụng\nCâu lệnh: Show me the applications that’s out of sync\nAmazon Q sẽ hiển thị các ứng dụng không đồng bộ\nCâu lệnh: Sync the application\nAmazon Q đang đồng bộ hóa các ứng dụng\nAmazon Q sẽ:\nKhởi tạo hoạt động đồng bộ cho ứng dụng đã chỉ định Giám sát quá trình động bộ hóa Báo trạng thái cuối cùng của hoạt động đồng bộ hóa Kiểm tra sức khỏe và giám sát\nCâu lệnh: Check the health of all resources in the team-geordie application\nAmazon Q đang đưa ra trạng thái sức khỏe của tất cả tài nguyên trong một ứng dụng\nAmazon Q sẽ:\nTruy vấn trạng thái sức khỏe của tất cả tài nguyên Xác định bất cứ thành phần nào không lành mạnh Cung cấp các khuyến nghị để giải quyết các vấn đề Câu lệnh: Show me the logs for the failing pod in the team-platform application\nAmazon Q đang hiển thị các log của pod gặp sự cố\nAmazon Q sẽ:\nXác định các pod gặp sự cố Truy vấn và hiển thị các log có liên quan Làm nổi bật các thông báo lỗi tiềm ẩn Tổng kết\rViệc tích hợp Amazon Q CLI với ArgoCD thông qua MCP server đánh dấu một bước tiến mang tính chuyển đổi trong việc quản lý Kubernets, kết hợp khả năng GitOps của ArgoCD với công nghệ xử lý ngôn ngữ tự nhiên của Amazon Q. Bằng cách chuyển đổi các thao tác phức tạp trên Kubernets thành các tương tác hội thoại đơn giản, giải pháp này cho phép các nhóm có thể tập trung vào điều thực sự quan trọng - tạo ra giá trị cho doanh nghiệp. Hơn là dành thời gian ghi nhớ các lệnh hoặc vật lộn với những kỹ thuật phức tạp, giờ đây các nhóm có thể quản lý hạ tầng cloud của họ thông qua đối thoại tự nhiên, giúp hành trình cloud-native trở nên dễ tiếp cận và hiệu quả hơn cho mọi người. Sẵn sàng cho việc chuyển đổi trải nghiệm EKS và ArgoCD của bạn chưa? Hãy thử tích hợp Amazon Q CLI với ArgoCD MCP ngay và khám phá lý do vì sao các đội DevOps đang đưa nó vào bộ công cụ thiết yếu của họ.",
    "description": "Triển khai liên tục dựa trên phương pháp GitOps với ArgoCD và EKS bằng cách sử dụng ngôn ngữ tự nhiên\rJagdish Komakula, Aditya Ambati và Anand Krishna Varanasi | 17/07/2025 | Amazon Elastic Kubernetes Service, Amazon Q, Amazon Q Developer, Developer Tools, Technical How-to | Permalink\nGiới thiệu\rArgoCD là một bộ công cụ GitOps hàng đầu giúp các nhóm quản lý việc triển khai Kubernets một cách khai báo, sử dụng Git là nguồn thông tin đáng tin cậy duy nhất. Bộ tính năng mạnh mẽ của nó bao gồm hệ thống đồng bộ tự động, hỗ trợ khôi phục, phát hiện sai lệch, chiến lược triển khai nâng cao, Tích hợp RBAC, và hỗ trợ đa cụm (multi-cluster), khiến nó trở thành giải pháp được ưa chuộng cho việc triển khai ứng dụng trên Kubernetes. Tuy nhiên, khi các tổ chức mở rộng quy mô, một vài điểm khó khăn và thử thách vận hành bắt đầu xuất hiện.",
    "tags": [],
    "title": "Blog 4",
    "uri": "/en/3-translated_blogs/blog_4/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Workshop",
    "content": "Cleanup Guide\rThis guide provides a comprehensive instruction to completely terminate all deployed AWS resources across the ap-southeast-1 (Singapore) and us-east-1 (N. Virginia) regions. The sequence is optimized to avoid orphaned resources and subsequent charges.\nI. Deployed AWS Services Summary\rBefore proceeding, verify the existence of the following active services in your account:\nService Category Service/Resource Name Region Purpose Compute/Scaling ECS Cluster/ASG, EC2 Instances ap-southeast-1 Application Hosting Database/Caching RDS DB Instance (Multi-AZ), ElastiCache Redis Cluster ap-southeast-1 Data Persistence \u0026 Session Management Networking Application Load Balancer (ALB), Target Groups, NAT Gateway, VPC ap-southeast-1 Traffic Distribution \u0026 Internet Access Storage/Artifacts S3 Primary Bucket, S3 DR Bucket, ECR Repositories ap-southeast-1, us-east-1 File Storage \u0026 Image Hosting Global/DNS/Security CloudFront Distribution, ACM Certificates, Route 53 Hosted Zone Global (Edge), us-east-1, ap-southeast-1 CDN \u0026 SSL Security II. Cleanup Procedure (Step-by-Step)\r1. Clean Up Application Layer (ap-southeast-1)\rWe start by eliminating resources that host the application and connect to the database.\nStop ECS/EC2 Compute:\nIf using ECS: Go to Amazon ECS → Clusters. Select Cluster → Tab Services. Select all Services → Click Update. Set Desired tasks to 0 → Next → Update Service. Wait for tasks to stop. Then, select Services again → Delete → Confirm with delete me. Delete Load Balancer (ALB) Components:\nGo to EC2 → Target Groups. Select Target Groups → Click Actions → Delete.\nGo to EC2 → Load Balancers. Select ALB → Click Actions → Delete.\n2. Delete Database and Caching Services\rDelete ElastiCache (Redis):\nGo to ElastiCache → Redis Clusters. Select Cluster → Click Actions → Delete. Delete RDS Database (CRITICAL COST STEP):\nGo to RDS → Databases. Select Instance.\nIf Deletion protection is Enabled, click Modify → Scroll to Deletion protection → Uncheck the box → Continue → Apply immediately.\nSelect Instance → Click Actions → Delete.\nUncheck Create final snapshot → Type the DB name to confirm → Click Delete.\n3. Clean Up Global Services and Certificates\rThese steps involve resources spanning multiple regions.\nDelete CloudFront Distribution:\nGo to CloudFront. Select Distribution → Click Disable. (Wait 10-15 minutes for status change).\nOnce Disabled, select Distribution → Click Delete.\nDelete ACM Certificates (Must be done in both regions):\nus-east-1 (N. Virginia): Switch Region → Go to ACM. Select Certificate → Click Delete.\nap-southeast-1 (Singapore): Switch Region → Go to ACM. Select Certificate → Click Delete.\n4. Clean Up Storage and Artifacts\rDelete ECR Repositories:\nGo to Elastic Container Registry (ECR). Select Repositories → Click Delete → Type delete to confirm. Clean and Delete S3 Buckets:\nRemove CRR: Go to S3 → Select Primary Bucket (ap-southeast-1) → Tab Management → Replication Rules → Delete the rule.\nEmpty: For both S3 Buckets (ap-southeast-1 and us-east-1), go to Tab Objects → Select all → Click Delete → Type permanently delete to confirm.\nDelete: After emptying, go to Tab Properties → Click Delete → Type the Bucket name to confirm.\n5. Delete Networking and DNS\rDelete NAT Gateway (CRITICAL COST STEP):\nGo to VPC → NAT Gateways. Select Gateway → Click Actions → Delete NAT Gateway. Delete Internet Gateway (IGW):\nGo to VPC → Internet Gateways. Select IGW → Click Actions → Detach from VPC.\nSelect IGW → Click Actions → Delete Internet Gateway.\nDelete VPC Components:\nDelete Security Groups → Delete Subnets → Delete VPC itself. Delete Route 53 Hosted Zone:\nGo to Route 53 → Hosted Zones. Delete all custom records → Click Delete Hosted Zone. III. Final Verification\rAWS Billing/Cost Explorer: Immediately check to ensure your estimated hourly cost has dropped to $0.00.\nDashboards: Verify the EC2, ECS, and RDS dashboards in ap-southeast-1 show zero running resources.",
    "description": "Cleanup Guide\rThis guide provides a comprehensive instruction to completely terminate all deployed AWS resources across the ap-southeast-1 (Singapore) and us-east-1 (N. Virginia) regions. The sequence is optimized to avoid orphaned resources and subsequent charges.\nI. Deployed AWS Services Summary\rBefore proceeding, verify the existence of the following active services in your account:",
    "tags": [],
    "title": "Clean Up",
    "uri": "/en/5-workshop/5.4-clean_up/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Workshop \u003e Deploy Flow \u003e Frontend Deploy",
    "content": "STAGE 3: CONFIGURATION CLOUDFRONT (CDN \u0026 FAILOVER)\rSTEP 3.1: Create Distribution\rStep 1: Get started\rDistribution name: sgutodolist-frontend-cloudfront\nDistribution type: Keep Single website or app.\nDomain:\nIn the Route 53 managed domain box, enter: sgutodolist.com.\n(If there is an Alternate domain names box, enter www.sgutodolist.com if the interface allows, otherwise we will add it later).\nClick Next.\nStep 2: Specify origin\rOrigin type: Select Amazon S3.\nOrigin:\nClick on the search box, select bucket Singapore: sgutodolist-frontend-sg....\nAllow private S3 bucket access to CloudFront:\nSelect: Allow private S3 bucket access to CloudFront - Recommended.\nCache settings: Leave “Use recommended cache settings…” unchanged.\nClick Next.\nStep 3: Enable security\rWeb Application Firewall (WAF):\nCheck the box on the right: Do not enable security protections (to save costs).\nClick Next.\nStep 4: Get TLS certificate\rTLS certificate:\nSelect the generated ACM certificate: sgutodolist.com (...).\nClick Next.\nStep 5: Review and create\rScroll down to the bottom and click the orange button Create distribution. STEP 3.1 (Additional): Post-creation configuration (Required)\rOrigin access control: This step is done after successfully creating a Distribution, go to the Origin tab of the newly created Distribution, click on origin and click edit\nClick Create new OAC.\nName: S3-OAC-HA.\nSigning behavior: Sign requests.\nClick Create.\nNext:\n1. Copy Policy (Important):\nClick on the newly created Distribution\nGo to the Origin tab of the newly created Distribution\nClick on the origin in the Origins list and click the Edit button\nClick the Copy policy button in the Origin access control. Go to the S3 Console tab \u003e Bucket Singapore \u003e Permissions \u003e Bucket Policy \u003e Paste \u003e Save. 2. Add Default Root Object (Fix white screen error):\nIn the newly created Distribution details screen, select the General tab (first tab).\nScroll down to the Settings section, click the Edit button (located to the right of the Settings section).\nFind the Default root object box.\nEnter: index.html.\n(By the way, check the Alternate domain names (CNAMEs) section: Make sure both sgutodolist.com and www.sgutodolist.com are present. If missing, add the item).\nScroll down and click Save changes.\nStep 3.2: Add Secondary Origin (Virginia)\rGo to the newly created Distribution \u003e Origins tab.\nClick Create origin.\nOrigin domain: Select the Virginia bucket (sgutodolist-frontend-us.s3...).\nOrigin access: Reselect the S3-OAC-HA created earlier.\nName: Failover-US.\nClick Create origin. Step 3.3: Create Origin Group (Enable High Availability)\rStill on the Origins tab \u003e Click Create origin group.\nName: HighAvailability-Group.\nOrigins:\nAdd Primary-SG (Up - Priority 1).\nAdd Failover-US (Down - Priority 2).\nFailover criteria: Select: 500, 502, 503, 504.\nClick Create origin group.\nWe will have 2 origins and 1 origin group: Step 3.4: Update Behavior\rBehaviors Tab \u003e Select Default (*) \u003e Edit.\nOrigin and origin groups: Change from Primary-SG to HighAvailability-Group.\nClick Save changes.\nStep 3.5: Configure SPA Routing (Handle 404 React errors)\rTab Error pages \u003e Create custom error response.\nRule 1 (For OAC):\nHTTP error code: 403.\nCustomize error response: Yes.\nResponse page path: /index.html.\nHTTP response code: 200.\nRule 2 (For React Router): Create another similar one for the code 404. (Path is still /index.html, code 200). ⬅ STEP 3: Route 53 and ACM\rSTEP 5: S3 Policy ➡",
    "description": "STAGE 3: CONFIGURATION CLOUDFRONT (CDN \u0026 FAILOVER)\rSTEP 3.1: Create Distribution\rStep 1: Get started\rDistribution name: sgutodolist-frontend-cloudfront\nDistribution type: Keep Single website or app.",
    "tags": [],
    "title": "ClouFront and Failover",
    "uri": "/en/5-workshop/5.3-deploy_flow/5.3.1-frontend-deploy/5.3.1.4-cloufront-and-failover/index.html"
  },
  {
    "breadcrumb": "Internship Report",
    "content": "During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1\rEvent Name: Kick-off AWS FCJ Workforce - FPTU OJT FALL 2025\nDate \u0026 Time: 8:30 AM Saturday, September 6, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nDescription: A kick-off event for FCAJ members to meet up and be familiar with the support \u0026 admin team, as well as get to know about the program\nOutcomes: Gained a clear understanding about the FCAJ program and know about the support team and admin team members\nEvent 2\rEvent Name: AWS CLOUD MASTERY SERIES #3 – Security on AWS\nDate \u0026 Time: 8:30 AM - 12:00 AM Saturday, November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nDescription: An in-depth session to help master the 5 pillars of security according to the AWS Well-Architected Security Pillar and protect cloud system against growing threats.\nOutcomes: Gained a solid understanding of how to design secure cloud systems based on the AWS Well-Architected Security Pillar, while experiencing hands-on demonstrations of IAM, threat detection, and incident response in real-world cloud environments.",
    "description": "During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1\rEvent Name: Kick-off AWS FCJ Workforce - FPTU OJT FALL 2025\nDate \u0026 Time: 8:30 AM Saturday, September 6, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City",
    "tags": [],
    "title": "Events Participated",
    "uri": "/en/4-events_participated/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Workshop \u003e Deploy Flow \u003e Backend Deploy",
    "content": "Task Definitions serve as blueprints that define how ECS should run containers, including resource allocation (RAM/CPU) and essential environment variables for service connectivity.\nPreparation\rRecord the following values before starting:\nRDS Endpoint: sgu-todolist-db.[random].ap-southeast-1.rds.amazonaws.com\nRedis Endpoint: sgu-redis.[random].cache.amazonaws.com (Without :6379)\nECR Image URIs: The URIs of the 6 repositories from the Image Build step.\nGoogle OAuth Credentials: Client ID and Client Secret.\nStandard Task Definition Process\rFor each service, follow this standard configuration process:\n1. Base Configuration:\nNavigate to Amazon ECS → Task definitions → Create new task definition.\nInfrastructure:\nLaunch type: AWS Fargate\nOperating system: Linux/X86_64\nTask Execution Role: ecsTaskExecutionRole\nContainer Details:\nProtocol: TCP\nPort Mapping: Refer to the specific service details below.\nLog collection: Turned on (Essential for debugging).\n1. Kafka Server (Infrastructure Backbone)\rNote: Kafka must be deployed first so other services can connect upon startup.\nParameter Value Family Name kafka-server-td Task Memory 2 GB (Critical) Task CPU 1 vCPU Container Port 9092 Environment Variables (Copy exactly):\nKey Value ALLOW_PLAINTEXT_LISTENER yes KAFKA_CFG_ADVERTISED_LISTENERS PLAINTEXT://kafka.sgu.local:9092 KAFKA_CFG_CONTROLLER_LISTENER_NAMES CONTROLLER KAFKA_CFG_CONTROLLER_QUORUM_VOTERS 0@127.0.0.1:9093 KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT KAFKA_CFG_LISTENERS PLAINTEXT://:9092,CONTROLLER://:9093 KAFKA_CFG_LOG_DIRS /tmp/kafka-logs KAFKA_CFG_NODE_ID 0 KAFKA_CFG_PROCESS_ROLES controller,broker KAFKA_HEAP_OPTS -Xmx512m 2. Auth Service (Critical Security Service)\rNote: This service requires high RAM to handle Login processes and Encryption.\nParameter Value Family Name auth-service-td Task Memory 2 GB (Mandatory to prevent Exit Code 137) Task CPU 0.5 vCPU Container Port 9999 Environment Variables:\nKey Value Notes SERVER_PORT 9999 JAVA_OPTS -Xmx768m Must include the hyphen -. SPRING_JPA_HIBERNATE_DDL_AUTO none CRITICAL: Prevents 500 Errors caused by DB schema conflicts. SERVER_FORWARD_HEADERS_STRATEGY native Fixes HTTP/HTTPS redirect issues behind ALB. SPRING_DATASOURCE_URL jdbc:mysql://[RDS-ENDPOINT]:3306/aws_todolist_database?allowPublicKeyRetrieval=true\u0026useSSL=false\u0026serverTimezone=UTC SPRING_DATASOURCE_USERNAME root SPRING_DATASOURCE_PASSWORD [DB_PASSWORD] SPRING_DATASOURCE_HIKARI_MAXIMUM_POOL_SIZE 5 SPRING_DATA_REDIS_HOST [REDIS-ENDPOINT] SPRING_DATA_REDIS_PORT 6379 SPRING_DATA_REDIS_SSL_ENABLED true Mandatory for AWS ElastiCache. SPRING_KAFKA_BOOTSTRAP_SERVERS kafka.sgu.local:9092 DOMAIN_FRONTEND https://sgutodolist.com APP_OAUTH2_REDIRECT_URI https://sgutodolist.com/oauth2/redirect SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_GOOGLE_CLIENT_ID [GOOGLE_CLIENT_ID] SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_GOOGLE_CLIENT_SECRET [GOOGLE_CLIENT_SECRET] SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_GOOGLE_REDIRECT_URI https://sgutodolist.com/api/auth/login/oauth2/code/google Note: Include /api/auth in path. 3. User Service \u0026 Taskflow Service\rThese are business logic services requiring permission to create DB tables on first run.\nParameter Value Family Name user-service-td / taskflow-service-td Task Memory 1 GB Task CPU 0.5 vCPU Container Port 8081 (User) / 8082 (Taskflow) Environment Variables (Common for both):\nKey Value Notes SERVER_PORT 8081 (User) or 8082 (Taskflow) JAVA_OPTS -Xmx512m SPRING_JPA_HIBERNATE_DDL_AUTO update Allows service to create tables on startup. SPRING_DATASOURCE_URL (Same as Auth Service) SPRING_DATASOURCE_USERNAME root SPRING_DATASOURCE_PASSWORD [DB_PASSWORD] SPRING_DATASOURCE_HIKARI_MAXIMUM_POOL_SIZE 5 SPRING_DATA_REDIS_HOST [REDIS-ENDPOINT] SPRING_DATA_REDIS_PORT 6379 SPRING_DATA_REDIS_SSL_ENABLED true SPRING_KAFKA_BOOTSTRAP_SERVERS kafka.sgu.local:9092 4. API Gateway (The Orchestrator)\rThe most critical service for internal routing.\nParameter Value Family Name api-gateway-td Task Memory 1 GB Task CPU 0.5 vCPU Container Port 8080 Environment Variables:\nKey Value Explanation SERVER_PORT 8080 CORS_ALLOWED_ORIGINS https://sgutodolist.com,https://www.sgutodolist.com Allows Frontend API access. AUTH_SERVICE_URL http://auth.sgu.local:9999 Internal routing via Service Discovery. USER_SERVICE_URL http://user.sgu.local:8081 Note the name: user (not user-service). TASKFLOW_SERVICE_URL http://taskflow.sgu.local:8082 Note the name: taskflow. NOTIFICATION_SERVICE_URL http://notification.sgu.local:9998 Note the name: notification. SPRING_DATA_REDIS_HOST [REDIS-ENDPOINT] Used for Rate Limiting. SPRING_DATA_REDIS_PORT 6379 SPRING_DATA_REDIS_SSL_ENABLED true Task Definition Validation Checklist\rAfter creation, please verify:\nRAM Allocation: Auth Service must be 2GB, others at least 1GB.\nRedis SSL: Variable SPRING_DATA_REDIS_SSL_ENABLED = true is set for all services connecting to Redis.\nInternal URLs: All _SERVICE_URL variables in Gateway must end with .sgu.local.\nDDL Auto: Auth Service must be none, User/Taskflow should be update.\n⬅ STEP 3: Code Update \u0026 Image Build\rSTEP 5: Services Deployment ➡",
    "description": "Task Definitions serve as blueprints that define how ECS should run containers, including resource allocation (RAM/CPU) and essential environment variables for service connectivity.\nPreparation\rRecord the following values before starting:\nRDS Endpoint: sgu-todolist-db.[random].ap-southeast-1.rds.amazonaws.com\nRedis Endpoint: sgu-redis.[random].cache.amazonaws.com (Without :6379)\nECR Image URIs: The URIs of the 6 repositories from the Image Build step.\nGoogle OAuth Credentials: Client ID and Client Secret.",
    "tags": [],
    "title": "Task Definitions Creation",
    "uri": "/en/5-workshop/5.3-deploy_flow/5.3.2-backend-deploy/5.3.2.4-task-definitions-creation-configure-settings-fix-environment-variables/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Worklog",
    "content": "Week 4 Objectives\rBegin coding the backend authentication service and frontend Discuss and list the APIs required for the taskflow service. Learn and practice S3 Explore API Gateway Translate technical blogs. Tasks to be carried out this week\rDay Task Start Date Completion Date Reference Material 1 - Team meeting to discuss about the architecture, backend and front end configuration 29/09/2025 29/09/2025 - Translate blog Translated Blog 1 - Research and make a list of APIs needed for the TaskFlow Service. 2 - Learn and practice S3 service 30/09/2025 30/09/2025 S3 lesson - Code backend Authentication Service 3 - Code backend Authentication Service 01/10/2025 01/10/2025 - Initialize the project (React, environment configuration, basic routing setup, folder structure, Git integration). 4 - Learn and practice API Gateway service 02/10/2025 02/10/2025 AWS API Gateway Introduction, Tạo API sử dụng Amazon API Gateway AWS Serverless Development Journey - Code backend Authentication Service - Set up UI/CSS libraries (e.g., Tailwind, Material UI), define global CSS variables, build basic Components (Button, Input, Card). - Translate blog Translated Blog 2, Translated Blog 3 5 - Code backend Authentication Service 03/10/2025 03/10/2025 - Set up UI/CSS libraries (e.g., Tailwind, Material UI), define global CSS variables, build basic Components (Button, Input, Card). - Code frontend: Build Header, Footer, Sidebar, and the Landing Page. Ensure basic responsive design. - Translate blog Translated Blog 4, Translated Blog 5",
    "description": "Week 4 Objectives\rBegin coding the backend authentication service and frontend Discuss and list the APIs required for the taskflow service. Learn and practice S3 Explore API Gateway Translate technical blogs. Tasks to be carried out this week\rDay Task Start Date Completion Date Reference Material 1 - Team meeting to discuss about the architecture, backend and front end configuration 29/09/2025 29/09/2025 - Translate blog Translated Blog 1 - Research and make a list of APIs needed for the TaskFlow Service. 2 - Learn and practice S3 service 30/09/2025 30/09/2025 S3 lesson - Code backend Authentication Service 3 - Code backend Authentication Service 01/10/2025 01/10/2025 - Initialize the project (React, environment configuration, basic routing setup, folder structure, Git integration). 4 - Learn and practice API Gateway service 02/10/2025 02/10/2025 AWS API Gateway Introduction, Tạo API sử dụng Amazon API Gateway AWS Serverless Development Journey - Code backend Authentication Service - Set up UI/CSS libraries (e.g., Tailwind, Material UI), define global CSS variables, build basic Components (Button, Input, Card). - Translate blog Translated Blog 2, Translated Blog 3 5 - Code backend Authentication Service 03/10/2025 03/10/2025 - Set up UI/CSS libraries (e.g., Tailwind, Material UI), define global CSS variables, build basic Components (Button, Input, Card). - Code frontend: Build Header, Footer, Sidebar, and the Landing Page. Ensure basic responsive design. - Translate blog Translated Blog 4, Translated Blog 5",
    "tags": [],
    "title": "Week 4 Worklog",
    "uri": "/en/1-worklog/1.4-week_4/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Translated Blogs",
    "content": "Ra Mắt Strands Agents 1.0: Việc điều phối Multi-Agent cho môi trường production đã được đơn giản hóa\rRyan Coleman và Belle Guttman | 15/07/2025 | Amazon Machine Learning, Announcements, Artificial Intelligence, Open Source| Permalink | Comments\nHôm nay, chúng tôi vui mừng thông báo về phiên bản 1.0 của Strands Agents SDK, đánh dấu một cột mốc quan trọng trong hành trình giúp việc xây dựng các agent AI trở nên đơn giản, đáng tin cậy và sẵn sàng cho môi trường production. Strands Agents là một SDK mã nguồn mở, áp dụng phương pháp model-driven, giúp bạn xây dựng và vận hành các agent AI chỉ trong vài dòng code. Strands có khả năng mở rộng từ các trường hợp sử dụng agent đơn giản đến phức tạp, cũng như từ phát triển cục bộ đến triển khai trong môi trường production.\nKể từ khi ra mắt bản xem trước vào tháng 5 năm 2025, chúng tôi đã nhận được hơn 2.000 lượt sao trên GitHub và hơn 150 ngàn lượt tải xuống trên PyPI. Strands 1.0 mang đến mức độ đơn giản tương tự cho các ứng dụng multi-agent như những gì Strands đã làm được với các agent đơn lẻ, với việc bổ sung bốn primitive mới và hỗ trợ cho giao thức Agent to Agent (A2A). Để đưa kiến trúc multi-agent vào môi trường production, phiên bản 1.0 cũng bao gồm một trình quản lý session mới để truy xuất trạng thái agent từ một kho dữ liệu từ xa, cùng với việc cải thiện hỗ trợ bất đồng bộ xuyên suốt toàn bộ SDK. Nhằm tăng tính linh hoạt để xây dựng agent của bạn với bất kỳ mô hình nào, cStrands 1.0 đã mở rộng hỗ trợ thêm API của năm nhà cung cấp mô hình mới, được đóng góp bởi các đối tác như Anthropic, Meta, OpenAI, Cohere, Mistral, Stability, Writer và Baseten (xem pull request). Bây giờ, hãy cùng đi sâu vào chi tiết các cập nhật này. Các mẫu code đầy đủ có sẵn tại trang strandsagents.com.\nĐơn giản hóa các mô hình multi-agent\rCác mô hình multi-agent cho phép các agent AI chuyên biệt cùng nhau làm việc—phân công nhiệm vụ, chia sẻ kiến thức và phối hợp hành động—nhằm giải quyết những vấn đề phức tạp mà một agent đơn lẻ không thể xử lý được. Strands 1.0 giới thiệu bốn primitive trực quan, giúp việc điều phối nhiều agent trở thành một phần mở rộng đơn giản của tổ hợp mô hình/công cụ/prompt mà bạn vốn đã sử dụng để tạo ra các agent đơn lẻ.\n1. Agents-as-Tools: Đơn giản hóa việc ủy quyền theo cấp bậc\nMô hình agents-as-tools biến các agent chuyên biệt thành những công cụ thông minh mà các agent khác có thể gọi đến. Điều này tạo điều kiện cho việc ủy quyền theo cấp bậc, nơi các agent hoạt động như người điều phối có thể chủ động tham vấn các chuyên gia theo từng lĩnh vực cụ thể mà không từ bỏ quyền kiểm soát yêu cầu. Điều này phản ánh cách thức làm việc của các đội nhóm con người — một quản lý dự án không cần phải biết mọi thứ, họ chỉ cần biết nên tham khảo ý kiến của chuyên gia nào cho từng nhiệm vụ cụ thể.\nfrom strands import Agent, tool from strands_tools import calculator, file_write, python_repl, journal @tool def web_search(query: str) -\u003e str: return \"Dummy web search results here!\" # Create specialized agents research_analyst_agent = Agent( system_prompt=\"You are a research specialist who gathers and analyzes information about local startup markets\", tools=[web_search, calculator, file_write, python_repl] ) travel_advisor_agent = Agent( system_prompt=\"You are a travel expert who helps with trip planning and destination advice\", tools=[web_search, journal] ) # Convert the agents into tools @tool def research_analyst(query: str) -\u003e str: response = research_analyst_agent(query) return str(response) @tool def travel_advisor(query: str) -\u003e str: response = travel_advisor_agent(query) return str(response) # Orchestrator naturally delegates to specialists executive_assistant = Agent( tools=[research_analyst, travel_advisor] ) result = executive_assistant(\"I have a business meeting in Portland next week. Suggest a nice place to stay near the local startup scene, and suggest a few startups to visit\") Trong ví dụ rút gọn này, chúng ta định nghĩa hai agent du lịch và nghiên cứu, mỗi agent có prompt và công cụ chuyên biệt cho lĩnh vực của mình, mà agent trợ lý điều hành có thể gọi đến để lấy thông tin phục vụ yêu cầu của người dùng. agent trợ lý điều hành chịu trách nhiệm tổng hợp đầu vào từ các agent khác và tạo ra phản hồi gửi lại cho người dùng. Tìm hiểu thêm về mô hình Agents-as-Tools trong tài liệu chính thức của Strands.\n2. Handoffs: chuyển giao quyền kiểm soát rõ ràng\nTính năng Handoffs cho phép các agent chủ động chuyển trách nhiệm sang con người khi gặp phải nhiệm vụ vượt quá phạm vi chuyên môn của mình, đồng thời giữ nguyên toàn bộ ngữ cảnh cuộc trò chuyện trong quá trình chuyển giao. Strands cung cấp sẵn công cụ được tích hợp sẵn handoff_to_user giúp cho các agent có thể dùng để chuyển giao quyền kiểm soát một cách liền mạch trong khi vẫn duy trì lịch sử và ngữ cảnh cuộc trò chuyện - giống như một nhân viên dịch vụ khách hàng cung cấp chủ động yêu cầu khách cung cấp thêm thông tin về trường hợp của họ.\nfrom strands import Agent from strands_tools import handoff_to_user SYSTEM_PROMPT=\"\"\" Answer the user's support query. Ask them questions with the handoff_to_user tool when you need more information \"\"\" # Include the handoff_to_user tool in our agent's tool list agent = Agent( system_prompt=SYSTEM_PROMPT, tools=[handoff_to_user] ) # The agent calls the handoff_to_user tool which includes the question for the customer agent(\"I have a question about my order.\")\rCác agent cũng có thể đặt câu hỏi cho con người khi được yêu cầu làm như vậy\nfrom strands import Agent SYSTEM_PROMPT=\"\"\" Answer the user's support query. Ask them questions when you need more information \"\"\" agent = Agent( system_prompt=SYSTEM_PROMPT, ) # The agent asks questions by streaming them back as text agent(\"I have a question about my order.\")\r3. Swarms: Các nhóm cộng tác tự tổ chức\nMột Swarm tạo ra các agent tự chủ phối hợp động thông qua việc chia sẻ bộ nhớ,cho phép nhiều chuyên gia có thể cộng tác cho các tác vụ phức tạp. Hãy hình dung nó giống như một buổi thảo luận nhóm, nơi các chuyên gia xây dựng ý tưởng dựa trên ý tưởng của nhau, với đội ngũ tự tổ chức để mang lại kết quả tập thể tốt nhất.\nimport logging from strands import Agent from strands.multiagent import Swarm from strands_tools import memory, calculator, file_write # Enables Strands debug logs level, and prints to stderr logging.getLogger(\"strands.multiagent\").setLevel(logging.DEBUG) logging.basicConfig( format=\"%(levelname)s | %(name)s | %(message)s\", handlers=[logging.StreamHandler()] ) researcher = Agent( name=\"researcher\", system_prompt=\"You research topics thoroughly using your memory and built-in knowledge\", tools=[memory] ) analyst = Agent( name=\"analyst\", system_prompt=\"You analyze data and create insights\", tools=[calculator, memory] ) writer = Agent( name=\"writer\", system_prompt=\"You write comprehensive reports based on research and analysis\", tools=[file_write, memory] ) # Swarm automatically coordinates agents market_research_team = Swarm([researcher, analyst, writer]) result = market_research_team( \"What is the history of AI since 1950? Create a comprehensive report\" ) Nghiên cứu thêm về Swarms trong tài liệu về Strands\n4. Graphs: Kiểm soát quy trình làm việc mang tính xác định\nGraphs cho phép bạn xác định quy trình làm việc của các agent một cách rõ ràng với những định tuyến có điều kiện và những điểm ra quyết định, rất hữu ích cho những quy trình yêu cầu các bước cụ thể, cơ chế phê duyệt hoặc các ngưỡng kiểm soát chất lượng. Giống như một dây chuyền lắp ráp hoặc chuỗi phê duyệt được thiết kế tốt, graphs đảm bảo các agent luôn tuân thủ các quy tắc nghiệp vụ đã định sẵn theo đúng trình tự mỗi lần thực thi.\nfrom strands import Agent from strands.multiagent import GraphBuilder analyzer_agent = Agent( name=\"analyzer\", system_prompt=\"Analyze customer requests and categorize them\", tools=[text_classifier, sentiment_analyzer] ) normal_processor = Agent( name=\"normal_processor\", system_prompt=\"Handle routine requests automatically\", tools=[knowledge_base, auto_responder] ) critical_processor = Agent( name=\"critical_processor\", system_prompt=\"Handle critical requests quickly\", tools=[knowledge_base, escalate_to_support_agent] ) # Build deterministic workflow builder = GraphBuilder() builder.add_node(analyzer_agent, \"analyze\") builder.add_node(normal_processor, \"normal_processor\") builder.add_node(critical_processor, \"critical_processor\") # Define conditional routing def is_approved(state): return True def is_critical(state): return False builder.add_edge(\"analyze\", \"normal_processor\", condition=is_approved) builder.add_edge(\"analyze\", \"critical_processor\", condition=is_critical) builder.set_entry_point(\"analyze\") customer_support_graph = builder.build() # Execute the graph with user input results = customer_support_graph(\"I need help with my order!\") Nghiên cứu thêm về Graphs trong tài liệu về Strands\nCác mô hình multi-agent (multi-agent patterns) này được thiết kế để được áp dụng dần dần và kết hợp một cách tự do — hãy bắt đầu với các agent đơn lẻ, thêm các chuyên gia như là công cụ, phát triển thành các Swarms, và điều phối bằng các Graphs khi nhu cầu của bạn tăng lên. Hãy kết hợp và tùy chỉnh các mô hình để tạo ra các hệ thống tinh vi: swarms có thể chứa các graphs, graphs có thể điều phối các swarms, và bất kỳ mô hình nào cũng có thể sử dụng các agent được trang bị thêm các agent khác làm công cụ.\nfrom strands import Agent, tool from strands.multiagent import GraphBuilder, Swarm from strands_tools import memory, calculator, python_repl, file_write # Start simple with a single agent agent = Agent(tools=[memory]) # Create specialist agents that a lead orchestrator agent can consult data_analyst = Agent(name=\"analyst\", tools=[calculator, python_repl]) @tool def data_analyst_tool(query: str) -\u003e str: return str(data_analyst(query)) analyst_orchestrator = Agent(tools=[memory, data_analyst_tool]) # Agents-as-tools # Compose patterns together - a graph that uses a swarm researcher = Agent(name=\"researcher\", tools=[memory]) writer = Agent(name=\"writer\", tools=[file_write]) research_swarm = Swarm([researcher, analyst_orchestrator, writer]) review_agent = Agent(system_prompt=\"Review the research quality and suggest improvements\") builder = GraphBuilder() builder.add_node(research_swarm, \"research\") # Swarm as graph node builder.add_node(review_agent, \"review\") builder.add_edge(\"research\", \"review\") graph = builder.build() # The patterns nest naturally - swarms in graphs, agents as tools everywhere result = graph(\"How has green energy evolved over the last few years?\") Hệ thống Multi-Agent với A2A\rStrand 1.0 đã bao gồm việc hỗ trợ cho giao thức Agent-to-Agent (A2A), một tiêu chuẩn mở cho phép các agent từ các nền tảng khác nhau có thể giao tiếp một cách liền mạch. Bất kỳ agent Strands cũng có thể được tích hợp các khả năng A2A để trở nên có thể truy cập qua mạng và tuân thủ giao thức A2A. Các agent A2A từ các tổ chức bên ngoài cũng có thể được sử dụng trực tiếp trong tất cả các mô hình multi-agent của Strands.\nfrom strands import Agent from strands.multiagent.a2a import A2AServer from strands_tools.a2a_client import A2AClientToolProvider # Serve your agent via A2A protocol local_agent = Agent(name=\"analyzer\", tools=[web_search, data_analysis]) a2a_agent = A2AServer(agent=local_agent, port=9000) a2a_agent.serve() # AgentCard available at http://localhost:9000/.well-known/agent.json # Use remote A2A agents partner_agent_url = \"https://partner.com\" cloud_agent_url = \"https://cloud.ai\" # Connect to remote A2A enabled agents a2a_tool_provider = A2AClientToolProvider(known_agent_urls=[partner_agent_url, cloud_agent_url]) # Orchestrate remote agents orchestrator = Agent(tools=[a2a_tool_provider.tools]) Bởi vì A2A cung cấp các tính năng như agent card, một mô tả được chuẩn hóa về khả năng của agent, các hệ thống multi-agent được bật A2A có thể dễ dàng khám phá và kết nối với các agent được tạo ra bởi các nhóm hoặc các tổ chức khác. Strands tự động tạo ra thẻ agent dựa trên các công cụ bạn đã cung cấp cho agent. Để xem các ví dụ hoạt động hoàn chỉnh và bắt đầu với tính năng tích hợp A2A, hãy tham khảo repository mẫu của chúng tôi và tài liệu A2A của Strands.\nSẵn sàng cho môi trường production\rMặc dù Strands đã được các đội nội bộ của Amazon như Amazon Q Developer và AWS Glue sử dụng trong môi trường production từ lâu trước khi ra mắt công chúng, chúng tôi đã và đang làm việc ngược lại với hàng trăm khách hàng trên toàn thế giới để mở rộng Strands nhằm đáp ứng các nhu cầu production của bạn. Các cập nhật này bao gồm một tầng trừu tượng quản lý session để hỗ trợ việc lưu trữ liên tục dữ liệu vào và phục hồi từ các kho dữ liệu bên ngoài, đầu ra có cấu trúc, cải thiện hỗ trợ bất đồng bộ, và nhiều thứ khác nữa (xem chi tiết trong changelog các phiên bản phát hành)\nQuản lý session bền vững: Chúng tôi đã thêm vào SessionManager, một công cụ trừu tượng hóa quản lý session cho phép tự động lưu trữ liên tục và khôi phục lịch sử hội thoại cũng như trạng thái của agent. Nhờ đó, các agent có thể lưu toàn bộ lịch sử cuộc trò chuyện vào một hệ thống lưu trữ như Amazon Simple Storage Service (Amazon S3) và tiếp tục cuộc hội thoại một cách liền mạch ngay cả sau khi hệ thống khởi động lại. Dưới đây là một ví dụ sử dụng cơ chế lưu trữ liên tục dựa trên file cơ bản.\nfrom strands import Agent from strands.session.file_session_manager import FileSessionManager # Create a session manager with file-based storage Session_manager = FileSessionManager(session_id=”customer_support”, base_dir=\"./agent_sessions\") # Agent automatically persists all conversations agent = Agent( id=\"support_bot_1\", session_manager=session_manager, tools=[knowledge_base, ticket_system] ) # Messages are automatically saved as the conversation progresses agent(\"Help me reset my password\") agent(\"I can't access my email\") # Later, even after a restart, restore the full conversation Bạn có thể mở rộng lớp trừu tượng này bằng cách tự triển khai backend lưu trữ của riêng mình thông qua mẫu thiết kế Data Access Object (DAO), và Strands đã tích hợp sẵn hai backend mặc định: hệ thống file cục bộ và Amazon S3. Mỗi agent nhận một ID duy nhất để theo dõi, và hệ thống xử lý các agent đồng thời trong cùng một session cho các kịch bản multi-agent, đảm bảo rằng các agent luôn duy trì ngữ cảnh qua các lần triển khai, mở rộng quy mô hệ thống hoặc khởi động lại. Tìm hiểu thêm về Session Management trong tài liệu của Strands.\nHỗ trợ bất đồng bộ nguyên bản và cải thiện hiệu năng: Các workload trong môi trường production đòi hỏi độ tin cậy cao và hiệu năng phản hồi nhanh. Trong phiên bản 1.0, chúng tôi đã cải tiến kiến trúc vòng lặp sự kiện của Strands để hỗ trợ thao tác bất đồng bộ trên toàn bộ stack. Các công cụ và nhà cung cấp mô hình giờ đây có thể chạy bất đồng bộ mà không bị chặn, cho phép thực thi đồng thời thực sự. Phương thức stream_async mới truyền phát tất cả các sự kiện của agent — văn bản, việc sử dụng công cụ, các bước suy luận — theo thời gian thực, vđồng thời tích hợp cơ chế hủy khi người dùng rời khỏi trang.\nimport asyncio from fastapi import FastAPI from fastapi.responses import StreamingResponse from strands import Agent from strands_tools import calculator app = FastAPI() @app.post(\"/chat\") async def chat_endpoint(message: str): async def stream_response(): agent = Agent(tools=[web_search, calculator]) # Stream agent responses in real-time async for event in agent.stream_async(message): if \"data\" in event: yield f\"data: {event['data']}\\n\\n\" elif \"current_tool_use\" in event: yield f\"event: tool\\ndata: Using {event['current_tool_use']['name']}\\n\\n\" return StreamingResponse(stream_response(), media_type=\"text/event-stream\") # Concurrent agent evaluation async def evaluate_models_concurrently(prompt: str): async def stream(agent: Agent): print(f\"STARTING: {agent.name}\") async for event in agent.stream_async(prompt): # handle events print(f\"ENDING: {agent.name}\") return event[“result”] # last event is the agent result agents = [ Agent(name=\"claude\", model=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0”), Agent(name=\"deepseek”, model=\"us.deepseek.r1-v1:0”), Agent(name=\"nova\", model=\"us.amazon.nova-pro-v1:0\") ] # Execute all agents concurrently responses = await asyncio.gather(*[stream(agent) for agent in agents]) return responses Tìm hiểu thêm về Hỗ trợ bất động bộ nguyên bản trong tài liệu chính thức của Strands.\nMở rộng hỗ trợ đa dạng nhà cung cấp mô hình: Khách hàng đã chia sẻ rằng họ cần linh hoạt trong việc sử dụng các mô hình khác nhau cho các tác vụ khác nhau. Để đáp ứng nhu cầu này, Strands Agents đã nhận được sự hỗ trợ mạnh mẽ từ cộng đồng các nhà cung cấp mô hình. Các nhà cung cấp như Anthropic, Meta, OpenAI, Cohere, Mistral, Stability và Writer đã đóng góp cho phép API mô hình của họ được sử dụng bởi một Strands Agent thông qua code. Việc truy cập Strands Agents thông qua hạ tầng API do chính các nhà cung cấp này cung cấp giúp các nhà phát triển tập trung vào việc xây dựng các giải pháp AI-powered, mà không cần lo lắng về quản lý cơ sở hạ tầng. Những bổ sung này bổ trợ hoàn hảo cho khả năng hỗ trợ từ giai đoạn xem trước đối với mọi mô hình trên Amazon Bedrock, OpenAI, và bất kỳ endpoint tương thích OpenAI nào thông qua LiteLLM. Strands cho phép bạn sử dụng các mô hình khác nhau cho mỗi agent, hoặc chuyển đổi mô hình và nhà cung cấp mô hình mà không cần sửa đổi công cụ hoặc logic của bạn.\nfrom strands import Agent from strands.models import BedrockModel from strands.models.openai import OpenAIModel from strands.models.anthropic import AnthropicModel # Configure different model providers bedrock_model = BedrockModel( model_id=\"us.amazon.nova-pro-v1:0\", temperature=0.3, top_p=0.8, region_name=\"us-west-2\" ) openai_model = OpenAIModel( client_args={ \"api_key\": \"your-api-key\", }, model_id=\"gpt-4o\", params={ \"max_tokens\": 1000, \"temperature\": 0.7, } ) anthropic_model = AnthropicModel( client_args={ \"api_key\": \"your-api-key\", }, max_tokens=1028, model_id=\"claude-3-7-sonnet-20250219\", params={ \"temperature\": 0.5, } ) # Swap models or use different models for different agents in the same system researcher = Agent( name=\"researcher\", model=anthropic_model, tools=[web_search] ) writer = Agent( name=\"writer\", model=openai_model, tools=[document_formatter] ) analyzer = Agent( name=\"analyzer\", model=bedrock_model, tools=[data_processor] )\rCộng đồng Strands đã đóng vai trò then chốt trong việc định hình tất cả những cải tiến này thông qua việc sử dụng thực tế, phản hồi và những đóng góp mã nguồn trực tiếp. Trong số hơn 150 pull request (PR) đã được merge vào Strands từ phiên bản 0.1.0 đến 1.0, 22% được đóng góp bởi các thành viên cộng đồng, những người đã sửa lỗi, thêm nhà cung cấp mô hình, viết tài liệu, thêm tính năng và tái cấu trúc các lớp để cải thiện hiệu suất. Chúng tôi vô cùng biết ơn mỗi người trong số các bạn vì đã giúp Strands trở thành cách đơn giản nhất để đưa một agent từ nguyên mẫu đến triển khai thực tế.\nTương lai của AI là multi-agent (multi-agent), và với Strands 1.0, tương lai đó đã sẵn sàng để triển khai trong môi trường production. Hãy bắt đầu xây dựng ngay hôm nay tại strandsagents.com.",
    "description": "Ra Mắt Strands Agents 1.0: Việc điều phối Multi-Agent cho môi trường production đã được đơn giản hóa\rRyan Coleman và Belle Guttman | 15/07/2025 | Amazon Machine Learning, Announcements, Artificial Intelligence, Open Source| Permalink | Comments\nHôm nay, chúng tôi vui mừng thông báo về phiên bản 1.0 của Strands Agents SDK, đánh dấu một cột mốc quan trọng trong hành trình giúp việc xây dựng các agent AI trở nên đơn giản, đáng tin cậy và sẵn sàng cho môi trường production. Strands Agents là một SDK mã nguồn mở, áp dụng phương pháp model-driven, giúp bạn xây dựng và vận hành các agent AI chỉ trong vài dòng code. Strands có khả năng mở rộng từ các trường hợp sử dụng agent đơn giản đến phức tạp, cũng như từ phát triển cục bộ đến triển khai trong môi trường production.",
    "tags": [],
    "title": "Blog 5",
    "uri": "/en/3-translated_blogs/blog_5/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Workshop",
    "content": "1. Hugo Commands\r1.1\nhugo build\r1.2\nhugo server -D\r2. Git Commands\r2.1 Check repository status\ngit status\r2.2 List local branches\ngit branch\r2.3 List all branches (local + remote)\ngit branch -a\r2.4 Switch to another branch\ngit checkout \u003cbranch-name\u003e\r2.5 Create and switch to a new branch\ngit checkout -b \u003cbranch-name\u003e\r2.6 Pull the latest updates from remote\ngit pull\r2.7 Fetch updates without merging\ngit fetch\r2.8 Add files to staging\ngit add \u003cfile\u003e\rAdd all changes:\ngit add .\r2.9 Commit changes\ngit commit -m \"your message\"\r2.10 Push changes to remote\ngit push\rPush a new branch for the first time:\ngit push -u origin \u003cbranch-name\u003e\r2.11 View remote repositories\ngit remote -v\r2.12 Merge a branch into the current branch\ngit merge \u003cbranch-name\u003e # Example: git checkout main git merge developer\r2.13 Delete a branch\nDelete local branch:\ngit branch -d \u003cbranch-name\u003e\rDelete remote branch:\ngit push origin --delete \u003cbranch-name\u003e\r2.14 Undo changes\nDiscard changes in a file:\ngit checkout -- \u003cfile\u003e\rReset everything to the last commit:\ngit reset --hard\r3. PowerShell Commands\r3.1 Automatically update _index.md for all weeks in a specific directory\n# Target directory path $targetDir = \"content/1-Worklog/1.3-DuongBinhMinh\" # Check if directory exists if (-not (Test-Path $targetDir)) { Write-Host \"Error: Directory not found $targetDir\" -ForegroundColor Red break } # Get subfolders $folders = Get-ChildItem -Path $targetDir -Directory foreach ($folder in $folders) { if ($folder.Name -match \"Week_(\\d+)\") { $weekNum = $matches[1] $filePath = Join-Path $folder.FullName \"_index.md\" if (Test-Path $filePath) { # Read file content $content = Get-Content -Path $filePath -Raw -Encoding UTF8 # Generate new pre value $newPreLine = \"pre = `\" \u003cb\u003e 1.3.$weekNum. \u003c/b\u003e `\"\" # Replace old pre line $newContent = $content -replace '(?m)^pre\\s*=\\s*\".*\"', $newPreLine # Overwrite file Set-Content -Path $filePath -Value $newContent -Encoding UTF8 Write-Host \"Updated Week $($weekNum): $newPreLine\" -ForegroundColor Green } else { Write-Host \"Skipped $folder.Name (_index.md not found)\" -ForegroundColor Yellow } } } Write-Host \"Update completed!\" -ForegroundColor Cyan\r3.2 Standardize folder structure, convert Leaf Bundles → Branch Bundles, fix frontmatter and internal links\n# --- CONFIGURATION --- $worklogPath = \"content/1-Worklog\" # --- HELPER FUNCTION: Rename index.md to _index.md --- Function Fix-IndexFileName ($dirPath) { $wrongFile = Join-Path $dirPath \"index.md\" $correctFile = Join-Path $dirPath \"_index.md\" if (Test-Path $wrongFile) { if (-not (Test-Path $correctFile)) { Rename-Item -Path $wrongFile -NewName \"_index.md\" Write-Host \" [File] Renamed index.md -\u003e _index.md\" -ForegroundColor Magenta } } } Write-Host \"=== STARTING HUGO DATA STANDARDIZATION ===\" -ForegroundColor Cyan # Get all user directories (e.g., 1.1, 1.2, 1.3 ...) $userFolders = Get-ChildItem -Path $worklogPath -Directory foreach ($userFolder in $userFolders) { if ($userFolder.Name -match \"^(\\d+\\.\\d+)-\") { $userPrefix = $matches[1] Write-Host \"`n--- Processing User: $($userFolder.Name) (Prefix: $userPrefix) ---\" -ForegroundColor Cyan # Fix parent index file Fix-IndexFileName $userFolder.FullName $parentIndexFile = Join-Path $userFolder.FullName \"_index.md\" if (Test-Path $parentIndexFile) { $pContent = Get-Content -Path $parentIndexFile -Raw -Encoding UTF8 # Fix Hugo shortcode format $pContent = $pContent -replace '\\{\\{\u003c\\s*relref', ('{' + '{% relref') $pContent = $pContent -replace '\u003e\\}\\}\\)', ('%' + '}})') # Fix broken internal links $pContent = [Regex]::Replace($pContent, 'relref\\s*\"[^\"]*?Week_(\\d+)\"', { param($m) $w = $m.Groups[1].Value return 'relref \"' + \"$userPrefix.$w-Week_$w\" + '\"' }) Set-Content -Path $parentIndexFile -Value $pContent -Encoding UTF8 Write-Host \" [Link] Updated links in parent _index.md\" -ForegroundColor Green } # Process week folders $weekFolders = Get-ChildItem -Path $userFolder.FullName -Directory | Where-Object { $_.Name -match \"Week_\" } foreach ($weekFolder in $weekFolders) { if ($weekFolder.Name -match \"Week_(\\d+)\") { $weekNum = $matches[1] $correctFolderName = \"$userPrefix.$weekNum-Week_$weekNum\" $currentFolderPath = $weekFolder.FullName if ($weekFolder.Name -ne $correctFolderName) { Rename-Item -Path $currentFolderPath -NewName $correctFolderName $currentFolderPath = Join-Path $userFolder.FullName $correctFolderName Write-Host \" [Folder] Renamed to: $correctFolderName\" -ForegroundColor Yellow } Fix-IndexFileName $currentFolderPath $childIndexFile = Join-Path $currentFolderPath \"_index.md\" if (Test-Path $childIndexFile) { $cContent = Get-Content -Path $childIndexFile -Raw -Encoding UTF8 $newPre = \"pre = `\" \u003cb\u003e $userPrefix.$weekNum. \u003c/b\u003e `\"\" if ($cContent -match '(?m)^pre\\s*=') { $cContent = $cContent -replace '(?m)^pre\\s*=\\s*\".*\"', $newPre } else { $cContent = $cContent -replace '(?m)^weight\\s*=\\s*(\\d+)', \"weight = `$1`n$newPre\" } Set-Content -Path $childIndexFile -Value $cContent -Encoding UTF8 Write-Host \" [Pre] Updated pre to: $userPrefix.$weekNum.\" -ForegroundColor Gray } } } } } Write-Host \"`n=== COMPLETED! PLEASE RUN: hugo server -D ===\" -ForegroundColor Green\r3.3 Create multiple folders at once\n$basePath = \"D:\\IT\\AWS-FCJ\\AWS-Workshop\\content\\5-Workshop\" $folders = @( \"5.1-Workshop_Overview\", \"5.2-Prerequisite\", \"5.3-Deploy_Flow\", \"5.4-Clean_Up\" ) foreach ($f in $folders) { $fullPath = Join-Path $basePath $f New-Item -ItemType Directory -Path $fullPath -Force | Out-Null New-Item -ItemType File -Path (Join-Path $fullPath \"_index.md\") -Force | Out-Null }\r3.4 Create a single folder\n$basePath = \"D:\\IT\\AWS-FCJ\\AWS-Workshop\\content\\5-Workshop\" $folderName = \"5.1-Workshop_Overview\" $fullPath = Join-Path $basePath $folderName New-Item -ItemType Directory -Path $fullPath -Force | Out-Null New-Item -ItemType File -Path (Join-Path $fullPath \"_index.md\") -Force | Out-Null\r3.5 Display the entire project structure\ntree /f /a\r3.6 Display a specific directory structure\ntree content/1-Worklog/1.1-PhanCanhTuanDat /F\r3.7 Recreate gh-pages worktree safely\n# 0. Remove current public folder Remove-Item -Recurse -Force .\\public # 1. Remove old gh-pages worktree (even if folder no longer exists) git worktree remove \"D:/IT/AWS-FCJ/AWS-Workshop/public/public\" --force # 2. Clean orphaned worktree metadata git worktree prune # 3. Verify remaining worktrees git worktree list # 4. Recreate gh-pages worktree at /public git worktree add -B gh-pages public origin/gh-pages # 5. Verify status cd public git status",
    "description": "1. Hugo Commands\r1.1\nhugo build\r1.2\nhugo server -D\r2. Git Commands\r2.1 Check repository status\ngit status\r2.2 List local branches\ngit branch\r2.3 List all branches (local + remote)",
    "tags": [],
    "title": "Mystic Skills",
    "uri": "/en/5-workshop/5.5-mystic-skills/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Workshop \u003e Deploy Flow \u003e Frontend Deploy",
    "content": "STAGE 4: S3 PERMITS (POLICY)\rCloudFront needs a “permission” to pull files from your 2 closed buckets.\nIn CloudFront \u003e Origins tab.\nSelect Origin Singapore \u003e Edit \u003e Copy policy.\nOpen a new tab \u003e S3 Console \u003e Bucket sgutodolist-frontend-sg.\nPermissions tab \u003e Bucket Policy \u003e Edit \u003e Paste \u003e Save.\nRepeat with Bucket Virginia:\nGo back to CloudFront \u003e Select Origin Virginia \u003e Edit \u003e Copy policy.\nGo to S3 sgutodolist-frontend-us \u003e Permissions \u003e Bucket Policy \u003e Paste \u003e Save.\n⬅ STEP 4: ClouFront and Failover\rSTEP 6: DNS Record ➡",
    "description": "STAGE 4: S3 PERMITS (POLICY)\rCloudFront needs a “permission” to pull files from your 2 closed buckets.\nIn CloudFront \u003e Origins tab.\nSelect Origin Singapore \u003e Edit \u003e Copy policy.\nOpen a new tab \u003e S3 Console \u003e Bucket sgutodolist-frontend-sg.\nPermissions tab \u003e Bucket Policy \u003e Edit \u003e Paste \u003e Save.\nRepeat with Bucket Virginia:",
    "tags": [],
    "title": "S3 Policy",
    "uri": "/en/5-workshop/5.3-deploy_flow/5.3.1-frontend-deploy/5.3.1.5-s3-policy/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Workshop \u003e Deploy Flow \u003e Backend Deploy",
    "content": "This phase deploys containerized applications to ECS Fargate following a specific order to ensure proper dependency resolution.\nCritical Network Configuration\rFor ALL service deployments, the following network settings are mandatory:\n1. VPC: SGU-Microservices-VPC\n2. Subnets: Select both Public Subnets (required for image pulling)\n3. Security Group: ecs-app-sg (remove default)\n4. Public IP: Turned on (critical for ECR access)\nDeployment Priority Order\rServices must be deployed in the following sequence to respect dependencies:\nGroup 1: Business Logic Services (Deploy first - required by API Gateway)\n1. Auth Service\n2. User Service\n3. Taskflow Service\n4. Notification Service\nGroup 2: Internal Services (Deploy second) 5. AI Model Service\nGroup 3: Entry Point (Deploy last) 6. API Gateway\nGroup 1: Business Logic Services Deployment\rThese services require both ALB integration (for external access) and Service Discovery (for internal communication).\nDeployment Parameters Reference Table:\nService Task Definition Service Name Target Group Service Discovery Name Auth auth-service-td auth-service auth-tg auth User user-service-td user-service user-tg user Taskflow taskflow-service-td taskflow-service task-tg taskflow Notification notification-service-td notification-service noti-tg notification Standard Deployment Process (Repeat 4 times):\n1. Navigate to ECS Cluster → Services → Create\n2. Service Configuration:\n- Task definition family: Select corresponding task definition (e.g., auth-service-td)\n- Revision: Latest\n- Service name: Enter service name (e.g., auth-service)\n3. Compute Configuration:\n- Compute options: Capacity provider strategy\n- Capacity provider strategy: Use custom (Advanced)\n- Capacity provider: FARGATE\n- Base: 0\n- Weight: 1\n- Platform version: LATEST\n4. Deployment Configuration:\n- Application type: Service\n- Scheduling strategy: Replica\n- Desired tasks: 1\n- Desired tasks: 1\n5. Networking:\n- VPC: SGU-Microservices-VPC\n- Subnets: Select both Public Subnets\n- SGU-Microservices-subnet-public1-ap-southeast-1a\n- SGU-Microservices-subnet-public2-ap-southeast-1b\n- Security group: Remove default, select ecs-app-sg\n- Public IP: Turned on\n6. Service Connect:\n- Do not enable (using traditional DNS-based Service Discovery)\n7. Service Discovery:\n- Enable: Use service discovery\n- Configure namespace: Select an existing namespace\n- Namespace: sgu.local\n- Configure service discovery service: Select an existing service discovery service\n- Existing service discovery service: Choose discovery name (e.g., auth)\n- Result DNS: auth.sgu.local (for example)\n8. Load Balancing:\n- Enable: Use load balancing\n- Load balancer type: Application Load Balancer\n- Load balancer: sgu-alb\n- Container to load balance: Select service container (e.g., auth-service 9999:9999)\n- Listener: Use an existing listener → HTTPS:443\n- Target group: Use an existing target group → Select corresponding TG (e.g., auth-tg)\n9. Service Auto Scaling:\n- Disable (for cost optimization)\n10. Click Create\nWait for each service to reach RUNNING state before deploying the next one.\nGroup 2: AI Model Service Deployment\rThis is an internal service accessed only through API Gateway.\nConfiguration:\n1. Navigate to ECS Cluster → Services → Create\n2. Service Configuration:\n- Task definition: ai-model-service-td\n- Service name: ai-model-service\n- Desired tasks: 1\n3. Networking:\n- VPC: SGU-Microservices-VPC\n- Subnets: Both Public Subnets\n- Security group: ecs-app-sg\n- Public IP: Turned on\n4. Load Balancing:\n- Disable (internal service only)\n5. Service Discovery:\n- Enable: Use service discovery\n- Namespace: sgu.local\n- Service discovery name: ai-model\n- Result DNS: ai-model.sgu.local\n6. Click Create\nGroup 3: API Gateway Deployment\rThis is the main entry point that routes traffic to all backend services.\nConfiguration:\n1. Navigate to ECS Cluster → Services → Create\n2. Service Configuration:\n- Task definition: api-gateway-td\n- Service name: api-gateway\n- Desired tasks: 1\n3. Compute Configuration:\n- Same as Group 1 services\n4. Networking:\n- VPC: SGU-Microservices-VPC\n- Subnets: Both Public Subnets\n- Security group: ecs-app-sg\n- Public IP: Turned on\n5. Service Discovery:\n- Enable: Use service discovery\n- Namespace: sgu.local\n- Service discovery name: api-gateway\n6. Load Balancing:\n- Enable: Use load balancing\n- Load balancer type: Application Load Balancer\n- Load balancer: sgu-alb\n- Container to load balance: api-gateway:8080\n- Listener: Use an existing listener → HTTP:80\n- Target group: Use an existing target group → api-gateway-tg\n7. Click Create\nDeployment Monitoring and Troubleshooting\rAfter deploying all services, monitor their status in the ECS Cluster.\nExpected Task States:\nNavigate to Cluster → Tasks tab\n1. Normal progression: PROVISIONING → PENDING → RUNNING\n2. PENDING state: 1-2 minutes is normal (downloading image from ECR)\n3. RUNNING state: Task is healthy and serving traffic\nCommon Issues and Resolution:\nIssue 1: Task Status = STOPPED\rSymptom: Task stops immediately after starting\nDiagnosis Steps:\n1. Click on the stopped task ID\n2. Check Stopped reason field\n3. Switch to Logs tab for detailed error messages\nCommon Causes:\nError Message Cause Solution Connection refused Incorrect RDS/Redis endpoint in environment variables Verify endpoint values in Task Definition CannotPullContainerError Missing Public IP or subnet without internet access Ensure Public IP is enabled and using Public Subnets Health check failed Service startup time exceeds ALB timeout Increase health check interval and timeout in Target Group settings ResourceInitializationError Insufficient CPU/Memory Verify task size configuration Solution for Health Check Failures:\n1. Navigate to Target Group → Select the failing TG\n2. Health checks tab → Edit\n3. Increase values:\n- Interval: 60 seconds\n- Timeout: 30 seconds\n- Healthy threshold: 3\n4. Save changes\nIssue 2: Container Keeps Restarting\rPossible Causes:\n- Database connection failures\n- Missing environment variables\n- Application configuration errors\nDiagnosis:\n1. Check CloudWatch Logs for the task\n2. Verify all environment variables are correctly set\n3. Confirm RDS and Redis are accessible from ECS security group\nDeployment Verification Checklist\rBefore proceeding to final configuration:\nServices Status:\n- [ ] Auth Service: RUNNING (1/1 tasks)\n- [ ] User Service: RUNNING (1/1 tasks)\n- [ ] Taskflow Service: RUNNING (1/1 tasks)\n- [ ] Notification Service: RUNNING (1/1 tasks)\n- [ ] AI Model Service: RUNNING (1/1 tasks)\n- [ ] API Gateway: RUNNING (1/1 tasks)\nTarget Groups Health:\n- [ ] auth-tg: 1 healthy target\n- [ ] user-tg: 1 healthy target\n- [ ] task-tg: 1 healthy target\n- [ ] noti-tg: 1 healthy target\n- [ ] api-gateway-tg: 1 healthy target\nService Discovery:\n- [ ] All services registered in Cloud Map\n- [ ] DNS names resolving correctly (verify in Route 53)\n⬅ BƯỚC 4: Task Definitions Creation\rBƯỚC 6: Completion \u0026 Verification ➡",
    "description": "This phase deploys containerized applications to ECS Fargate following a specific order to ensure proper dependency resolution.\nCritical Network Configuration\rFor ALL service deployments, the following network settings are mandatory:\n1. VPC: SGU-Microservices-VPC\n2. Subnets: Select both Public Subnets (required for image pulling)\n3. Security Group: ecs-app-sg (remove default)\n4. Public IP: Turned on (critical for ECR access)",
    "tags": [],
    "title": "Services Deployment ",
    "uri": "/en/5-workshop/5.3-deploy_flow/5.3.2-backend-deploy/5.3.2.5-services-deployment/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Worklog",
    "content": "Week 5 Objectives\rContinue frontend and backend development Learn EC2 Auto Scaling Group Practice AWS CloudShell setups Translate blogs. Tasks to be carried out this week\rDay Task Start Date Completion Date Reference Material 1 - Team meeting to discuss about the development process 06/10/2025 06/10/2025 - Translate blogs Translated Blog 6, Translated Blog 7 - Code backend Authentication Service, User Service - Install state management library (Redux), define the initial State Model, integrate API services for Login/Register pages. 2 - Learn and practice EC2 Auto Scaling Group 07/10/2025 07/10/2025 Module-03-02 - EC2 Autoscaling - EFS/FSx - Lightsail - MGN, AWS EC2 Auto Scaling : Step By Step Tutorial - Code backend Authentication Service, User Service, Taskflow Service - Code frontend: Build UI for displaying main data lists (table, grid). Integrate Get List API and Pagination. - Translate blog Translated Blog 8, Translated Blog 9 3 - Code backend Authentication Service, User Service 08/10/2025 08/10/2025 - Learn and pratice Kafka for messaging - Code frontend: Build UI for displaying main data lists (table, grid). Integrate Get List API and Pagination. - Update worklog 4 - Code backend Authentication Service, User Service 09/10/2025 09/10/2025 - Code frontend: Build UI for displaying main data lists (table, grid). Integrate Get List API and Pagination. - Learn about Caching to apply cache for database - Translate blog Translated Blog 10, Translated Blog 11 5 - Learn and practice using CloudShell 10/10/2025 10/10/2025 AWS Certified Cloud Practitioner Certification Course (CLF-C02) - Translate blog Translated Blog 12",
    "description": "Week 5 Objectives\rContinue frontend and backend development Learn EC2 Auto Scaling Group Practice AWS CloudShell setups Translate blogs. Tasks to be carried out this week\rDay Task Start Date Completion Date Reference Material 1 - Team meeting to discuss about the development process 06/10/2025 06/10/2025 - Translate blogs Translated Blog 6, Translated Blog 7 - Code backend Authentication Service, User Service - Install state management library (Redux), define the initial State Model, integrate API services for Login/Register pages. 2 - Learn and practice EC2 Auto Scaling Group 07/10/2025 07/10/2025 Module-03-02 - EC2 Autoscaling - EFS/FSx - Lightsail - MGN, AWS EC2 Auto Scaling : Step By Step Tutorial - Code backend Authentication Service, User Service, Taskflow Service - Code frontend: Build UI for displaying main data lists (table, grid). Integrate Get List API and Pagination. - Translate blog Translated Blog 8, Translated Blog 9 3 - Code backend Authentication Service, User Service 08/10/2025 08/10/2025 - Learn and pratice Kafka for messaging - Code frontend: Build UI for displaying main data lists (table, grid). Integrate Get List API and Pagination. - Update worklog 4 - Code backend Authentication Service, User Service 09/10/2025 09/10/2025 - Code frontend: Build UI for displaying main data lists (table, grid). Integrate Get List API and Pagination. - Learn about Caching to apply cache for database - Translate blog Translated Blog 10, Translated Blog 11 5 - Learn and practice using CloudShell 10/10/2025 10/10/2025 AWS Certified Cloud Practitioner Certification Course (CLF-C02) - Translate blog Translated Blog 12",
    "tags": [],
    "title": "Week 5 Worklog",
    "uri": "/en/1-worklog/1.5-week_5/index.html"
  },
  {
    "breadcrumb": "Internship Report",
    "content": "Cost-Optimized SaaS Task Management Platform\rOverview\rSGU TodoList is a robust and resilient task management application built using a Microservices architecture designed for high scalability and decoupling. The primary goal of this workshop is to guide you through deploying and managing a complex application stack on the AWS Cloud platform, with a strong focus on cost optimization and serverless operations.\nBy leveraging core AWS services such as ECS Fargate, S3, ALB, and VPC, you will learn how to set up an infrastructure that is performant, highly available, and minimizes overhead costs compared to traditional EC2 instances.\nKey Technologies and Concepts\rThis workshop covers practical implementation using:\nArchitecture: Microservices, Event-Driven (Kafka). Compute: AWS ECS Fargate (Serverless Containers). Backend: Spring Boot (Java 21). Frontend: ReactJS. Data Stores: AWS RDS (MySQL), ElastiCache (Redis). Networking \u0026 Discovery: AWS ALB, AWS Cloud Map (Service Discovery). Deployment: AWS CLI and PowerShell Automation Scripts. Table of Contents\r1. Workshop Overview - Goals and learning outcomes.\n2. Prerequisite - Tools and AWS permissions required for deployment.\n3. Deploy Flow - Step-by-step guide on deploying Backend and Frontend.\n4. Clean Up - Instructions to tear down all resources to avoid charges.\n5. Mystic Skills - Additional mystic skills",
    "description": "Cost-Optimized SaaS Task Management Platform\rOverview\rSGU TodoList is a robust and resilient task management application built using a Microservices architecture designed for high scalability and decoupling. The primary goal of this workshop is to guide you through deploying and managing a complex application stack on the AWS Cloud platform, with a strong focus on cost optimization and serverless operations.",
    "tags": [],
    "title": "Workshop",
    "uri": "/en/5-workshop/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Translated Blogs",
    "content": "Kích hoạt phân tích dữ liệu Genomic và Multiomic nhanh chóng với Illumina DRAGEN™ v4.4 trên các instance Amazon EC2 F2\rEric Allen, Mark Azadpour, Deepthi Shankar, Olivia Choudhury, và Shyamal Mehtalia | 15/07/2025 | High Performance Computing, Life Sciences, Partner solutions\nBài viết này được đóng góp bởi Eric Allen (AWS), Olivia Choudhury (AWS), Mark Azadpour (AWS), Deepthi Shankar (Illumina), và Shyamal Mehtalia (Illumina)\nViệc phân tích lượng dữ liệu genomic và multiomic ngày càng gia tăng đòi hỏi các giải pháp tính toán hiệu quả, có khả năng mở rộng và tiết kiệm chi phí. Amazon Web Services (AWS) tiếp tục hỗ trợ các workload này thông qua các dịch vụ tính toán tăng tốc FPGA như các instance Amazon EC2 F2.\nGiải pháp phân tích thứ cấp DRAGEN (Dynamic Read Analysis for GENomics) của Illumina đã khẳng định vị thế là một trong những giải pháp phân tích thứ cấp hàng đầu cho dữ liệu sequencing thế hệ mới, cung cấp các thuật toán được tối ưu hóa cao cho phân tích genomic và triển khai tăng tốc phần cứng cho các pipeline phân tích toàn diện về genomics và multiomics, bao gồm DNA germline, DNA somatic, cũng như phân tích RNA ở mức bulk và single-cell, proteomics, spatial, và nhiều hơn nữa.\nDRAGEN chạy nguyên bản trên các instance EC2 F2 và cung cấp cho khách hàng một phương pháp nhanh chóng để phân tích tập dữ liệu sinh học của họ. Việc di chuyển sang F2 được đơn giản hóa vì cùng một DRAGEN AMI được sử dụng cho cả F1 và F2, và kết quả phân tích sẽ tương đương nhau. Trong bài viết này, chúng tôi sẽ thảo luận về các đặc tính hiệu năng của DRAGEN trên các môi trường tính toán AWS khác nhau, cũng như cách triển khai phiên bản DRAGEN v4.4 mới ra mắt gần đây trên các instance Amazon EC2 F2.\nTổng quan về các instance Amazon EC2 F2\rCác instance F2 là thế hệ thứ 2 của các instance EC2 được trang bị FPGA để tăng tốc phân tích dữ liệu genomic và multimedia trong môi trường cloud. Những instance này mang lại cải tiến đáng kể so với thế hệ trước — các instance F1 — với hiệu suất giá thành được cải thiện tốt hơn tới 60%. Dưới đây là các tính năng và thông số kỹ thuật chính của instance F2:\nCấu hình FPGA: Instance F2 được trang bị tối đa tám AMD Virtex UltraScale+ HBM VU47P FPGAs, mỗi FPGA tích hợp 16GB bộ nhớ băng thông cao (HBM – High-Bandwidth Memory).\nBộ xử lý: Được vận hành bởi bộ vi xử lý AMD EPYC thế hệ thứ 3 (Milan), instance F2 cung cấp tới 192 vCPU — gấp ba lần số lõi xử lý so với instance F1.\nBộ nhớ: Instance này hỗ trợ tới 2 TiB bộ nhớ hệ thống, gấp đôi dung lượng bộ nhớ của instance F1.\nLưu trữ: F2 đi kèm với tối đa 7.6 TiB bộ nhớ SSD NVMe, gấp đôi dung lượng lưu trữ của F1.\nNetworking: Tốc độ băng thông mạng lên tới 100 Gbps, cao gấp bốn lần so với băng thông mạng có sẵn trên instance F1.\nInstance Name\rFPGAs\rvCPUs\rInstance Memory\rNVMe Storage\rNetwork Bandwidth\rf1.2xlarge\r1\r8\r122\r470\rUp to 10 Gbps\rf1.4xlarge\r2\r16\r244\r940\r10 Gbps\rf2.6xlarge\r1\r24\r256\r950\r10 Gbps\rF2.12xlarge\r2\r48\r512\r1900\r25 Gbps\rf1.16xlarge\r8\r64\r976\r44x940\r25 Gbps\rF2.48xlarge\r8\r192\r2048\r7600\r100 Gbps\rBảng 1: Bảng so sánh thông số kỹ thuật về compute, memory, storage, và networking giữa các instance F1 và F2.\nPhương pháp đánh giá hiệu năng\rIllumina khuyến nghị sử dụng f1.4xlarge khi dùng instance F1 và f2.6xlarge khi dùng instance F2. Để đánh giá hiệu năng trên các instance này, DRAGEN đã được cấu hình và chạy trên AWS theo hướng dẫn người dùng Illumina DRAGEN, và các liên kết tới genome reference file có thể tìm thấy trên trang web Illumina DRAGEN Product Files.\nPhân tích Whole Genome Sequencing (WGS) với độ phủ khoảng 35x sử dụng một mẫu có sẵn công khai. Mẫu HG002 từ dự án NIST Genome in a Bottle đã được sử dụng. Mẫu này được phân tích bằng DRAGEN v4.4 Germline pipeline theo hai cách khác nhau. Phân tích “cơ bản” chỉ bao gồm alignment cơ bản và small variant calling, nhằm tạo điều kiện so sánh với các pipeline tin sinh học phổ biến cho mẫu germline như BWA/GATK. Phân tích “đầy đủ” sử dụng tất cả các variant caller, bao gồm cả copy number và structural variants, cùng các tùy chọn bổ sung như gọi pharmacogenetic star allele và gọi HLA (Human Leukocyte Antigen), để tạo ra một bộ genome được phân tích đầy đủ. Trong cả hai trường hợp, tham chiếu đồ thị multigenome hg38 của DRAGEN được sử dụng cho phân tích WGS. Các file dữ liệu fastq của mẫu có thể truy cập trên Amazon S3 qua các liên kết: fastq R1, fastq R2.\nPhân tích Tumor-Normal sử dụng một cặp mẫu đã được khảo sát trong một ấn phẩm về phân tích ung thư của DRAGEN trước đây. Hai mẫu này có độ phủ khoảng 110x (tumor) và 40x (normal). Các mẫu được phân tích bằng DRAGEN v4.4 Somatic pipeline, bao gồm alignment, small variant calling, cũng như phân tích CNV và SV. Phân tích Tumor-Normal sử dụng hg38 linear genome reference của DRAGEN. Các file dữ liệu fastq của mẫu có thể truy cập trên Amazon S3 qua các liên kết: Tumor fastq R1, Tumor fastq R2, Normal fastq R1, Normal fastq R2.\nSo sánh hiệu năng về tốc độ và chi phí: Phân tích WGS\rPhân tích WGS sử dụng DRAGEN v4.4 cho thấy lợi thế đáng kể về hiệu suất chi phí trên các instance Amazon EC2 F2 so với F1, đồng thời vẫn cho ra kết quả phân tích tương đương giữa hai thế hệ instance¹:\nPhân tích WGS cơ bản, bao gồm alignment và small variant calling. DRAGEN v4.4 chạy trên f2.6xlarge đạt tốc độ nhanh hơn 1,5 lần và chỉ tốn 40% chi phí compute EC2 so với f1.4xlarge.\nPhân tích WGS đầy đủ, bao gồm alignment, small variant calling, calling of CNVs, SVs, repeat expansions, và variant annotation. Phân tích DRAGEN trên f2.6xlarge đạt tốc độ nhanh gấp 2 lần và chỉ tốn 30% chi phí compute EC2 so với f1.4xlarge.\nHình 1: Instance f2.6xlarge nhanh hơn 1.5 lần trong Phân tích WGS cơ bản và nhanh hơn 2.1 lần trong Phân tích WGS đầy đủ so với f1.4xlarge.\nHình 2: Chi phí compute EC2 trên f2.6xlarge chỉ bằng 40% chi phí trên f1.4xlarge cho Phân tích WGS cơ bản và bằng 30% chi phí trên f1.4xlarge cho Phân tích WGS đầy đủ.\nSo sánh hiệu năng về tốc độ và chi phí: Phân tích Tumor-Normal\rGiống như kết quả WGS, trong phân tích Tumor-Normal, DRAGEN v4.4 cũng cho thấy lợi thế đáng kể về hiệu suất chi phí trên các instance Amazon EC2 F2 so với F1, đồng thời vẫn tạo ra kết quả phân tích tương đương giữa hai thế hệ instance¹:\nPhân tích Tumor-Normal, bao gồm alignment, small variant calling và calling CNVs/SVs. Phân tích bằng DRAGEN trên f2.6xlarge đạt tốc độ nhanh hơn 1.7 lần và chỉ tốn 35% chi phí compute EC2 so với f1.4xlarge. Hình 3: Instance f2.6xlarge nhanh hơn 1.7 lần so với f1.4xlarge trong Phân tích Tumor-Normal.\nHình 4: Chi phí compute EC2 trên f2.6xlarge chỉ bằng 35% chi phí trên f1.4xlarge cho Phân tích Tumor-Normal WGS.\nCác lợi ích khác\rCác pipeline phân tích genomic truyền thống sử dụng BWA-MEM và GATK chạy trên CPU từng là tiêu chuẩn công nghiệp trong quá khứ, nhưng DRAGEN đã ngày càng được ưa chuộng nhờ những ưu thế vượt trội về tốc độ và độ chính xác. Nhiều công bố đã được bình duyệt đã so sánh tốc độ và độ chính xác của DRAGEN với các pipeline dựa trên BWA/GATK chạy trên CPU. Ví dụ, Ziegler et al. (2022) đã phát hiện ra rằng phân tích DRAGEN trên phần cứng FPGA nhanh hơn gấp hơn 8 lần và chính xác hơn so với các pipeline dựa trên BWA/GATK chạy trên CPU, trong khi Sedlazek et al. (2024) cũng ghi nhận DRAGEN cho hiệu suất và độ chính xác cao hơn so với các pipeline dựa trên BWA/GATK.\nViệc sử dụng DRAGEN trên các instance F, được tăng tốc bởi FPGA, còn mang lại lợi thế về tiêu thụ điện năng so với các giải pháp truyền thống dựa trên CPU và GPU. FPGA vốn dĩ tiết kiệm năng lượng hơn, tiêu thụ ít điện năng hơn nhưng vẫn đạt hiệu năng tính toán tương đương cho các workload này. Điều này đặc biệt quan trọng trong các tác vụ phân tích dữ liệu genomic, nơi khối lượng dữ liệu và thời gian xử lý có thể rất lớn.\nChẳng hạn, FPGA đạt được hiệu suất trên mỗi watt cao hơn so với CPU và GPU trong các tác vụ WGS của DRAGEN. Các bộ tăng tốc dựa trên FPGA có thể cung cấp thông lượng vượt trội với mức tiêu thụ điện năng thấp hơn. Điều này là nhờ khả năng tùy chỉnh linh hoạt của FPGA, cho phép cấu hình tối ưu hóa nhằm nâng cao hiệu quả năng lượng. Ngược lại, CPU và GPU, dù mạnh mẽ, thường tiêu tốn nhiều năng lượng hơn để thực hiện cùng một tác vụ, dẫn đến chi phí vận hành cao hơn và tác động môi trường lớn hơn.\nViệc tiêu thụ điện năng thấp hơn của các FPGA dẫn đến giảm các yêu cầu về làm mát, đây có thể là một yếu tố chi phí đáng kể trong các môi trường điện toán quy mô lớn. Ngoài ra, hiệu quả năng lượng của FPGA khiến chúng trở thành lựa chọn hấp dẫn cho các ứng dụng điện toán hiệu năng cao, nơi khả năng mở rộng và hiệu quả chi phí đóng vai trò then chốt.\nTóm lại, việc triển khai DRAGEN trên các instance thuộc họ ‘F’ của Amazon EC2 mang lại một giải pháp tiết kiệm năng lượng hơn cho phân tích dữ liệu genomic so với các phương pháp truyền thống dựa trên CPU hoặc GPU, đồng thời mang lại cả lợi ích về chi phí lẫn lợi ích môi trường.\nKhả năng triển khai và các tùy chọn triển khai trên cloud của instance F2\rCác instance Amazon EC2 F2 hiện đã có sẵn tại nhiều Region của AWS, bao gồm US East (N. Virginia), US West (Oregon), Europe (London) và Asia Pacific (Sydney), với kế hoạch mở rộng thêm sang nhiều Region khác trong tương lai. F2 cung cấp nhiều kích cỡ khác nhau như f2.6xlarge, f2.12xlarge và f2.48xlarge, nhằm đáp ứng đa dạng nhu cầu workload.\nKhi cấu hình lưu trữ và compute để chạy các workload DRAGEN trên các instance F của AWS, bạn cần lựa chọn các tùy chọn phù hợp để cân bằng giữa hiệu năng và chi phí. Hãy cân nhắc các tùy chọn lưu trữ như là Amazon EBS gp3 volumes được cấu hình theo RAID, Amazon FSx for Lustre cho thông lượng cao hơn và Amazon Elastic File System (EFS) cho lưu trữ liên tục. Ngoài ra, việc truyền trực tuyến các tệp BAM và dữ liệu tham chiếu từ Amazon Simple Storage Service (Amazon S3) hoặc sử dụng Mountpoint for Amazon S3 để mount bucket S3 vào hệ thống file cục bộ giúp truy cập dữ liệu một cách tiết kiệm chi phí và dễ dàng. Bằng cách lựa chọn và cấu hình cẩn thận các giải pháp lưu trữ này, bạn có thể đảm bảo hiệu năng tối ưu và hiệu quả chi phí cho các workload HPC của mình. Bên cạnh đó, bạn cũng nên cân nhắc sử dụng Illumina Connected Analytics (ICA) hoặc AWS Batch để quản lý workflow. Illumina cung cấp hướng dẫn chi tiết về cách triển khai DRAGEN trên AWS trong tài liệu hướng dẫn người dùng DRAGEN trực tuyến của họ.\nKết luận\rTóm lại, các instance Amazon EC2 F2 đánh dấu một bước tiến đáng kể trong lĩnh vực điện toán đám mây được cung cấp bởi FPGA, mang lại hiệu năng, bộ nhớ, dung lượng lưu trữ và khả năng kết nối mạng vượt trội so với thế hệ trước. Sự kết hợp giữa các pipeline toàn diện của DRAGEN và sức mạnh tính toán được nâng cấp của Amazon EC2 F2 cho phép xử lý nhanh hơn, hiệu quả hơn đối với các bộ dữ liệu phức tạp – từ whole genome sequencing đến phân tích single-cell RNA.\nHãy bắt đầu chuyển đổi sang F2 ngay hôm nay. Vui lòng liên hệ với đội ngũ tài khoản AWS của bạn hoặc Illumina để được hỗ trợ trong quá trình chuyển đổi sang Amazon EC2 F2 instances.\nĐể biết thêm thông tin về DRAGEN trên AWS, hãy tìm kiếm DRAGEN Complete Suite trên AWS Marketplace, xem blog DRAGEN v4.4, hoặc truy cập trang chủ Illumina Informatics Solutions.\nTài liệu tham khảo\rScheffler, K. et al. “Somatic small-variant calling methods in Illumina DRAGEN™ Secondary Analysis.” BioRxiv (2023). https://www.biorxiv.org/content/10.1101/2023.03.23.534011v2\nSedlazeck, F. J. et al. “Comprehensive genome analysis and variant detection at scale using DRAGEN.” Nature Biotechnology (2024). https://www.nature.com/articles/s41587-024-02382-1\nZiegler, A. et al. “Comparison of calling pipelines for whole genome sequencing: an empirical study demonstrating the importance of mapping and alignment.” Scientific Reports (2022). https://pubmed.ncbi.nlm.nih.gov/36513709/\nChú thích\rSự tương đương giữa F1 và F2 hard-filtered.vcf cùng các tệp kết quả khác dựa trên kết quả phân tích sử dụng DRAGEN 4.4.4 theo hướng dẫn sử dụng DRAGEN của Illumina. Lệnh vim diff cho thấy sự khác biệt duy nhất giữa hai tệp VCF là một mục hiển thị thời gian thực hiện phân tích. Khách hàng có thể tự thực hiện các phân tích tương tự để xác minh hoặc liên hệ với Illumina để biết thêm thông tin.",
    "description": "Kích hoạt phân tích dữ liệu Genomic và Multiomic nhanh chóng với Illumina DRAGEN™ v4.4 trên các instance Amazon EC2 F2\rEric Allen, Mark Azadpour, Deepthi Shankar, Olivia Choudhury, và Shyamal Mehtalia | 15/07/2025 | High Performance Computing, Life Sciences, Partner solutions\nBài viết này được đóng góp bởi Eric Allen (AWS), Olivia Choudhury (AWS), Mark Azadpour (AWS), Deepthi Shankar (Illumina), và Shyamal Mehtalia (Illumina)",
    "tags": [],
    "title": "Blog 6",
    "uri": "/en/3-translated_blogs/blog_6/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Workshop \u003e Deploy Flow \u003e Backend Deploy",
    "content": "This final phase connects the deployed backend to the public domain and verifies end-to-end functionality.\nDNS Configuration\rStep 1: Create Route 53 A Record\nNavigate to Route 53 → Hosted zones Select sgutodolist.com hosted zone Click Create record Configure record: Record name: api Record type: A Alias: Enable Route traffic to: Alias to Application Load Balancer Region: Asia Pacific (Singapore) Load balancer: Select sgu-alb Click Create records DNS Propagation: Wait 2-5 minutes for DNS changes to propagate.\nVerification:\nbash\nnslookup sgutodolist.com\r# Should return ALB's IP addresses\rGoogle OAuth Configuration Update\rStep 1: Update Authorized Redirect URIs\nAccess Google Cloud Console (console.cloud.google.com) Navigate to APIs \u0026 Services → Credentials Select your OAuth 2.0 Client ID Under Authorized redirect URIs, add: https://sgutodolist.com/api/auth/login/oauth2/code/google\rClick Save Note: Keep existing localhost URIs for local development.\nSystem Health Verification\rPerform the following tests to verify deployment success:\nTest 1: API Gateway Health Check\rbash\ncurl https://sgutodolist.com/actuator/health\rExpected Response:\njson\n{\r\"status\": \"UP\"\r}\rTest 2: Individual Service Health Checks\rbash\n# Auth Service\rcurl https://sgutodolist.com/api/auth/actuator/health\r# User Service\rcurl https://sgutodolist.com/api/user/actuator/health\r# Taskflow Service\rcurl https://sgutodolist.com/api/taskflow/actuator/health\r# Notification Service\rcurl https://sgutodolist.com/api/notification/actuator/health\rAll should return {\"status\":\"UP\"}.\nTest 3: Service Discovery Verification\rFrom the Bastion Host, verify internal DNS resolution:\nbash\n# SSH to Bastion\rssh -i sgutodolist-key.pem ec2-user@[BASTION-IP]\r# Test DNS resolution\rnslookup auth.sgu.local\rnslookup user.sgu.local\rnslookup taskflow.sgu.local\rnslookup notification.sgu.local\rnslookup ai-model.sgu.local\rnslookup kafka.sgu.local\rAll should resolve to internal ECS task IP addresses.\nTest 4: Database Connectivity\rVerify services can connect to RDS:\nCheck CloudWatch Logs for any service Look for successful database connection messages Verify no connection errors in startup logs Test 5: Redis Connectivity\rbash\n# From Bastion Host\rredis-cli -h [REDIS-ENDPOINT] ping\r# Expected response: PONG\rTest 6: End-to-End Authentication Flow\rAccess frontend at https://sgutodolist.com Click “Sign in with Google” Complete OAuth flow Verify successful login and token issuance Verify user profile loads correctly Performance Baseline\rRecord initial performance metrics:\nResponse Time Benchmarks:\nbash\n# API Gateway response time\rtime curl -o /dev/null -s https://sgutodolist.com/actuator/health\r# Auth service response time\rtime curl -o /dev/null -s https://sgutodolist.com/api/auth/actuator/health\rCloudWatch Metrics to Monitor:\nECS Task CPU Utilization ECS Task Memory Utilization ALB Target Response Time ALB Request Count RDS CPU Utilization Redis CPU Utilization Post-Deployment Security Checklist\rAll sensitive environment variables secured (not in version control) Database password meets complexity requirements Security groups follow least privilege principle SSL/TLS certificates valid and auto-renewal enabled Bastion Host accessible only from authorized IPs CloudWatch Logs retention configured AWS Budget alerts active Final Deployment Checklist\rInfrastructure:\nVPC and subnets operational All 4 security groups correctly configured RDS database accessible and initialized Redis cache operational Kafka service running ALB active with healthy targets Application:\nAll 6 services deployed and running Service Discovery functional ALB routing rules working correctly Health checks passing CloudWatch Logs collecting data Integration:\nDNS record pointing to ALB SSL certificate valid Google OAuth configured Frontend can communicate with backend Authentication flow working Monitoring:\nCloudWatch dashboards created Budget alerts configured Performance baseline recorded Known Limitations and Future Improvements\rCurrent Architecture Constraints:\nSingle-AZ Database: RDS is deployed in a single availability zone for cost optimization Single-Node Redis: No automatic failover for cache layer Single-Node Kafka: Not production-grade for high-throughput scenarios Public Subnet ECS Tasks: Security trade-off for cost savings Recommended Production Enhancements:\nEnable RDS Multi-AZ deployment Implement Redis Cluster Mode with multiple replicas Deploy Kafka with multiple brokers across AZs Add NAT Gateway and move ECS tasks to private subnets Implement AWS WAF on ALB for DDoS protection Enable ECS Service Auto Scaling Implement CI/CD pipeline for automated deployments Troubleshooting Guide\rQuick Diagnosis Commands\rbash\n# Check ECS service status\raws ecs describe-services --cluster [CLUSTER-NAME] --services [SERVICE-NAME] --region ap-southeast-1\r# Check task status\raws ecs describe-tasks --cluster [CLUSTER-NAME] --tasks [TASK-ARN] --region ap-southeast-1\r# View recent logs\raws logs tail /ecs/[SERVICE-NAME] --follow --region ap-southeast-1\r# Check target health\raws elbv2 describe-target-health --target-group-arn [TG-ARN] --region ap-southeast-1\rEmergency Rollback Procedure\rIf deployment fails:\nIdentify failing service: bash\naws ecs list-services --cluster [CLUSTER-NAME] --region ap-southeast-1\rUpdate service to previous task definition revision: bash\naws ecs update-service\\\r--cluster [CLUSTER-NAME]\\\r--service [SERVICE-NAME]\\\r--task-definition [TASK-DEF-FAMILY]:[PREVIOUS-REVISION]\\\r--region ap-southeast-1\rForce new deployment: bash\naws ecs update-service\\\r--cluster [CLUSTER-NAME]\\\r--service [SERVICE-NAME]\\\r--force-new-deployment\\\r--region ap-southeast-1\rSuccess Criteria\rDeployment is considered successful when:\n✅ All 6 ECS services show status: RUNNING ✅ All 5 target groups show: Healthy ✅ https://sgutodolist.com/actuator/health returns HTTP 200 ✅ Frontend at https://sgutodolist.com can authenticate via Google OAuth ✅ CloudWatch Logs show no critical errors ✅ All services accessible via internal DNS (*.sgu.local) ⬅ BƯỚC 5: Code Update \u0026 Image Build",
    "description": "This final phase connects the deployed backend to the public domain and verifies end-to-end functionality.\nDNS Configuration\rStep 1: Create Route 53 A Record\nNavigate to Route 53 → Hosted zones Select sgutodolist.com hosted zone Click Create record Configure record: Record name: api Record type: A Alias: Enable Route traffic to: Alias to Application Load Balancer Region: Asia Pacific (Singapore) Load balancer: Select sgu-alb Click Create records DNS Propagation: Wait 2-5 minutes for DNS changes to propagate.",
    "tags": [],
    "title": "Completion \u0026 Verification (Route 53, Google Console, Final Test)",
    "uri": "/en/5-workshop/5.3-deploy_flow/5.3.2-backend-deploy/5.3.2.6-completion--verification-route-53-google-console-final-test/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Workshop \u003e Deploy Flow \u003e Frontend Deploy",
    "content": "PHASE 5: POINT OFFICIAL DOMAIN (DNS RECORD)\rGo to Route 53 \u003e Hosted zones \u003e sgutodolist.com.\nCreate Record for Root domain:\nClick Create record.\nRecord name: (leave blank).\nType: A.\nAlias: Yes (Swipe the button to the right).\nRoute traffic to: Alias ​​to CloudFront distribution.\nChoose distribution: Select the CloudFront domain (e.g. d123...cloudfront.net).\nClick Create records.\nCreate Record for WWW: Do the same, but fill in www for Record name. ⬅ STEP 5: S3 Policy\rSTEP 7: Deploy and Test ➡",
    "description": "PHASE 5: POINT OFFICIAL DOMAIN (DNS RECORD)\rGo to Route 53 \u003e Hosted zones \u003e sgutodolist.com.\nCreate Record for Root domain:\nClick Create record.\nRecord name: (leave blank).\nType: A.\nAlias: Yes (Swipe the button to the right).\nRoute traffic to: Alias ​​to CloudFront distribution.\nChoose distribution: Select the CloudFront domain (e.g. d123...cloudfront.net).\nClick Create records.",
    "tags": [],
    "title": "DNS Record",
    "uri": "/en/5-workshop/5.3-deploy_flow/5.3.1-frontend-deploy/5.3.1.6-dns-record/index.html"
  },
  {
    "breadcrumb": "Internship Report",
    "content": "During my internship at AMAZON WEB SERVICES VIETNAM COMPANY LIMITED from 08/09/2025 to 09/12/2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in developing a Multi-region SaaS Task Management Web Application, through which I enhanced my skills in Backend Development, AWS Services, and Cloud Architecture. In the backend domain, I strengthened my ability to:\nDevelop RESTful APIs Work with Microservices Architecture Effectively utilize various AWS services I also gained practical experience in designing AWS architectures that are scalable, secure, and cost-optimized for real-world projects.\nIn terms of work ethic, I consistently strived to complete tasks to a high standard, complied with workplace regulations, and actively collaborated with colleagues to improve overall work efficiency.\nDuring this period, I also:\nStrengthened my teamwork skills Worked effectively with other team members Expanded my professional network by connecting with many peers. To objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Technical Competence Applying backend knowledge, building REST APIs, and writing maintainable code 2 Cloud Skills (AWS) Ability to work with AWS services such as EC2, ECS, Lambda, S3, IAM, CloudFront, etc. 3 System Design Ability Understanding of microservices, multi-region architecture, scalability, and best practices 4 Learning Agility Quick adaptation to new tools, frameworks, and cloud concepts 5 Initiative \u0026 Ownership Ability to self-start tasks, research solutions, and handle responsibilities independently 6 Delivery Quality Accuracy, stability, and completeness of delivered tasks or features 7 Time Management Managing workload, meeting deadlines, and prioritizing effectively 8 Workplace Discipline Punctuality, adherence to team processes, and compliance with company policies 9 Growth Mindset Openness to feedback, willingness to iterate, and continuous improvement 10 Communication Skills Clarity in reporting progress, documenting work, and discussing issues 11 Team Collaboration Coordination with teammates, supporting others, and contributing to group success 12 Professional Attitude Respectful behavior, positive energy, and professional interactions with peers 13 Problem-Solving Ability Identifying root causes, proposing solutions, and using logical thinking 14 Adaptability Ability to handle changes, switch tasks, and stay productive under shifting priorities 15 Innovation \u0026 Creativity Bringing new ideas, improving workflow, or suggesting optimizations 16 Overall Contribution Impact on project outcomes, reliability, and overall performance during the internship Further Improvements\rWiden the relationships more with the peers and seniors Deeper delve into the AWS Services for further understandings Acquire more knowledge from peers",
    "description": "During my internship at AMAZON WEB SERVICES VIETNAM COMPANY LIMITED from 08/09/2025 to 09/12/2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in developing a Multi-region SaaS Task Management Web Application, through which I enhanced my skills in Backend Development, AWS Services, and Cloud Architecture. In the backend domain, I strengthened my ability to:\nDevelop RESTful APIs Work with Microservices Architecture Effectively utilize various AWS services I also gained practical experience in designing AWS architectures that are scalable, secure, and cost-optimized for real-world projects.",
    "tags": [],
    "title": "Self Assesment",
    "uri": "/en/6-self-assesment/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Worklog",
    "content": "Week 6 Objectives\rContinue frontend and backend development Explore Elasticache for Redis and Amazon RDS services Tasks to be carried out this week\rDay Task Start Date Completion Date Reference Material 1 - Team meeting to discuss about the development process 13/10/2025 13/10/2025 - Code backend Authentication Service, User Service - Code frontend: Build complex forms for Creating/Editing data. Apply form validation. 2 - Learn about Elasticache for Redis 14/10/2025 14/10/2025 Amazon AWS ElastiCache Deployment, AWS Elasticache Redis Creation and redis-cli Configuration step by step process - Code backend Authentication Service, User Service - Code frontend: Build complex forms for Creating/Editing data. Apply form validation. - Integrate message sending via Kafka into the Taskflow Service so it can notify the Notification Service to send alerts to users. 3 - Update worklog 15/10/2025 15/10/2025 - Code backend Authentication Service, User Service - Code frontend: Build complex forms for Creating/Editing data. Apply form validation. Develop Detail Page and integrate Get By ID API. Build advanced Search/Filter functionality. - Learn and integrate WebSocket into the Notification Service to deliver real-time notifications to users. 4 - Learn and practice Amazon RDS 16/10/2025 16/10/2025 Module 06-02 - Amazon RDS \u0026 Amazon Aurora, Module 06-Lab05-2.3 - Create RDS Security group, Module 06-Lab05-4 - Create RDS database instance - Code backend Authentication Service, User Service - Code frontend: Develop Detail Page and integrate Get By ID API. Build advanced Search/Filter functionality. 5 - Code backend Authentication Service, User Service 17/10/2025 17/10/2025 - Code frontend: Develop Detail Page and integrate Get By ID API. Build advanced Search/Filter functionality. - Develop the Upcoming Task UI for the project and integrate the API calls from the Taskflow Service. - Update worklog",
    "description": "Week 6 Objectives\rContinue frontend and backend development Explore Elasticache for Redis and Amazon RDS services Tasks to be carried out this week\rDay Task Start Date Completion Date Reference Material 1 - Team meeting to discuss about the development process 13/10/2025 13/10/2025 - Code backend Authentication Service, User Service - Code frontend: Build complex forms for Creating/Editing data. Apply form validation. 2 - Learn about Elasticache for Redis 14/10/2025 14/10/2025 Amazon AWS ElastiCache Deployment, AWS Elasticache Redis Creation and redis-cli Configuration step by step process - Code backend Authentication Service, User Service - Code frontend: Build complex forms for Creating/Editing data. Apply form validation. - Integrate message sending via Kafka into the Taskflow Service so it can notify the Notification Service to send alerts to users. 3 - Update worklog 15/10/2025 15/10/2025 - Code backend Authentication Service, User Service - Code frontend: Build complex forms for Creating/Editing data. Apply form validation. Develop Detail Page and integrate Get By ID API. Build advanced Search/Filter functionality. - Learn and integrate WebSocket into the Notification Service to deliver real-time notifications to users. 4 - Learn and practice Amazon RDS 16/10/2025 16/10/2025 Module 06-02 - Amazon RDS \u0026 Amazon Aurora, Module 06-Lab05-2.3 - Create RDS Security group, Module 06-Lab05-4 - Create RDS database instance - Code backend Authentication Service, User Service - Code frontend: Develop Detail Page and integrate Get By ID API. Build advanced Search/Filter functionality. 5 - Code backend Authentication Service, User Service 17/10/2025 17/10/2025 - Code frontend: Develop Detail Page and integrate Get By ID API. Build advanced Search/Filter functionality. - Develop the Upcoming Task UI for the project and integrate the API calls from the Taskflow Service. - Update worklog",
    "tags": [],
    "title": "Week 6 Worklog",
    "uri": "/en/1-worklog/1.6-week_6/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Translated Blogs",
    "content": "Những hiểu biết sâu sắc và bài học kinh nghiệm từ Amazon Q trong tích hợp trình thu thập thông tin web Connect\rTác giả: Vikas Prasad \u0026 Ayush Mehta Ngày: 22/07/2025\nGiới thiệu\rCác nhân viên tổng đài (Human agents) là yếu tố then chốt trong mọi trung tâm chăm sóc khách hàng, và việc cung cấp cho họ các công cụ phù hợp để thành công là điều thiết yếu đối với mọi tổ chức. Khi được hỗ trợ tốt, các nhân viên không chỉ có trải nghiệm làm việc tốt hơn, mà còn giúp nâng cao trải nghiệm của khách hàng cuối cùng.\nĐể đáp ứng nhu cầu này, Amazon Connect giới thiệu Amazon Q in Connect — một trợ lý AI sinh nội dung (Generative AI assistant), cung cấp các phản hồi và hành động gợi ý theo thời gian thực, được cá nhân hóa giúp nhân viên tổng đài thực hiện công việc hiệu quả và nhanh chóng hơn.\nAmazon Q in Connect có thể tích hợp với nhiều cơ sở tri thức (knowledge base) khác nhau thông qua các phương thức tích hợp đa dạng. Bài viết này tập trung vào phương pháp tích hợp bằng Web crawler, bao gồm những lưu ý quan trọng khi triển khai và các thực hành tốt nhất. Chúng ta sẽ cùng tìm hiểu cách khách hàng có thể truy cập và sử dụng hiệu quả các URL trang web của mình làm nguồn dữ liệu cho Amazon Q in Connect.\nVì sao cần tích hợp Web Crawler?\rNội dung thường bị phân tán trên nhiều website, buộc agent phải tra cứu nhiều nguồn khác nhau khi hỗ trợ khách hàng. Điều này làm chậm quá trình xử lý và ảnh hưởng đến chất lượng tương tác.\nKhi được cấu hình với Web Crawler, Amazon Q in Connect có thể: Tự động thu thập và lập chỉ mục nội dung từ nhiều website, trang hỗ trợ. Cung cấp cho agent thông tin sản phẩm nhanh chóng và chính xác.\nCung cấp liên kết nguồn gốc để agent và khách có thể đối chiếu.\nLoại bỏ việc cập nhật thủ công khi nội dung website thay đổi.\nBài viết này sẽ trình bày các giai đoạn và cách tối ưu triển khai Web Crawler trong Amazon Q in Connect. Biểu đồ 1: Các giai đoạn khác nhau để tối ưu hóa việc triển khai trình thu thập thông tin web\n1. Giai đoạn Planning (Lập kế hoạch)\rTrước khi bắt đầu triển khai tích hợp Web crawler cho Amazon Q in Connect, bạn nên tham khảo tài liệu hướng dẫn chính thức của Amazon Connect để biết chi tiết về cách kích hoạt Amazon Q in Connect cũng như các yêu cầu tiên quyết cần đáp ứng. Việc hiểu rõ nền tảng kỹ thuật và điều kiện cần thiết sẽ giúp quá trình triển khai diễn ra thuận lợi và tránh phát sinh lỗi khi cấu hình hoặc thu thập dữ liệu.\nTrước khi triển khai tích hợp Web crawler, hãy xem xét kỹ ba nhóm câu hỏi nền tảng sau:\nChiến lược quản lý tri thức\rTrước hết, tổ chức cần xác định rõ bộ phận chịu trách nhiệm chính trong việc quản lý và duy trì nội dung — có thể là phòng Marketing, bộ phận IT, hoặc đội ngũ Trải nghiệm khách hàng (Customer Experience Team). Việc chỉ định rõ trá ch nhiệm giúp đảm bảo tính nhất quán trong quản trị dữ liệu. Đồng thời, cần thiết lập một quy trình cập nhật nội dung minh bạch, trong đó các thay đổi phải được đồng bộ hóa với hệ thống tổng đài Amazon Connect để các nhân viên luôn truy cập được phiên bản mới nhất của tri thức nội bộ.\nPhân tích người dùng cuối\rBước tiếp theo là xác định đối tượng sẽ trực tiếp sử dụng cơ sở tri thức. Họ có thể là các agent tuyến 1 — những người chủ yếu xử lý các câu hỏi tổng quát, hoặc các nhân viên hỗ trợ kỹ thuật chuyên sâu phụ trách những vấn đề phức tạp hơn. Trong nhiều trường hợp, hệ thống có thể được sử dụng bởi cả hai nhóm người dùng. Việc phân tích chính xác đặc điểm của người dùng cuối giúp doanh nghiệp xác định phạm vi nội dung cần crawl, cũng như độ sâu chi tiết của thông tin cần được lập chỉ mục để phù hợp với nhu cầu thực tế của từng nhóm.\nKhả năng truy cập nội dung\rCuối cùng, tổ chức cần kiểm tra khả năng truy cập của các nguồn dữ liệu được chọn để tích hợp vào Amazon Q in Connect. Một số nguồn thông tin có thể công khai, trong khi những nguồn khác yêu cầu xác thực hoặc quyền truy cập đặc biệt. Do đó, việc đảm bảo có sự cho phép hợp pháp trước khi tiến hành crawl là cực kỳ quan trọng. Điều này không chỉ tuân thủ các chính sách bảo mật và quyền riêng tư, mà còn giúp quá trình tích hợp diễn ra ổn định, bền vững và không bị gián đoạn bởi các vấn đề liên quan đến quyền truy cập.\n2. Giai đoạn Staged Implementation (Triển khai theo giai đoạn)\rTrong quá trình triển khai tích hợp Web crawler cho Amazon Q in Connect, doanh nghiệp nên bắt đầu với cách tiếp cận có trọng tâm và từng bước thay vì thực hiện đồng loạt trên toàn bộ hệ thống. Cụ thể, hãy tạo các cơ sở tri thức (knowledge base) riêng biệt cho từng nhóm nội dung quan trọng hoặc lĩnh vực trọng điểm. Việc này giúp dễ dàng quản lý, thử nghiệm và kiểm soát chất lượng dữ liệu của từng phần trước khi hợp nhất.\nTiếp theo, cần kiểm thử độc lập nhiều cấu hình URL khác nhau nhằm xác định đâu là phương án crawl hiệu quả nhất cho từng loại nội dung. Chỉ sau khi đã tối ưu từng phần, bạn mới nên kết hợp các cấu hình này thành một giải pháp tổng thể. Phương pháp này giúp bạn phân tích và xử lý lỗi (troubleshoot) đối với các nguồn nội dung cụ thể mà không ảnh hưởng đến toàn bộ hệ thống. Đồng thời, nó cho phép bạn tinh chỉnh mô hình quét (crawl pattern) để đạt được hiệu suất thu thập dữ liệu tối ưu.\nKhi hệ thống bắt đầu hoạt động, các nhân viên tổng đài (agent) sẽ cung cấp phản hồi thực tế về chất lượng và mức độ phù hợp của các phản hồi do AI đề xuất. Dựa trên phản hồi này, bạn có thể mở rộng dần phạm vi crawl, bổ sung thêm các nhóm nội dung mới để hoàn thiện cơ sở tri thức.\nPhương pháp triển khai theo giai đoạn không chỉ giúp rút ngắn thời gian triển khai ban đầu, mà còn đảm bảo mỗi phần nội dung được tối ưu kỹ lưỡng trước khi tích hợp vào cơ sở tri thức chính (primary knowledge base). Nhờ đó, hệ thống Amazon Q in Connect có thể đạt hiệu suất cao, dữ liệu nhất quán và độ tin cậy tối đa trong quá trình hỗ trợ nhân viên và khách hàng.\n3. Giai đoạn Optimization (Tối ưu hóa)\rTrong giai đoạn này, mục tiêu chính là tinh chỉnh cấu hình và chiến lược tích hợp web crawler để đạt hiệu suất cao nhất, đồng thời đảm bảo tính ổn định của hệ thống và chất lượng dữ liệu được thu thập. Dưới đây là các yếu tố thực tiễn quan trọng cần xem xét khi tối ưu hóa chiến lược tích hợp web crawler cho Amazon Q in Connect.\nGiới hạn dịch vụ (Service Limit)\rKhi thiết kế hệ thống tích hợp web crawler cho Amazon Q, điều quan trọng là phải nhận thức rõ rằng mỗi crawler chỉ có thể xử lý tối đa 25.000 tệp (files) trong một phiên thu thập dữ liệu (ingestion). Giới hạn này đòi hỏi doanh nghiệp phải có chiến lược chọn lọc URL và ưu tiên nội dung thật hiệu quả.\nCụ thể, nên xây dựng cơ chế lọc nội dung (content filtering mechanism) với thứ tự ưu tiên rõ ràng — tập trung vào:\nCác tài liệu quan trọng đối với hoạt động kinh doanh,\nCác bài viết thường xuyên được truy cập,\nVà tài nguyên hỗ trợ khách hàng có giá trị cao.\nBên cạnh đó, có thể áp dụng các kỹ thuật như nhận dạng mẫu URL (URL pattern matching), chấm điểm mức độ liên quan nội dung (content relevancy scoring) và lọc theo siêu dữ liệu (metadata filtering) để tận dụng tối đa giới hạn tệp cho phép. Do hạn chế này, việc thiết kế kiến trúc nội dung hợp lý và phân tách cơ sở tri thức thành nhiều cấu hình crawler riêng biệt (multiple crawler configurations) là điều cần thiết trong một số trường hợp phức tạp.\nTốc độ crawl (Crawl Rate)\rKhi triển khai web crawler cho Amazon Q, doanh nghiệp cần phối hợp chặt chẽ với đội hạ tầng hoặc IT — những người trực tiếp quản lý website — để xác định ngưỡng tốc độ crawl (crawl rate threshold) phù hợp với khả năng tải và cấu hình cân bằng tải (load balancing) của hệ thống web. Các yếu tố như số lượng yêu cầu đồng thời (concurrent requests) và tần suất crawl cần được hiệu chỉnh cẩn thận, vì việc quét quá nhanh có thể gây ra lỗi 504 Gateway Timeout và dẫn đến dữ liệu thu thập không đầy đủ.\nDo đó, nên bắt đầu với tốc độ crawl thấp, sau đó tăng dần (progressive rate limit) trong khi theo dõi thời gian phản hồi của máy chủ web thông qua các công cụ giám sát lưu lượng truy cập (web traffic monitoring tools). Bằng cách này, bạn có thể tinh chỉnh tham số crawl để vừa đảm bảo chỉ mục hóa dữ liệu chính xác và ổn định, vừa duy trì độ ổn định cho website.\nĐịnh nghĩa phạm vi crawl (Crawl Scope)\rThay vì cố gắng crawl toàn bộ trang web, bạn nên xác định chính xác các điểm bắt đầu crawl (seed URLs) cho từng danh mục nội dung quan trọng. Cách làm này giúp hệ thống khám phá nội dung liên quan một cách hiệu quả trong khi giảm số lượng seed URL cần thiết. Ví dụ:\n{ \"url\": \"https://example.com/products/\" }, { \"url\": \"https://example.com/documentation/\" }\rTrong ví dụ trên, mỗi seed URL trỏ đến trang danh mục chính (category index page) thay vì từng trang sản phẩm riêng lẻ, cho phép web crawler tự động tìm thấy toàn bộ nội dung liên quan mà không cần khai báo thủ công hàng trăm URL.\nĐánh giá chiến lược cơ sở tri thức – đơn hay đa\rTùy thuộc vào mô hình kinh doanh và đặc thù hỗ trợ khách hàng, bạn có thể chọn giữa:\nMột cơ sở tri thức thống nhất (unified knowledge base) — phù hợp khi khách hàng thường xuyên đặt câu hỏi liên quan đến nhiều sản phẩm khác nhau.\nNhiều cơ sở tri thức tách biệt (separate knowledge bases) — phù hợp khi mỗi nhóm khách hàng chỉ quan tâm đến một dòng sản phẩm cụ thể.\nTuy nhiên, cần lưu ý đến giới hạn số lượng knowledge base mà dịch vụ Amazon Q cho phép. Trong một số trường hợp nâng cao, bạn có thể thay đổi động (dynamic switching) giữa các knowledge base ngay trong quá trình chạy (runtime) bằng cách:\nGhi đè cấu hình cơ sở tri thức bằng AI Agent phù hợp.\nSau đó sử dụng API Update session để chuyển đổi giữa các AI Agent tương ứng.\nCách tiếp cận linh hoạt này giúp Amazon Q in Connect có thể phục vụ nhiều loại khách hàng và tình huống hỗ trợ khác nhau mà không cần tái cấu hình toàn bộ hệ thống. Sơ đồ 2: Các thành phần của Q trong Connect Assistant\n4. Giai đoạn Implementation (Triển khai thực tế)\rGiới hạn thời gian (Execution Timeout)\rKhi triển khai web crawler của Amazon Q, các kiến trúc sư hệ thống cần tính đến giới hạn thời gian thực thi (timeout) cho mỗi phiên crawl. Giới hạn này yêu cầu phải áp dụng chiến lược crawl hiệu quả, bao gồm:\nƯu tiên URL (URL prioritization) để tập trung vào các trang quan trọng nhất.\nTối ưu hóa mô hình crawl (optimized crawl patterns) nhằm giảm tải và tăng tốc độ thu thập dữ liệu.\nVà phân tách nội dung thành nhiều phiên crawl nhỏ (segmenting content into multiple crawl jobs) để đảm bảo hoàn thành quá trình thu thập dữ liệu trong khoảng thời gian cho phép.\nGiới hạn URL và cấu hình (URL Configuration)\rGiao diện người dùng (UI) của Amazon Connect cho phép thêm tối đa 10 URL, trong khi API UrlConfiguration hỗ trợ lên tới 100 URL khởi tạo (seed URLs). Đối với các tổ chức có nhiều website hoặc lượng nội dung lớn, việc lập kế hoạch thông minh là rất cần thiết. Thay vì liệt kê từng trang một, hãy xây dựng sơ đồ cấu trúc nội dung (content structure mapping) để xác định các danh mục chính (key categories). Sau đó, tạo các biểu thức chính quy (regex patterns) để xác định:\nNội dung cần thu thập (inclusion patterns),\nVà nội dung cần loại trừ (exclusion patterns).\nVí dụ:\n._domain\\.com/support/._\\.pdf\rKết luận\rTóm lại, thực tiễn tốt nhất (best practice) cho việc tích hợp trình thu thập dữ liệu web (web crawler integration) trong Amazon Q in Connect tuân theo một quy trình hệ thống gồm bốn giai đoạn chính: Planning (Lập kế hoạch), Staged Implementation (Triển khai theo giai đoạn), Optimization (Tối ưu hóa) và Implementation (Triển khai thực tế).\nGiai đoạn Lập kế hoạch (Planning phase)\nTập trung vào việc xây dựng chiến lược cơ sở tri thức (knowledge base strategy), phân tích nhu cầu người dùng cuối, và đảm bảo khả năng truy cập nội dung. Việc chuẩn bị kỹ lưỡng ở giai đoạn này giúp định hướng chính xác cho các bước triển khai sau.\nGiai đoạn Triển khai theo giai đoạn (Staged Implementation phase) Ở bước này, tổ chức sẽ tạo và kiểm thử các cơ sở tri thức riêng biệt cho từng nhóm nội dung quan trọng, trước khi tích hợp chúng lại thành một hệ thống hoàn chỉnh. Cách tiếp cận theo từng phần giúp kiểm soát chất lượng dữ liệu, xử lý lỗi hiệu quả, và đảm bảo độ chính xác của thông tin mà các agent sử dụng.\nGiai đoạn Tối ưu hóa (Optimization phase) Giai đoạn này tập trung giải quyết các yếu tố kỹ thuật then chốt như: Giới hạn dịch vụ (service limits) — chẳng hạn giới hạn 25.000 tệp cho mỗi lần crawl.\nTốc độ thu thập dữ liệu (crawl rate) — để tránh gây tải cho máy chủ.\nVà chiến lược cơ sở tri thức (knowledge base strategy) — xác định xem nên hợp nhất hay tách riêng cơ sở tri thức theo từng dòng sản phẩm.\nGiai đoạn Triển khai thực tế (Implementation phase) Xử lý các vấn đề thực thi như: Giới hạn thời gian thu thập (execution timeout),\nVà giới hạn cấu hình URL (URL configuration limits). Tại đây, việc giám sát, tinh chỉnh và xác minh quá trình crawl là rất quan trọng để đảm bảo toàn bộ nội dung được thu thập chính xác và đầy đủ.\nQuy trình lặp lại (Iterative Process) Toàn bộ quy trình được thiết kế theo mô hình lặp (iterative), trong đó phản hồi giữa các giai đoạn được sử dụng để liên tục cải thiện và tối ưu hiệu suất tích hợp. Cách tiếp cận có hệ thống này giúp các tổ chức triển khai Amazon Q’s web crawler một cách hiệu quả, đồng thời duy trì sự ổn định của hệ thống và đảm bảo bao quát toàn diện nguồn tri thức cho hoạt động của trung tâm liên hệ (contact center).\nĐể đạt hiệu quả cao nhất khi sử dụng web crawler integration, cần lưu ý:\n1.Xác định nội dung ưu tiên cao (Identify high-priority content) và phân tích kỹ các URL.\n2.Đặt các điểm bắt đầu crawl cụ thể (Set specific starting points for crawls) để giới hạn phạm vi hợp lý.\n3.Sử dụng bộ lọc regex (Use regex filters) nhằm nhắm chính xác đến các URL có liên quan.\nTheo dõi hiệu suất của Amazon Q in Connect qua Amazon CloudWatch Logs, đảm bảo hệ thống hoạt động ổn định và hiệu quả. Bằng cách áp dụng các chiến lược này, các tổ chức có thể tích hợp thành công dữ liệu tri thức từ nhiều website vào Amazon Q in Connect, giúp nhân viên hỗ trợ (agents) có quyền truy cập toàn diện vào thông tin sản phẩm và tài liệu hướng dẫn, đồng thời duy trì hoạt động thu thập dữ liệu hiệu quả trong giới hạn dịch vụ.",
    "description": "Những hiểu biết sâu sắc và bài học kinh nghiệm từ Amazon Q trong tích hợp trình thu thập thông tin web Connect\rTác giả: Vikas Prasad \u0026 Ayush Mehta Ngày: 22/07/2025\nGiới thiệu\rCác nhân viên tổng đài (Human agents) là yếu tố then chốt trong mọi trung tâm chăm sóc khách hàng, và việc cung cấp cho họ các công cụ phù hợp để thành công là điều thiết yếu đối với mọi tổ chức. Khi được hỗ trợ tốt, các nhân viên không chỉ có trải nghiệm làm việc tốt hơn, mà còn giúp nâng cao trải nghiệm của khách hàng cuối cùng.",
    "tags": [],
    "title": "Blog 7",
    "uri": "/en/3-translated_blogs/blog_7/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Workshop \u003e Deploy Flow \u003e Frontend Deploy",
    "content": "PHASE 6: DEPLOY \u0026 TESTING\rStep 6.1: Build \u0026 Deploy\rOn your local computer (in the React project folder):\n# 1. Build to the production folder npm run build # 2. Upload to the MAIN bucket (Singapore) # Note: Just upload to Sing, AWS will automatically copy to US aws s3 sync build/ s3://sgutodolist-frontend-sg --delete # 3. Clear the CloudFront cache so users can see the new code immediately aws cloudfront create-invalidation --distribution-id \u003cID_CUA_BAN\u003e --paths \"/*\"\rStep 6.2: Test\rGo to https://sgutodolist.com.\nTry reloading the page (F5) at the sub-links (e.g. /tasks) to see if there is a 404 error.\nTest Replication: Go to S3 Console bucket Virginia to see if the file has appeared automatically (usually takes 15s - 1 minute).\n⬅ STEP 6: DNS Record",
    "description": "PHASE 6: DEPLOY \u0026 TESTING\rStep 6.1: Build \u0026 Deploy\rOn your local computer (in the React project folder):\n# 1. Build to the production folder npm run build # 2. Upload to the MAIN bucket (Singapore) # Note: Just upload to Sing, AWS will automatically copy to US aws s3 sync build/ s3://sgutodolist-frontend-sg --delete # 3. Clear the CloudFront cache so users can see the new code immediately aws cloudfront create-invalidation --distribution-id \u003cID_CUA_BAN\u003e --paths \"/*\"\rStep 6.2: Test\rGo to https://sgutodolist.com.",
    "tags": [],
    "title": "Deploy and Test",
    "uri": "/en/5-workshop/5.3-deploy_flow/5.3.1-frontend-deploy/5.3.1.7-deploy-and-test/index.html"
  },
  {
    "breadcrumb": "Internship Report",
    "content": "Overall Evaluation\r1. Working Environment\nThe working environment is incredibly friendly, open, and supportive. FCAJ members are always willing to help each other whenever an issue arises, and everyone is very open to discussing not only work-related topics but also studies, hobbies, and personal interests. The workspace itself is comfortable and well-organized, creating a productive atmosphere that makes it easier to stay focused and engaged.\n2. Support from Mentors / Support Team\nThe mentors are extremely approachable and always provide thorough, step-by-step guidance whenever any FCAJ member needs help. Their explanations are clear, detailed, and delivered with great patience. In addition to that, the support team ensures all announcements, documents, and task-related information are communicated promptly and completely throughout the internship period. This combination of technical guidance and administrative support made my experience smooth and highly valuable.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned during the internship were highly relevant to my academic background and played an important role in strengthening my technical foundation. They allowed me to apply what I had learned in school while gaining additional knowledge that will be extremely beneficial for the career path I am pursuing. The work was both meaningful and aligned with the direction I want to grow professionally.\n4. Learning \u0026 Skill Development Opportunities\nThroughout the internship, I had the opportunity to work hands-on with essential AWS core services such as EC2, S3, and ECS. I also benefited greatly from comprehensive learning resources like AWS Skill Builder and the FCAJ learning playlists. Moreover, senior members from the support and admin teams shared valuable insights, career advice, and personal experiences that helped me better understand what is needed to grow in the technology and cloud computing fields.\n5. Company Culture \u0026 Team Spirit\nThe company culture is very open and positive, where everyone communicates freely and interacts with one another in a supportive and uplifting way. Team members frequently engage in casual conversations, share stories, and create a comfortable atmosphere for collaboration. There were even times when mentors bought fruit for the interns so everyone could sit together, eat, and talk—small gestures that made the environment feel friendly and inclusive.\n6. Internship Policies / Benefits\nThe greatest benefit of the internship program is the strong foundational knowledge it provides—particularly in understanding how cloud environments operate in real-world settings. This experience offered me practical insights into the rapidly growing cloud industry, as well as the essential skills needed to build a solid professional path in technology. The supportive environment, exposure to real projects, and structured learning resources made this internship truly valuable.\nA special thank you to Master Hung and the seniors who founded the FCAJ program, giving students like us the opportunity and environment to get familiar with and practice using Cloud tools and services, as well as to connect with many peers who share the same passion.\nI would also like to express my gratitude to the Support Team for their dedicated assistance throughout our internship.\nAnd thank you to Mr. Kevin Tran, the Captain of AWS CLOUD CLUB SGU, for inspiring and motivating us throughout this journey.\nThank you to everyone.",
    "description": "Overall Evaluation\r1. Working Environment\nThe working environment is incredibly friendly, open, and supportive. FCAJ members are always willing to help each other whenever an issue arises, and everyone is very open to discussing not only work-related topics but also studies, hobbies, and personal interests. The workspace itself is comfortable and well-organized, creating a productive atmosphere that makes it easier to stay focused and engaged.",
    "tags": [],
    "title": "Sharing and Feedback",
    "uri": "/en/7-sharing_and_feedback/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Worklog",
    "content": "Week 7 Objectives\rCode Backend User Service and Notification Service Code Frontend Explore the Application Load Balancer service Learn how to deploy the system on cloud resources Test deploying the Frontend on S3 Tasks to be carried out this week\rDay Task Start Date Completion Date Reference Material 1 - Team meeting to discuss about the development process 20/10/2025 20/10/2025 - Code backend User Service, Notification Service - Code frontend: Develop Detail Page and integrate Get By ID API. Build advanced Search/Filter functionality. 2 - Learn about Application Load Balancer 21/10/2025 21/10/2025 AWS ALB (Application Load Balancer) - Step By Step Tutorial, Create an Application Load Balancer (ALB) in AWS - AWS Elastic Load Balancing Tutorial for Beginners - Code backend User Service, Notification Service - Code frontend: Develop Detail Page and integrate Get By ID API. Build advanced Search/Filter functionality. 3 - Learn how to deploy the system on cloud resources 22/10/2025 22/10/2025 - Code backend User Service, Notification Service - Code frontend: Develop Detail Page and integrate Get By ID API. Build advanced Search/Filter functionality. 4 - Code backend User Service, Notification Service 23/10/2025 23/10/2025 - Code frontend: Develop Detail Page and integrate Get By ID API. Build advanced Search/Filter functionality. Build modals, popups, drag-and-drop features. Complete 80% of the main business flow. 5 - Code backend User Service, Notification Service 24/10/2025 24/10/2025 - Code frontend: Build modals, popups, drag-and-drop features. Complete 80% of the main business flow. - Test deploying the Frontend on S3",
    "description": "Week 7 Objectives\rCode Backend User Service and Notification Service Code Frontend Explore the Application Load Balancer service Learn how to deploy the system on cloud resources Test deploying the Frontend on S3 Tasks to be carried out this week\rDay Task Start Date Completion Date Reference Material 1 - Team meeting to discuss about the development process 20/10/2025 20/10/2025 - Code backend User Service, Notification Service - Code frontend: Develop Detail Page and integrate Get By ID API. Build advanced Search/Filter functionality. 2 - Learn about Application Load Balancer 21/10/2025 21/10/2025 AWS ALB (Application Load Balancer) - Step By Step Tutorial, Create an Application Load Balancer (ALB) in AWS - AWS Elastic Load Balancing Tutorial for Beginners - Code backend User Service, Notification Service - Code frontend: Develop Detail Page and integrate Get By ID API. Build advanced Search/Filter functionality. 3 - Learn how to deploy the system on cloud resources 22/10/2025 22/10/2025 - Code backend User Service, Notification Service - Code frontend: Develop Detail Page and integrate Get By ID API. Build advanced Search/Filter functionality. 4 - Code backend User Service, Notification Service 23/10/2025 23/10/2025 - Code frontend: Develop Detail Page and integrate Get By ID API. Build advanced Search/Filter functionality. Build modals, popups, drag-and-drop features. Complete 80% of the main business flow. 5 - Code backend User Service, Notification Service 24/10/2025 24/10/2025 - Code frontend: Build modals, popups, drag-and-drop features. Complete 80% of the main business flow. - Test deploying the Frontend on S3",
    "tags": [],
    "title": "Week 7 Worklog",
    "uri": "/en/1-worklog/1.7-week_7/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Translated Blogs",
    "content": "Xây dựng hệ thống đa tenant resilient với hàng đợi công bằng Amazon SQS\rTác giả: Maximilian Schellhorn \u0026 Dirk Fröhner\nNgày: 21/07/2025\nChuyên mục: Amazon Simple Queue Service (SQS), Announcements, Intermediate (200), Serverless, Technical How-to\nGiới thiệu\rHôm nay, AWS giới thiệu Amazon Simple Queue Service (Amazon SQS) fair queues — một tính năng mới giúp giảm thiểu ảnh hưởng “noisy neighbor” trong các hệ thống đa tenant. Với fair queues, ứng dụng của bạn trở nên resilient hơn và dễ vận hành hơn, giảm chi phí vận hành trong khi cải thiện chất lượng dịch vụ cho khách hàng.\nTrong kiến trúc phân tán, hàng đợi thông điệp (message queue) đã trở thành xương sống của thiết kế hệ thống resilient. Chúng hoạt động như bộ đệm giữa các thành phần, cho phép dịch vụ xử lý công việc bất đồng bộ và theo tốc độ riêng của chúng. Khi ứng dụng của bạn bất ngờ gặp lượng truy cập tăng đột biến, hàng đợi sẽ ngăn việc lỗi lan truyền bằng cách buffer công việc và đảm bảo các service phía sau không bị quá tải.\nAmazon SQS từ lâu đã là lựa chọn hàng đầu cho các nhà phát triển xây dựng ứng dụng có khả năng mở rộng bởi vì đây là dịch vụ serverless được quản lý hoàn toàn, có thể mở rộng liền mạch để tiếp nhận hàng triệu message mỗi giây. Trong bài viết này, bạn sẽ học cách sử dụng Amazon SQS fair queues và hiểu cơ chế hoạt động của chúng thông qua một ví dụ thực tế.\nTổng quan\rNhiều ứng dụng hiện đại sử dụng kiến trúc multi-tenant, nơi một phiên bản ứng dụng duy nhất phục vụ nhiều tenant. Tenant là bất kỳ thực thể nào chia sẻ tài nguyên với thực thể khác. Đó có thể là khách hàng, ứng dụng khách, hoặc loại yêu cầu. Cách tiếp cận này giúp giảm chi phí vận hành và đơn giản hóa việc bảo trì thông qua việc sử dụng tài nguyên hiệu quả. Một ví dụ về tài nguyên được chia sẻ như vậy là các hàng đợi và năng lực xử lý (consumer capacity) tương ứng của chúng.\nTuy nhiên, hệ thống multi-tenant gặp thách thức khi một tenant trở thành “noisy neighbor” — nghĩa là tenant này làm ảnh hưởng đến những tenant khác bằng cách sử dụng quá mức tài nguyên của hệ thống. Với hàng đợi, tenant này có thể tạo backlog bằng việc gửi lượng lớn message hoặc yêu cầu thời gian xử lý lâu hơn. Hàng đợi thông thường sẽ xử lý các message cũ hơn trước, điều này làm tăng thời gian lưu message (dwell time) cho tất cả tenant trong tình huống như vậy. Điều này khiến việc duy trì chất lượng dịch vụ trở nên khó khăn và buộc đội ngũ phải mở rộng tài nguyên quá mức hoặc xây dựng các giải pháp tùy chỉnh phức tạp.\nAmazon SQS fair queues giúp duy trì thời gian lưu message thấp cho các tenant khác khi có một “noisy neighbor”. Việc này diễn ra một cách minh bạch mà không yêu cầu thay đổi logic xử lý message hiện tại của bạn. Bạn chỉ cần định nghĩa tenant trong hệ thống của mình là gì, và Amazon SQS sẽ xử lý điều phối phức tạp để giảm thiểu ảnh hưởng của noisy neighbor.\nCách hoạt động\rAmazon SQS liên tục theo dõi sự phân bố các message đã được nhận nhưng chưa bị xóa (in-flight) bởi các consumer trên tất cả tenant. Khi hệ thống phát hiện sự mất cân bằng:\nNó xác định tenant gây ồn — tenant khiến hàng đợi bị backlog.\nNó tự động điều chỉnh thứ tự phân phối message để ưu tiên message của các tenant “yên lặng” (không gây ồn).\nNó duy trì tổng throughput của hàng đợi.\nHãy xem ví dụ sau với một hàng đợi multi-tenant và bốn tenant khác nhau (A, B, C và D). Trong trạng thái ổn định (steady state), hàng đợi không có backlog, và message in-flight được phân phối đều giữa các tenant. Tất cả message được xử lý ngay khi xuất hiện. Thời gian lưu message (dwell time) thấp cho tất cả tenant. Lưu ý rằng năng lực xử lý của consumer không phải lúc nào cũng được dùng hết trong trạng thái này.\nHình 1: Một hàng đợi multi-tenant ở trạng thái steady state\nBây giờ xét kịch bản có tenant gây ồn: số lượng message của tenant A tăng mạnh và tạo backlog trong hàng đợi. Consumer bận xử lý chủ yếu message từ tenant A, còn các message từ các tenant khác phải chờ trong hàng đợi, dẫn tới tăng dwell time cho tất cả.\nHình 2: Hàng đợi multi-tenant với một noisy tenant\nKhi một tenant bắt đầu chiếm phần lớn tài nguyên consumer, Amazon SQS fair queues xem tenant đó là noisy neighbor và ưu tiên trả message của các tenant còn lại. Cách ưu tiên này giúp duy trì dwell time thấp cho các tenant yên lặng (B, C, D), trong khi dwell time của tenant A sẽ cao hơn cho tới khi backlog của nó được xử lý — nhưng không ảnh hưởng đến các tenant khác.\nHình 3: Hàng đợi multi-tenant với fair queues\nAmazon SQS không giới hạn tốc độ tiêu thụ theo tenant. Consumer vẫn có thể nhận message từ tenant gây ồn khi còn tài nguyên xử lý và hàng đợi không còn message nào khác để trả về. Giống như SQS Standard queue, fair queues cho phép throughput gần như không giới hạn, và không có giới hạn số lượng tenant trong hàng đợi.\nCách sử dụng\r1. Kích hoạt fair queues với MessageGroupId\rDưới đây là phần giới thiệu nhanh về cách bắt đầu sử dụng Amazon SQS fair queues trong ứng dụng của bạn. Xem tài liệu tính năng để có hướng dẫn chi tiết. Đây là các bước chính:\nKích hoạt Amazon SQS fair queues bằng cách thêm định danh tenant (MessageGroupId) vào message\nCấu hình Amazon CloudWatch metrics để theo dõi hoạt động của SQS fair queues\nBạn có thể dùng ứng dụng mẫu để quan sát hành vi của fair queues với khối lượng message thay đổi\nKích hoạt Amazon SQS fair queues bằng cách thêm MessageGroupId (tenant identifier) Producer có thể thêm định danh tenant bằng cách đặt MessageGroupId cho message gửi đi: Tính năng fairness mới sẽ được áp dụng tự động cho tất cả SQS standard queues đối với các message có thuộc tính MessageGroupId. Không cần thay đổi code phía consumer. Không ảnh hưởng đến độ trễ API và không có giới hạn throughput.\nCấu hình Amazon CloudWatch Metrics để theo dõi SQS Fair Queues\rBạn có thể giám sát SQS fair queues bằng CloudWatch metrics. Các thuật ngữ quan trọng:\nNoisy groups – nhóm message thuộc tenant gây ồn\nQuiet groups – các nhóm message còn lại (tenant không gây ồn)\nSQS giờ cung cấp một số metric mới:\nApproximateNumberOfNoisyGroups\nApproximateNumberOfMessagesVisibleInQuietGroups\nApproximateNumberOfMessagesNotVisibleInQuietGroups\nApproximateNumberOfMessagesDelayedInQuietGroups\nApproximateAgeOfOldestMessageInQuietGroups\nMetric mới quan trọng nhất là: ApproximateNumberOfNoisyGroups — số group (tenant) được coi là noisy. Dùng nó để phát hiện tenant tiêu thụ tài nguyên quá mức và thiết lập cảnh báo. Fair queues cung cấp thêm các metric với hậu tố InQuietGroups, cho phép theo dõi riêng các tenant không gây ồn — thay vì toàn hàng đợi.\nTheo dõi hiệu ứng công bằng\rBạn có thể so sánh metric InQuietGroups với metric queue thông thường:\nKhi một tenant tạo traffic đột biến, metric toàn queue tăng backlog Nhưng metric quiet groups vẫn thấp → chứng tỏ tenant khác không bị ảnh hưởng Hình 4: Biểu đồ backlog tenant noisy vs quiet\nXác định tenant gây tải cao\rDùng Amazon CloudWatch Contributor Insights để:\nXem top-N tenant Tổng số tenant Mức độ sử dụng của từng tenant Điều này hữu ích khi có hàng nghìn tenant, giúp tránh chi phí metric lớn (high-cardinality).\nHình 5: Dashboard Contributor Insights dựa trên MessageGroupId\nContributor Insights tạo metric từ log ứng dụng của bạn. Ứng dụng cần log số lượng message xử lý và MessageGroupId. Ví dụ đầy đủ có trong sample app phần tiếp theo.\nỨng dụng ví dụ\rĐể giúp bạn bắt đầu dễ dàng hơn, chúng tôi đã chuẩn bị một ứng dụng mẫu để bạn có thể quan sát hành vi của Amazon SQS fair queues với các mức tải message khác nhau. Bạn có thể tìm mã nguồn, hạ tầng dưới dạng code (IaC) và hướng dẫn chạy mẫu trong kho GitHub sqs-fair-queues.\nỨng dụng ví dụ này bao gồm một trình tạo tải (load generator) để mô phỏng traffic multi-tenant và cung cấp một CloudWatch dashboard hiển thị các metric quan trọng nhất để trực quan hóa cách fair queues hoạt động. Hình dưới đây minh hoạ dashboard đó:\nHình 6: CloudWatch FairQueuesDashboard\nMã nguồn và hướng dẫn chạy trong GitHub: sqs-fair-queues Kết luận\rAmazon SQS fair queues tự động giảm thiểu ảnh hưởng của noisy neighbor trong các hàng đợi multi-tenant. Ngay cả khi một tenant tạo ra lượng message lớn hoặc cần thời gian xử lý lâu hơn (tức trở thành noisy neighbor), tính năng này vẫn duy trì thời gian lưu message ổn định cho các tenant khác. Khi bạn thêm định danh tenant vào message, Amazon SQS fair queues sẽ tự động phát hiện và giảm thiểu tác động của tenant gây ồn, đảm bảo quyền truy cập công bằng vào hàng đợi cho các tenant còn lại.\nChúng tôi khuyến nghị bạn xem Amazon SQS Developer Guide để bắt đầu và thử nghiệm với ứng dụng mẫu nhằm quan sát hành vi của hệ thống với các mức tải khác nhau.\nTham khảo\rAmazon SQS Developer Guide AWS Blog – Fair Queues",
    "description": "Xây dựng hệ thống đa tenant resilient với hàng đợi công bằng Amazon SQS\rTác giả: Maximilian Schellhorn \u0026 Dirk Fröhner\nNgày: 21/07/2025\nChuyên mục: Amazon Simple Queue Service (SQS), Announcements, Intermediate (200), Serverless, Technical How-to\nGiới thiệu\rHôm nay, AWS giới thiệu Amazon Simple Queue Service (Amazon SQS) fair queues — một tính năng mới giúp giảm thiểu ảnh hưởng “noisy neighbor” trong các hệ thống đa tenant. Với fair queues, ứng dụng của bạn trở nên resilient hơn và dễ vận hành hơn, giảm chi phí vận hành trong khi cải thiện chất lượng dịch vụ cho khách hàng.",
    "tags": [],
    "title": "Blog 8",
    "uri": "/en/3-translated_blogs/blog_8/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Worklog",
    "content": "Week 8 Objectives\rCode Backend User Service and Notification Service Code Frontend Explore the Application Load Balancer service Learn how to deploy the system on cloud resources Test deploying the Frontend on S3 Tasks to be carried out this week\rDay Task Start Date Completion Date Reference Material 1 - Team meeting to discuss about the development process 27/10/2025 27/10/2025 - Build the Notification interface for the project, integrate the APIs, and establish a WebSocket connection to the backend. - Check the services code 2 - Code front end: Write Unit Tests for key components and functions. Perform Integration Tests for main business flows. 28/10/2025 28/10/2025 - Code backend User Service, Notification Service - Learn and practice ALB 3 - Code front end: Write Unit Tests for key components and functions. Perform Integration Tests for main business flows. 29/10/2025 29/10/2025 - Code backend User Service, Notification Service - Experiment deploy static resource on S3 4 - Code front end: Optimize loading performance (Code Splitting, Lazy Loading). 30/10/2025 30/10/2025 - Code backend User Service, Notification Service 5 - Code front end: Improve Accessibility (A11Y) and error handling (Error Boundaries). 31/10/2025 31/10/2025",
    "description": "Week 8 Objectives\rCode Backend User Service and Notification Service Code Frontend Explore the Application Load Balancer service Learn how to deploy the system on cloud resources Test deploying the Frontend on S3 Tasks to be carried out this week\rDay Task Start Date Completion Date Reference Material 1 - Team meeting to discuss about the development process 27/10/2025 27/10/2025 - Build the Notification interface for the project, integrate the APIs, and establish a WebSocket connection to the backend. - Check the services code 2 - Code front end: Write Unit Tests for key components and functions. Perform Integration Tests for main business flows. 28/10/2025 28/10/2025 - Code backend User Service, Notification Service - Learn and practice ALB 3 - Code front end: Write Unit Tests for key components and functions. Perform Integration Tests for main business flows. 29/10/2025 29/10/2025 - Code backend User Service, Notification Service - Experiment deploy static resource on S3 4 - Code front end: Optimize loading performance (Code Splitting, Lazy Loading). 30/10/2025 30/10/2025 - Code backend User Service, Notification Service 5 - Code front end: Improve Accessibility (A11Y) and error handling (Error Boundaries). 31/10/2025 31/10/2025",
    "tags": [],
    "title": "Week 8 Worklog",
    "uri": "/en/1-worklog/1.8-week_8/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Translated Blogs",
    "content": "Amazon Braket ra mắt bộ xử lý lượng tử siêu dẫn 54-qubit IQM Emerald\rTác giả: Zia Mohammad, Charunethran Panchalam Govindarajan, Peter Komar, Stefan Seegerer\nNgày: 21/07/2025\nChuyên mục: Amazon Braket – Quantum Technologies\nGiới thiệu\rAmazon Braket cho phép khách hàng thiết kế và chạy thuật toán lượng tử trên nhiều loại phần cứng lượng tử thông qua một giao diện thống nhất. Hôm nay, chúng tôi mở rộng phần cứng có sẵn trên Braket bằng việc chính thức cung cấp bộ xử lý lượng tử (QPU) mới nhất của IQM. Thiết bị, có tên Emerald, là một QPU siêu dẫn 54 qubit, đem đến cho khách hàng các cổng (gates) có độ trung thực cao hơn và kết nối lưới vuông (full square lattice connectivity).\nVới việc bổ sung IQM Emerald vào Braket, khách hàng hiện có thể truy cập hai bộ xử lý IQM: Garnet 20-qubit và Emerald 54-qubit mới. Cả hai thiết bị đều có sẵn qua Vùng Châu Âu (Stockholm), giúp khách hàng có nhiều lựa chọn phần cứng lượng tử phù hợp với nhu cầu nghiên cứu và yêu cầu thuật toán.\nHình 1: Bộ xử lý IQM Emerald\nTrong giai đoạn đầu của tính toán lượng tử, thử nghiệm trên nhiều thiết bị sẵn có là rất quan trọng để phát triển thuật toán lượng tử, nhằm mục tiêu xử lý các vấn đề phức tạp trong những lĩnh vực như tài chính, năng lượng, dược phẩm, và logistics. Khách hàng trên toàn cầu giờ có thể chạy thử nghiệm trên phần cứng lượng tử tiên tiến nhất của IQM, sử dụng truy cập theo yêu cầu (on-demand) để thiết kế và thực thi chương trình, và truy cập ưu tiên qua Hybrid Jobs để chạy các thuật toán lượng tử biến phân (variational quantum algorithms), tất cả với giá cả thanh toán theo mức sử dụng (pay-as-you-go). Với những workload yêu cầu độ trễ thấp hoặc có tính thời gian nhạy cảm, khách hàng có thể đặt trước (reserve) năng lực dành riêng trên QPU Emerald theo giờ thông qua Braket Direct.\nKiến trúc nâng cao cho nghiên cứu lượng tử\rEmerald sử dụng kiến trúc Crystal 54 của IQM, được xây dựng bằng các qubit transmon siêu dẫn sắp xếp theo lưới vuông (square lattice) và kết nối với nhau thông qua các coupler có thể điều chỉnh (tunable couplers). Cấu hình có tính kết nối cao này cho phép ánh xạ (mapping) các thuật toán lượng tử hiệu quả lên topology của thiết bị. Thiết kế lưới vuông hỗ trợ trực tiếp mã sửa lỗi bề mặt (surface-code error correction), đặt thiết bị vào vị trí phù hợp cho các ứng dụng tính toán lượng tử chịu lỗi trong tương lai.\nThiết bị hỗ trợ các phép quay X và Y bất kỳ (arbitrary X and Y rotations) như các cổng một qubit nguyên thủy (native single-qubit gates) và sử dụng cổng CZ như cổng hai-qubit nguyên thủy (native two-qubit operation). Tập cổng này cung cấp sự linh hoạt để triển khai thuật toán lượng tử trong khi vẫn duy trì hoạt động với độ trung thực cao. Dữ liệu đặc tính ban đầu cho thấy độ trung thực trung vị (median) của cổng một-qubit là 99.93% và độ trung thực trung vị của cổng hai-qubit là 99.5%. Các chỉ số hiệu năng cập nhật có thể được tìm thấy trên trang chi tiết thiết bị Emerald trong Braket Console.\nHình 2: Topology của QPU Emerald hiển thị lưới vuông 54-qubit với các coupler có thể điều chỉnh\nTính khả dụng và truy cập được cải thiện\rCả IQM Emerald và Garnet đều có mặt 19 giờ mỗi ngày, cho phép khách hàng chạy workload lượng tử theo thời gian thuận tiện bất kể múi giờ. Việc mở rộng cửa sổ khả dụng từ IQM cho phép vòng lặp phát triển và thử nghiệm liên tục cho khách hàng toàn cầu.\nKhách hàng có thể truy cập các QPU đặt tại EU này trong Vùng Châu Âu (Stockholm), nghĩa là họ có thể đáp ứng các yêu cầu về cư trú dữ liệu ở châu Âu và làm việc với các khả năng hybrid quantum-classical tiên tiến trong cùng một môi trường.\nBắt đầu với Emerald trên Braket\rBraket cung cấp cho khách hàng một giao diện lập trình thống nhất, dễ sử dụng để truy cập phần cứng lượng tử. Khách hàng có thể xây dựng, kiểm thử và chạy chương trình lượng tử trên Emerald với Braket SDK, hoặc các framework phổ biến khác như Qiskit, Pennylane, và NVIDIA CUDA-Q. Để thực thi chương trình trên Emerald, chỉ cần chỉ định ARN của thiết bị trong định nghĩa device trước khi chạy mạch: device = AwsDevice(“arn:aws:braket:eu-north-1::device/qpu/iqm/Emerald”)\nVí dụ Bell State (Python) Bạn có thể chạy một mạch chỉ với vài dòng code — ví dụ tạo trạng thái Bell: from braket.aws import AwsDevice from braket.circuits import Circuit\ndevice = AwsDevice(“arn:aws:braket:eu-north-1::device/qpu/iqm/Emerald”)\n# run circuit bell = Circuit().h(0).cnot(control=0, target=1) result = device.run(bell, shots=1000).result()\r(Ghi chú: đoạn code trên khởi tạo một mạch, áp cổng Hadamard (h) lên qubit 0 rồi CNOT giữa qubit 0 và 1, chạy 1000 lần (shots) và thu kết quả.)\nKhám phá vướng mắc lượng tử quy mô lớn hơn trên Emerald\rDung lượng 54-qubit của Emerald cho phép khám phá các thuật toán lượng tử cần nhiều qubit. Xây trên các thí nghiệm trạng thái GHZ đã demo trên Garnet, số qubit mở rộng và độ kết nối cao của Emerald cho phép tạo ra các trạng thái vướng mắc (entangled states) lớn hơn và các mạch lượng tử phức tạp hơn.\nDưới đây là ví dụ chuẩn bị trạng thái GHZ 49-qubit sử dụng kết nối lưới vuông của Emerald:\n(Ghi chú: ví dụ trên đặt một đường đi (path) qua topology của Emerald, áp cổng Hadamard lên qubit đầu tiên rồi nối các cặp qubit liền kề bằng CNOT để tạo GHZ, chạy 5000 shots. disable_qubit_rewiring=True ngăn Braket tự map lại qubit — tức đảm bảo mạch tuân theo topology đã chỉ định.)\nLập trình lượng tử nâng cao với dynamic circuits\rEmerald hỗ trợ khả năng dynamic circuits (tính năng thử nghiệm của Amazon Braket), cho phép chương trình lượng tử đo qubit giữa chừng (mid-circuit measurement) và áp dụng phép toán lượng tử có điều kiện dựa trên kết quả đo. Khả năng này mở ra các khả năng mới cho giao thức sửa lỗi lượng tử, thuật toán thích ứng (adaptive algorithms), và tận dụng qubit hiệu quả thông qua active reset. Ví dụ sau minh họa tái sử dụng qubit bằng active reset sử dụng dynamic circuits trên Emerald:\n(Ghi chú chi tiết các lệnh trên)\nEnableExperimentalCapability() bật các khả năng thử nghiệm như dynamic circuits.\nprx(1, pi, 0) lật qubit 1 về trạng thái |1⟩ (ở đây prx là cổng xoay/đảo tương ứng).\nmeasure_ff(1, feedback_key=0) đo qubit 1 giữa mạch và lưu giá trị đo dưới feedback_key=0.\ncc_prx(1, pi, 0, feedback_key=0) thực hiện thao tác có điều kiện dựa trên giá trị feedback (ví dụ: nếu đo ra 1 thì làm flip lại để active reset).\nadd_verbatim_box đóng gói đoạn mạch động để truyền vào device.\nKết quả mong đợi là đếm measurement chủ yếu ra “0” do active reset hiệu quả. Bắt đầu ngay hôm nay\rTruy cập Amazon Braket Management Console để xem cấu trúc kết nối (device topology), thời gian khả dụng của thiết bị, và cập nhật mới nhất về độ chính xác (fidelity) của cổng một qubit, hai qubit và độ chính xác phép đo (readout fidelity).\nCác nhà nghiên cứu thuộc các tổ chức được công nhận có thể đăng ký nhận tín dụng AWS để hỗ trợ cho các thí nghiệm trên Amazon Braket thông qua chương trình AWS Cloud Credits for Research.\nĐể bắt đầu chạy các chương trình lượng tử của bạn trên bộ xử lý Emerald, hãy tham khảo repository GitHub của chúng tôi để xem các notebook ví dụ và hướng dẫn. Bạn có thể chạy các chương trình này bằng môi trường Jupyter notebook được quản lý (managed) của AWS hoặc từ môi trường phát triển cục bộ trên máy bạn.",
    "description": "Amazon Braket ra mắt bộ xử lý lượng tử siêu dẫn 54-qubit IQM Emerald\rTác giả: Zia Mohammad, Charunethran Panchalam Govindarajan, Peter Komar, Stefan Seegerer\nNgày: 21/07/2025\nChuyên mục: Amazon Braket – Quantum Technologies\nGiới thiệu\rAmazon Braket cho phép khách hàng thiết kế và chạy thuật toán lượng tử trên nhiều loại phần cứng lượng tử thông qua một giao diện thống nhất. Hôm nay, chúng tôi mở rộng phần cứng có sẵn trên Braket bằng việc chính thức cung cấp bộ xử lý lượng tử (QPU) mới nhất của IQM. Thiết bị, có tên Emerald, là một QPU siêu dẫn 54 qubit, đem đến cho khách hàng các cổng (gates) có độ trung thực cao hơn và kết nối lưới vuông (full square lattice connectivity).",
    "tags": [],
    "title": "Blog 9",
    "uri": "/en/3-translated_blogs/blog_9/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Worklog",
    "content": "Week 9 Objectives\rReview and check backend service code Develop and integrate frontend features for Notification \u0026 Auth Adjust and refine system architecture through team discussions Implement and update WebSocket for real-time notifications Integrate Google OAuth2 login for Authentication Service Fix bugs related to WebSocket reconnection and authentication state Tasks to be carried out this week\rDay Task Start Date Completion Date Reference Material 1 - Team meeting to discuss about the development process 03/11/2025 03/11/2025 - Build the Notification interface for the project, integrate the APIs, and establish a WebSocket connection to the backend. - Check the services code 2 - Update WebSocket for Notification Service 04/11/2025 04/11/2025 - Refactor NotificationService flow to support real-time push notifications - Code front end: Review services UI for flaws 3 - Implement OAuth2 Login Google for Auth Service 05/11/2025 05/11/2025 - Test login flow and integrate Google OAuth callback - Update worklog 4 - Continue integrating Notification \u0026 Auth features on frontend 06/11/2025 06/11/2025 - Fix bugs related to WebSocket reconnection \u0026 auth state 5 - Testing for the week’s features 07/11/2025 07/11/2025 - Prepare weekly report \u0026 deployment checklist",
    "description": "Week 9 Objectives\rReview and check backend service code Develop and integrate frontend features for Notification \u0026 Auth Adjust and refine system architecture through team discussions Implement and update WebSocket for real-time notifications Integrate Google OAuth2 login for Authentication Service Fix bugs related to WebSocket reconnection and authentication state Tasks to be carried out this week\rDay Task Start Date Completion Date Reference Material 1 - Team meeting to discuss about the development process 03/11/2025 03/11/2025 - Build the Notification interface for the project, integrate the APIs, and establish a WebSocket connection to the backend. - Check the services code 2 - Update WebSocket for Notification Service 04/11/2025 04/11/2025 - Refactor NotificationService flow to support real-time push notifications - Code front end: Review services UI for flaws 3 - Implement OAuth2 Login Google for Auth Service 05/11/2025 05/11/2025 - Test login flow and integrate Google OAuth callback - Update worklog 4 - Continue integrating Notification \u0026 Auth features on frontend 06/11/2025 06/11/2025 - Fix bugs related to WebSocket reconnection \u0026 auth state 5 - Testing for the week’s features 07/11/2025 07/11/2025 - Prepare weekly report \u0026 deployment checklist",
    "tags": [],
    "title": "Week 9 Worklog",
    "uri": "/en/1-worklog/1.9-week_9/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Translated Blogs",
    "content": "Empower đã mở rộng quy mô đảm bảo chất lượng trung tâm liên hệ như thế nào với Amazon Connect và Amazon Bedrock\rTác giả: Marcos Ortiz, Illan Geller (Accenture), Ozlem Celik-Tinmaz (Accenture), Prabhu Akula (Accenture), và Ryan Baham (Empower)\nNgày: 04 tháng 8 năm 2025\nThể loại: Amazon Bedrock, Amazon Connect, Amazon Transcribe, Trung tâm liên hệ, AI Tạo sinh Giới thiệu\rEmpower là một công ty dịch vụ tài chính hàng đầu phục vụ hơn 18 triệu người Mỹ với 1,8 nghìn tỷ đô la tài sản đang quản lý. Họ tiếp nhận khoảng 10 triệu cuộc gọi của khách hàng hàng năm thông qua các trung tâm chăm sóc của mình. Để duy trì dịch vụ xuất sắc ở quy mô này, Empower đã hợp tác với AWS (Amazon Web Services) và Accenture để chuyển đổi quy trình đảm bảo chất lượng (QA) bằng cách sử dụng AI tạo sinh. Bằng cách triển khai một giải pháp tùy chỉnh với Amazon Connect và Amazon Bedrock, Empower có thể mở rộng phạm vi bao phủ cuộc gọi để đảm bảo chất lượng lên 20 lần, hiện phân tích hàng nghìn bản gỡ băng cuộc gọi hàng ngày và giảm thời gian xem xét QA từ vài ngày xuống còn vài phút.\nTrong bài viết này, chúng tôi khám phá cách thức sự hợp tác ba bên này đã mang lại một giải pháp AI tạo sinh sẵn sàng cho sản xuất từ [giai đoạn] thử nghiệm đến [giai đoạn] sản xuất chỉ trong 7 tháng. Điều này chứng minh sức mạnh của việc kết hợp công nghệ AWS, chuyên môn triển khai của Accenture, và tầm nhìn của Phòng thí nghiệm Đổi mới Công nghệ của Empower.\nThách thức: Đảm bảo chất lượng thủ công trên quy mô lớn\rTrung tâm liên hệ của Empower sử dụng một khuôn khổ đánh giá toàn diện gọi là GEDAC (Chào hỏi, Tương tác, Khám phá, Hành động, Kết thúc) để đánh giá hiệu suất của nhân viên dựa trên năm lĩnh vực kỹ năng chính. Mỗi lĩnh vực bao gồm nhiều kỹ năng phụ, từ chào hỏi khách hàng một cách thích hợp đến duy trì thái độ thân thiện và phản hồi kịp thời. Các chuyên viên phân tích chất lượng đã xem xét thủ công các bản ghi cuộc gọi và chấm điểm nhân viên dựa trên các tiêu chí định trước cho mỗi lĩnh vực kỹ năng. Quy trình thủ công này đặt ra một số thách thức. Với việc những người đánh giá (con người) chỉ có thể đánh giá một tập hợp con nhỏ trong số 10 triệu cuộc gọi hàng năm, phạm vi bao phủ vẫn còn hạn chế. Quy trình đánh giá thủ công cũng tạo cơ hội cho các đánh giá không nhất quán, vì những người đánh giá khác nhau có thể chấm điểm các tương tác giống hệt nhau một cách khác nhau. Hơn nữa, tính chất tốn thời gian của mỗi lần đánh giá đã hạn chế số lượng các bài xem xét có thể thực hiện. Điều này dẫn đến phản hồi chậm trễ, với việc các nhân viên nhận được đánh giá hiệu suất sau nhiều ngày hoặc thậm chí nhiều tuần kể từ khi tương tác với khách hàng. Khi khối lượng cuộc gọi tiếp tục tăng, những hạn chế về khả năng mở rộng này khiến cho việc bao phủ toàn diện ngày càng khó đạt được.\n“Chúng tôi nhận ra rằng để thực sự nâng cao trải nghiệm khách hàng trên quy mô lớn, chúng tôi cần phải tái định hình cơ bản phương pháp tiếp cận của mình đối với việc đảm bảo chất lượng,” Joe Mieras, Phó Giám đốc (VP) Dịch vụ Thành viên tại Empower, cho biết. “Quy trình thủ công đơn giản là không thể theo kịp tốc độ tăng trưởng và cam kết của chúng tôi về dịch vụ xuất sắc.”\nThách thức này hoàn toàn phù hợp với sứ mệnh của Phòng thí nghiệm Đổi mới Công nghệ của Empower là khơi dậy sự đổi mới bằng cách thử nghiệm các khả năng mới, tăng tốc các công nghệ mới và tạo sự khác biệt cho trải nghiệm khách hàng thông qua việc khám phá an toàn và minh bạch. Phòng thí nghiệm, vốn đã thu hút hơn 11.000 cộng sự thông qua các bản demo và roadshow trong khi nghiên cứu hơn 80 công nghệ mới nổi, đã xác định QA cuộc gọi là một ứng cử viên hàng đầu cho việc thử nghiệm AI tạo sinh.\nTổng quan giải pháp\rLàm việc với AWS và Accenture, Empower đã phát triển một giải pháp QA tự động tận dụng Amazon Connect Contact Lens, dịch vụ mà họ đã kích hoạt và đang cung cấp các bản gỡ băng chất lượng cao, đã loại bỏ PII. Bằng cách kết hợp các bản gỡ băng sẵn sàng sử dụng này với Amazon Bedrock và Claude 3.5 Sonnet của Anthropic để đánh giá thông minh, đội ngũ đã tránh được hàng tuần phát triển ETL (trích xuất, chuyển đổi và tải) và che giấu dữ liệu. Giải pháp xử lý 5.000 bản gỡ băng đã được loại bỏ (thông tin nhạy cảm) mỗi ngày theo lô, đánh giá nhân viên trên tất cả các hạng mục GEDAC.\nSơ đồ sau đây minh họa kiến trúc giải pháp cấp cao:\nSơ đồ kiến trúc cấp cao\r🧩 Quy trình làm việc bao gồm các bước sau:\rGỡ băng cuộc gọi Amazon Connect Contact Lens tự động gỡ băng các cuộc gọi của khách hàng với độ chính xác cao, loại bỏ PII và ghi lại sự phân tách người nói, cảm xúc, cùng các siêu dữ liệu khác.\nSau đó, nó lưu trữ các tệp gỡ băng vào Amazon S3, loại bỏ nhu cầu về các đường ống dữ liệu tùy chỉnh.\nThông báo sự kiện Amazon EventBridge phát hiện các tệp gỡ băng mới trong S3 và tự động kích hoạt các hành động tiếp theo.\nQuản lý hàng đợi EventBridge gửi một tin nhắn đến Amazon SQS, dịch vụ này sẽ quản lý hàng đợi các bản gỡ băng cần được xử lý, đảm bảo việc xử lý hàng loạt đáng tin cậy và có khả năng mở rộng.\nXử lý hàng loạt Các hàm AWS Lambda thăm dò hàng đợi SQS và truy xuất các lô gỡ băng để xử lý.\nĐiều phối quy trình GEDAC Hàm Lambda kích hoạt AWS Step Functions liên kết để đánh giá mỗi bản gỡ băng cuộc gọi dựa trên tất cả các chủ đề được định nghĩa trong khuôn khổ GEDAC.\nĐánh giá bằng AI Step Functions gửi các bản gỡ băng đến Amazon Bedrock, nơi Claude 3.5 Sonnet đánh giá hiệu suất của nhân viên dựa trên năm hạng mục GEDAC.\nĐưa kết quả vào hàng đợi Kết quả đánh giá được gửi đến một hàng đợi SQS khác để xử lý và gửi đi một cách có kiểm soát.\nXử lý kết quả Một hàm Lambda thứ hai xử lý các kết quả đánh giá từ hàng đợi.\nGửi (Phân phối) kết quả Hàm Lambda ghi kết quả đánh giá trở lại Amazon Connect bằng cách sử dụng API Đánh giá Nhân viên (Agent Evaluation API).\nCác nhà quản lý sau đó có thể xem kết quả trực tiếp trong giao diện Quản lý Chất lượng Amazon Connect hiện có — không yêu cầu phát triển GUI tùy chỉnh.\nTận dụng các dịch vụ AWS\rMột yếu tố then chốt trong việc phát triển và triển khai nhanh chóng của giải pháp là việc sử dụng các dịch vụ và tính năng AWS hiện có, thay vì xây dựng mọi thứ từ đầu.\nKhả năng kết hợp Amazon Connect cho trung tâm liên hệ đám mây của chúng tôi với các dịch vụ AWS khác, thay vì xây dựng từ đầu, là một yếu tố then chốt trong việc phát triển và triển khai nhanh chóng của giải pháp.\nAmazon Connect Contact Lens đã cung cấp sẵn [tính năng] loại bỏ tự động thông tin nhận dạng cá nhân (PII) trong các bản gỡ băng cuộc gọi. Điều này loại bỏ nhu cầu Empower phải triển khai các đường ống ETL tùy chỉnh và các giải pháp che giấu dữ liệu, giúp giảm đáng kể thời gian phát triển và đảm bảo tuân thủ các yêu cầu bảo vệ dữ liệu ngay từ ngày đầu tiên. Đội ngũ [phát triển] có thể tập trung vào việc xây dựng logic đánh giá khi mà các vấn đề về quyền riêng tư dữ liệu và tuân thủ đã được giải quyết sẵn.\nContact Lens tự động truyền các tệp gỡ băng cuộc gọi đến Amazon S3, kích hoạt quy trình xử lý tiếp theo. Sự tích hợp gốc này đã loại bỏ nhu cầu về các giải pháp di chuyển dữ liệu tùy chỉnh và cung cấp một nền tảng đáng tin cậy, có khả năng mở rộng cho đường ống xử lý hàng loạt.\nNgoài ra, các tính năng đánh giá hiệu suất nhân viên hiện có sẵn trong Amazon Connect đã cung cấp một giao diện người dùng có sẵn để hiển thị kết quả đánh giá. Giải pháp sử dụng API Đánh giá Nhân viên của Amazon Connect để ghi các kết quả đánh giá từ Bedrock trực tiếp vào Amazon Connect, nơi các nhà quản lý có thể xem chúng cùng với các chỉ số chất lượng khác trong một giao diện quen thuộc. “Chúng tôi không cần phải ‘phát minh lại bánh xe’,” Joseph Mieras, Phó Giám đốc (VP) Trải nghiệm Khách hàng tại Empower, giải thích. “Bằng cách sử dụng API Quản lý Chất lượng của Amazon Connect, chúng tôi có thể trình bày các đánh giá do AI tạo ra trên chính giao diện mà đội ngũ của chúng tôi vốn đã sử dụng, giúp cải thiện đáng kể việc tiếp nhận và giảm thiểu các yêu cầu đào tạo.”\nGiao diện Người dùng Đồ họa (GUI) Đánh giá Nhân viên của Amazon Connect\rTại sao chọn triển khai tùy chỉnh thay vì các tính năng có sẵn\rCác tính năng đánh giá hiệu suất nhân viên của Amazon Connect cung cấp chức năng có sẵn tuyệt vời cho nhiều tổ chức. Tuy nhiên, khuôn khổ GEDAC của Empower là kết quả của nhiều thập kỷ tinh chỉnh dành riêng cho hoạt động kinh doanh của họ. Sự linh hoạt của Amazon Connect trong việc tùy chỉnh giải pháp của họ bằng cách sử dụng Amazon Bedrock đã cho phép Empower triển khai các tiêu chí đánh giá chính xác của họ, đồng thời duy trì khả năng phát triển giải pháp khi nhu cầu của họ thay đổi.\nGiải pháp này cho phép Empower triển khai phương pháp luận GEDAC của họ với sự đánh giá có sắc thái trên nhiều kỹ năng phụ, nắm bắt được các tiêu chí cụ thể làm nên sự độc đáo cho khuôn khổ của họ. Nó cung cấp sự linh hoạt để điều chỉnh các câu lệnh và tiêu chí đánh giá mà không cần thay đổi hệ thống, cho phép việc tinh chỉnh liên tục dựa trên nhu cầu kinh doanh. Ngoài ra, nó còn cung cấp lý giải chi tiết cho mỗi điểm số, mang lại khả năng giải thích cần thiết để hỗ trợ việc huấn luyện nhân viên hiệu quả và cải thiện hiệu suất.\nCác cân nhắc về Bảo mật và AI có trách nhiệm Giải pháp áp dụng các biện pháp bảo mật mạnh mẽ để bảo vệ thông tin nhạy cảm trong suốt quá trình đánh giá. Tất cả dữ liệu đều được mã hóa cả khi đang truyền và khi lưu trữ, để bảo vệ khỏi sự truy cập trái phép. Các biện pháp kiểm soát truy cập dựa trên vai trò đảm bảo rằng chỉ những nhân sự được ủy quyền mới có thể xem kết quả đánh giá, duy trì việc quản trị dữ liệu nghiêm ngặt. Các chính sách tự động quản lý vòng đời dữ liệu theo các yêu cầu quy định, trong khi đó, việc ghi nhật ký kiểm toán toàn diện cung cấp một dấu vết hoàn chỉnh về mọi hoạt động của hệ thống cho mục đích tuân thủ và giám sát an ninh.\nEmpower đã triển khai một khuôn khổ quản trị AI mạnh mẽ nhằm giải quyết nhiều khía cạnh của AI có trách nhiệm. Một Ủy ban Quản trị AI cung cấp sự giám sát tập trung đối với mọi hoạt động sử dụng và phát triển AI, xem xét và đánh giá rủi ro trước khi thực thi. Công ty đã thiết lập các rào cản pháp lý và tuân thủ toàn diện cho việc phát triển và sử dụng mô hình AI, cùng với một quy trình giám sát mô hình nhằm duy trì một kho AI tập trung với quyền sở hữu minh bạch, sự giám sát liên tục, và chứng nhận hàng năm được chính thức hóa.\nPhương pháp tiếp cận AI có trách nhiệm của Empower\rPhương pháp tiếp cận đa diện này chủ động hoạt động để tránh thiên vị AI, ngăn chặn gian lận, bảo vệ khỏi lộ lọt dữ liệu, quản lý rủi ro pháp lý và quy định, và đảm bảo chất lượng dữ liệu huấn luyện. Khuôn khổ này cũng nhấn mạnh tính minh bạch, cho phép các nhân viên xem xét các đánh giá của AI và hiểu lý giải điểm số, đồng thời duy trì sự giám sát của con người, nơi các chuyên viên phân tích chất lượng có thể xem xét và ghi đè các đánh giá của AI. Việc giám sát liên tục đảm bảo sự đánh giá không ngừng về hiệu suất và tính công bằng của mô hình, tạo ra một sự triển khai AI bền vững và có đạo đức.\nSức mạnh của quan hệ đối tác AWS, Accenture và Empower\rViệc triển khai thành công giải pháp này nhấn mạnh giá trị của quan hệ đối tác chiến lược trong việc cung cấp các giải pháp AI doanh nghiệp. Amazon Connect, với tư cách là giải pháp trung tâm liên hệ của Empower, cung cấp các bản gỡ băng chất lượng cao thiết yếu cho việc đánh giá chính xác. Việc tích hợp Amazon Bedrock cung cấp quyền truy cập vào các mô hình nền tảng tiên tiến với bảo mật cấp doanh nghiệp. Đội ngũ đã đảm bảo thiết kế tối ưu cho quy mô và hiệu suất thông qua hướng dẫn về kiến trúc, và chia sẻ các bài học kinh nghiệm từ các lần triển khai tương tự trong ngành dịch vụ tài chính, mang lại các thông lệ tốt nhất có giá trị cho dự án.\nVai trò của Accenture với tư cách là đối tác triển khai là rất quan trọng trong việc chuyển hóa các yêu cầu của Empower thành một giải pháp sẵn sàng cho sản xuất. Đội ngũ đã phát triển các câu lệnh phức tạp nắm bắt chính xác các tiêu chí đánh giá GEDAC, tiến hành nhiều vòng kiểm thử và tối ưu hóa với các chuyên gia trong lĩnh vực để đảm bảo độ chính xác. Họ đảm bảo tích hợp liền mạch với hệ thống công nghệ hiện có của Empower, đồng thời hỗ trợ quá trình chuyển đổi của tổ chức sang việc đảm bảo chất lượng được tăng cường bằng AI thông qua quản lý thay đổi toàn diện. Đội ngũ đã làm việc chặt chẽ với các chuyên viên phân tích chất lượng của Empower để hiểu rõ các sắc thái trong tiêu chí đánh giá của họ, sau đó chuyển hóa chuyên môn đó thành các câu lệnh mà Claude 3.5 Sonnet có thể thực thi một cách nhất quán và chính xác. Quá trình hợp tác này đảm bảo mô hình AI có thể tái tạo độ sâu và tính đặc thù của các đánh giá của con người, trong khi vẫn duy trì sự nhất quán cần thiết cho các hoạt động ở quy mô lớn.\nSự đóng góp của Empower vượt xa vai trò là một khách hàng—họ là một đối tác tích cực trong thiết kế giải pháp. Họ cung cấp sự hiểu biết sâu sắc về hoạt động của trung tâm liên hệ và các yêu cầu về chất lượng, chia sẻ phương pháp luận GEDAC chi tiết và các tiêu chí chấm điểm, vốn là nền tảng của hệ thống đánh giá AI. Các chuyên viên phân tích chất lượng của họ đã đóng một vai trò quan trọng trong việc xác thực các đánh giá của AI và cung cấp phản hồi để cải tiến liên tục. Thêm vào đó, Empower định vị sáng kiến này trong chiến lược chuyển đổi AI tạo sinh rộng lớn hơn của họ, đảm bảo sự phù hợp với các mục tiêu dài hạn của tổ chức và tạo ra một kế hoạch chi tiết cho các lần triển khai AI trong tương lai trên toàn doanh nghiệp.\nKết quả và tác động kinh doanh\rVới giải pháp này, Empower đã chứng kiến sự gia tăng đáng kể gấp 20 lần về phạm vi bao phủ cuộc gọi QA, mở rộng từ việc chỉ xem xét một mẫu con sang khả năng chấm điểm tất cả các cuộc gọi. Thời gian đánh giá giảm từ vài ngày xuống còn vài phút, trong khi các đánh giá AI được chuẩn hóa đã loại bỏ sự thiếu nhất quán giữa những người đánh giá vốn trước đây là một thách thức về tính nhất quán. Amazon Connect xử lý các biến động về khối lượng hàng ngày mà không yêu cầu thêm tài nguyên, cung cấp khả năng mở rộng cần thiết cho các hoạt động đang phát triển của Empower. Thêm vào đó, các nhóm QA có thể ưu tiên các cuộc gọi cần nhiều phản hồi nhất, đảm bảo những người đánh giá (con người) tập trung nỗ lực vào nơi họ tạo ra nhiều giá trị nhất.\nViệc triển khai đã thúc đẩy tối ưu hóa chi phí đáng kể bằng cách tự động hóa các tác vụ thủ công lặp đi lặp lại, cho phép các chuyên viên phân tích QA chuyển hướng chuyên môn của họ sang các hoạt động có giá trị cao như xử lý các trường hợp phức tạp, phát triển các chương trình đào tạo và cung cấp huấn luyện nhân viên được cá nhân hóa. Thay vì dành hàng giờ cho các đánh giá thông thường, nhân sự QA giờ đây tập trung vào các cải tiến chiến lược giúp nâng cao trực tiếp trải nghiệm của khách hàng. Ngoài lợi ích về hiệu suất tức thì, giải pháp còn tạo ra một nền tảng có thể mở rộng, có thể tái sử dụng cho các trường hợp sử dụng khác trong toàn tổ chức.\nCác nhân viên nhận được thông tin chi tiết về hiệu suất trong vòng vài giờ thay vì vài tuần, làm thay đổi chu trình phản hồi. Các giải thích chi tiết đi kèm với mỗi đánh giá sẽ giúp các nhà quản lý cung cấp huấn luyện có mục tiêu dựa trên các ví dụ tương tác cụ thể. Khả năng phân tích cũng có thể tiết lộ các mẫu hình giữa các nhóm và loại cuộc gọi mà trước đây không thể thấy rõ, cho phép cải tiến dựa trên dữ liệu đối với quy trình và đào tạo. Việc cập nhật mô hình thường xuyên mang đến các thông lệ tốt nhất mới và một chu trình cải tiến liên tục.\n“Tác động của những giải pháp như thế này sẽ mang tính chuyển đổi,” Kyle Caffey, Phó Giám đốc (VP) của Innovation Lab tại Empower, cho biết. “Chúng ta không chỉ cải thiện đáng kể hiệu suất hoạt động của mình mà còn thúc đẩy chất lượng tốt hơn, điều này trực tiếp chuyển thành trải nghiệm khách hàng được cải thiện.”\nBài học kinh nghiệm và các thông lệ tốt nhất\rViệc triển khai đã nhấn mạnh một số bài học kinh nghiệm chính. Đầu tiên và quan trọng nhất, việc bắt đầu với các mục tiêu kinh doanh rõ ràng đã chứng tỏ là điều thiết yếu. Sự tập trung cụ thể của Empower vào việc tự động hóa các đánh giá GEDAC đã cung cấp các tiêu chí thành công có thể đo lường được, vốn đã định hướng cho mọi quyết định kỹ thuật. Chất lượng của đầu ra AI tương quan trực tiếp với chất lượng câu lệnh, khiến cho phương pháp tiếp cận lặp của Accenture đối với kỹ thuật tạo câu lệnh trở nên quan trọng để đạt được độ chính xác cần thiết cho việc triển khai sản xuất.\nLên kế hoạch cho quy mô ngay từ ngày đầu tiên là một yếu tố thành công quan trọng khác. Empower có thể xử lý khối lượng ngày càng tăng mà không cần thay đổi kiến trúc bằng cách triển khai sớm các khả năng suy luận liên khu vực và xử lý hàng loạt của Amazon Bedrock. Đội ngũ cũng nhận ra rằng AI tăng cường chứ không thay thế sự phán đoán của con người. Các chuyên viên phân tích chất lượng của Empower tiếp tục đóng một vai trò quan trọng trong việc xác thực các đánh giá của AI và xử lý các trườngChúng tôi nhận ra rằng để thực sự nâng cao trải nghiệm khách hàng trên quy mô lớn, chúng tôi cần phải tái định hình cơ bản phương pháp tiếp cận của mình đối với việc đảm bảo chất lượng,” Joe Mieras, Phó Giám đốc (VP) Dịch vụ Thành viên tại Empower, cho biết. “Quy trình thủ công đơn giản là không thể theo kịp tốc độ tăng trưởng và cam kết của chúng tôi về dịch vụ xuất sắc.” trường hợp ngoại lệ, đảm bảo hệ thống duy trì các tiêu chuẩn chất lượng cao.\nCuối cùng, việc triển khai đã củng cố tầm quan trọng của việc cải tiến liên tục. Việc xem xét thường xuyên các đánh giá của AI giúp tinh chỉnh các câu lệnh và cải thiện độ chính xác theo thời gian, tạo ra một vòng lặp phản hồi đảm bảo hệ thống phát triển song song với nhu cầu kinh doanh. Phương pháp tiếp cận lặp này, kết hợp với quan hệ đối tác mạnh mẽ và các mục tiêu rõ ràng, đã tạo ra một kế hoạch chi tiết cho việc triển khai AI doanh nghiệp thành công, vượt ra ngoài một trường hợp sử dụng đơn lẻ này.\nMở rộng tầm nhìn: Điều gì tiếp theo trên hành trình AI tạo sinh của Empower\rĐiều này chỉ là sự khởi đầu cho quá trình chuyển đổi AI tạo sinh của Empower. Dựa trên thành công này, Empower đang phát triển một nền tảng AI tạo sinh tập trung để dân chủ hóa các khả năng AI cho hơn 1.500 nhà phát triển của họ. Nền tảng này sẽ cung cấp quản trị tập trung để đảm bảo các biện pháp kiểm soát bảo mật, tuân thủ và AI có trách nhiệm nhất quán. Nó sẽ cung cấp một lớp trừu tượng để truy cập đơn giản hóa vào các mô hình và khả năng AI khác nhau, giám sát sử dụng toàn diện để theo dõi và quản lý chi phí, và chia sẻ các thông lệ tốt nhất với các thành phần có thể tái sử dụng nhằm tăng tốc độ phát triển giữa các nhóm.\nLớp Trừu tượng AI Tạo sinh của Empower\rEmpower đã xác định một số trường hợp sử dụng bổ sung để triển khai, trải rộng trên nhiều chức năng kinh doanh. Chúng bao gồm việc mở rộng đánh giá tự động sang các cuộc gọi tư vấn đầu tư và quản lý tài sản, đào tạo nhân viên bằng các trình mô phỏng hội thoại do AI cung cấp, tự động hóa việc phân tích tài liệu kế hoạch hưu trí, và nâng cao năng suất của nhà phát triển bằng các công cụ viết mã do AI cung cấp. Mỗi trường hợp sử dụng đều được xây dựng dựa trên nền tảng đã được thiết lập bởi giải pháp QA nhân viên, tận dụng các mẫu hình và quyết định kiến trúc đã được chứng minh.\nBằng cách kết hợp các dịch vụ AI mạnh mẽ có sẵn trong AWS, chuyên môn triển khai của Accenture và kiến thức chuyên môn về lĩnh vực của Empower, sự hợp tác này đã mang lại một giải pháp không chỉ đáp ứng các nhu cầu kinh doanh trước mắt mà còn đặt nền móng cho việc áp dụng AI rộng rãi hơn. Khi AI tạo sinh tiếp tục phát triển, những bài học này từ hành trình của Empower sẽ giúp các tổ chức khác định hướng cho quá trình chuyển đổi của riêng họ bằng cách bắt đầu với các mục tiêu rõ ràng, lựa chọn các đối tác phù hợp và duy trì sự tập trung không ngừng vào việc mang lại giá trị kinh doanh.",
    "description": "Empower đã mở rộng quy mô đảm bảo chất lượng trung tâm liên hệ như thế nào với Amazon Connect và Amazon Bedrock\rTác giả: Marcos Ortiz, Illan Geller (Accenture), Ozlem Celik-Tinmaz (Accenture), Prabhu Akula (Accenture), và Ryan Baham (Empower)\nNgày: 04 tháng 8 năm 2025\nThể loại: Amazon Bedrock, Amazon Connect, Amazon Transcribe, Trung tâm liên hệ, AI Tạo sinh ",
    "tags": [],
    "title": "Blog 10",
    "uri": "/en/3-translated_blogs/blog_10/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Worklog",
    "content": "Week 10 Objectives\rConduct internal UAT for frontend, collect feedback, fix UI issues, and refine user interface throughout the week Fix backend bugs mainly in Task Service, with additional fixes in User Service Participate in team meetings to discuss development workflow, architecture adjustments, and backend issues Update worklogs and maintain progress tracking Tasks to be carried out this week\rDay Task Start Date Completion Date Reference Material 1 - Team meeting to discuss about the development process 10/11/2025 10/11/2025 - Update worklog 2 - Code front end: Conduct internal UAT (User Acceptance Testing). Collect feedback and fix detected issues. Refine UI based on feedback. 11/11/2025 11/11/2025 - Code back end: Fix bugs in Task Service 3 - Code front end: Conduct internal UAT (User Acceptance Testing). Collect feedback and fix detected issues. Refine UI based on feedback. 12/11/2025 12/11/2025 - Code back end: Fix bugs in Task Service 4 - Teem meeting to discuss about the architecture, backend bugs 13/11/2025 13/11/2025 - Code front end: Conduct internal UAT (User Acceptance Testing). Collect feedback and fix detected issues. Refine UI based on feedback. - Code back end: Fix bugs in Task Service, User Service 5 - Update worklog 14/11/2025 14/11/2025 - Code front end: Conduct internal UAT (User Acceptance Testing). Collect feedback and fix detected issues. Refine UI based on feedback.",
    "description": "Week 10 Objectives\rConduct internal UAT for frontend, collect feedback, fix UI issues, and refine user interface throughout the week Fix backend bugs mainly in Task Service, with additional fixes in User Service Participate in team meetings to discuss development workflow, architecture adjustments, and backend issues Update worklogs and maintain progress tracking Tasks to be carried out this week\rDay Task Start Date Completion Date Reference Material 1 - Team meeting to discuss about the development process 10/11/2025 10/11/2025 - Update worklog 2 - Code front end: Conduct internal UAT (User Acceptance Testing). Collect feedback and fix detected issues. Refine UI based on feedback. 11/11/2025 11/11/2025 - Code back end: Fix bugs in Task Service 3 - Code front end: Conduct internal UAT (User Acceptance Testing). Collect feedback and fix detected issues. Refine UI based on feedback. 12/11/2025 12/11/2025 - Code back end: Fix bugs in Task Service 4 - Teem meeting to discuss about the architecture, backend bugs 13/11/2025 13/11/2025 - Code front end: Conduct internal UAT (User Acceptance Testing). Collect feedback and fix detected issues. Refine UI based on feedback. - Code back end: Fix bugs in Task Service, User Service 5 - Update worklog 14/11/2025 14/11/2025 - Code front end: Conduct internal UAT (User Acceptance Testing). Collect feedback and fix detected issues. Refine UI based on feedback.",
    "tags": [],
    "title": "Week 10 Worklog",
    "uri": "/en/1-worklog/1.10-week_10/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Translated Blogs",
    "content": "Làm thế nào để quản lý Bot AI bằng AWS WAF và tăng cường bảo mật\rTác giả: Kartik Bheemisetty, và David MacDonald\nNgày: 01 tháng 8 năm 2025\nThể loại: AWS WAF, Thông lệ tốt nhất, Bảo mật, Danh tính, \u0026 Tuân thủ, Hướng dẫn kỹ thuật, Lãnh đạo tư tưởng\nGiới thiệu\rTrình thu thập dữ liệu web (web crawler) đầu tiên được tạo ra vào năm 1993 để đo lường kích thước của web, và giờ đây chúng đã phát triển thành các bot hiện đại được hỗ trợ bởi AI tự hành. Internet ngày nay ngày càng có nhiều và bị thống trị bởi các bot AI tự động tương tác với các ứng dụng để hỗ trợ các tác vụ liên quan đến AI.\nChúng tôi phân loại bot AI thành ba loại:\nBot AI cào dữ liệu (AI scrapers), loại bot này thu thập dữ liệu một cách có hệ thống từ ứng dụng của bạn để huấn luyện các mô hình AI. Công cụ AI (AI tools), loại này hiển thị dữ liệu từ ứng dụng của bạn trong các ứng dụng AI bằng cách sử dụng tính năng gọi hàm (Function calling). Tác tử AI (AI agents), loại này có thể tự chủ điều hướng và tương tác động với ứng dụng của bạn để thực hiện các tác vụ phức tạp. Mặc dù một số bot AI cung cấp các dịch vụ có giá trị như tự động hóa các tác vụ tẻ nhạt, nhưng một số bot độc hại có thể gây ra những thách thức đáng kể cho chủ sở hữu và người vận hành ứng dụng web. Các bot độc hại có thể làm quá tải máy chủ với lưu lượng truy cập quá mức, dẫn đến suy giảm hiệu suất hoặc thậm chí ngừng hoạt động. Nếu không được kiểm soát, các bot này không chỉ làm tổn hại đến bảo mật mà còn có thể làm xói mòn lòng tin của người dùng và làm tổn hại danh tiếng thương hiệu.\nTrong bài viết này, chúng tôi khám phá các vấn đề khác nhau do bot AI gây ra và tìm hiểu các cơ chế khác nhau để phát hiện và quản lý bot AI bằng cách sử dụng Amazon Web Services (AWS) WAF.\nĐiều kiện tiên quyết\rBài viết này tập trung vào AWS WAF như là tuyến phòng thủ đầu tiên để quan sát và quản lý hoạt động của bot AI nhắm vào ứng dụng của bạn.\nNếu bạn chưa kích hoạt bảo vệ bằng AWS WAF, thì bạn có thể bắt đầu bằng cách trực quan hóa bối cảnh mối đe dọa bằng AWS Shield network security director. Nó giúp bạn xác định các tài nguyên không được bảo vệ bằng AWS WAF.\nSau đó, bạn có thể bắt đầu bằng cách tạo một tư thế bảo mật ban đầu bằng cách sử dụng các tích hợp bảo mật chỉ bằng một cú nhấp chuột. Nó tự động tạo ra một gói bảo vệ hoặc web ACL với các quy tắc để bảo vệ ứng dụng của bạn khỏi các mối đe dọa phổ biến nhất. Xem các tài liệu tham khảo sau:\nNếu bạn đang sử dụng Amazon CloudFront để lưu trữ ứng dụng của mình, thì hãy kích hoạt bảo vệ bằng tích hợp AWS WAF một cú nhấp chuột của CloudFront. Nếu bạn đang sử dụng Application Load Balancer (ALB) để lưu trữ ứng dụng của mình, thì hãy kích hoạt bảo vệ bằng tích hợp AWS WAF một cú nhấp chuột của ALB. Các vấn đề do bot AI gây ra\rBot không phải là một mối đe dọa mới trên web. Tuy nhiên, các yêu cầu về dữ liệu của Mô hình Ngôn ngữ Lớn (LLMs) và các mẫu tương tác mới được kích hoạt bởi tác tử AI đã khiến hành vi của bot trở nên rắc rối hơn trên nhiều ứng dụng.\nCác ứng dụng web có thể phải đối mặt với các vấn đề sau đây do bot AI gây ra:\nSử dụng dữ liệu độc quyền để huấn luyện mô hình: Việc sử dụng trái phép dữ liệu của tổ chức bạn có thể tạo ra các lo ngại về sở hữu trí tuệ khi chúng được sử dụng để huấn luyện các mô hình AI.\nVí dụ, nội dung của bạn có thể được sử dụng để tạo ra các dịch vụ có khả năng cạnh tranh mà không có sự đền bù và làm loãng giá trị thị trường độc nhất của nội dung.\nHiệu suất kém và chi phí cao: Các bot AI đang thu thập nội dung ứng dụng của bạn một cách ráo riết có thể tạo ra lưu lượng truy cập quá tải, dẫn đến suy giảm hiệu suất đối với người dùng hợp lệ.\nĐiều này cũng có thể phát sinh chi phí truyền dữ liệu ra ngoài (DTO), gây lãng phí tài nguyên tính toán và có khả năng gây ngừng dịch vụ trong các thời kỳ cào dữ liệu cao điểm.\nHành vi tự động/tự hành không mong muốn: Các bot AI có thể tự động tương tác với ứng dụng của bạn mà không cần sự tham gia của con người.\nĐiều này có thể lấy đi lưu lượng truy cập giá trị của con người khỏi ứng dụng của bạn vì AI có thể tóm tắt các phát hiện của nó.\nCác bot AI cũng có thể cạnh tranh với người dùng hợp lệ để hoàn thành các quy trình công việc có giá trị cao, nhạy cảm về thời gian như mua hàng tồn kho có hạn.\nCác bot này thường sử dụng các kỹ thuật sau để tương tác với ứng dụng của bạn:\nGọi hàm (Function calling) và tìm kiếm AI: Các ứng dụng AI sử dụng các công cụ để tìm kiếm và đưa ra các yêu cầu dữ liệu một lần trực tiếp từ ứng dụng của bạn.\nTương tác qua Khuôn khổ Tự động hóa Trình duyệt: Các tác tử AI như Amazon Nova Act sử dụng Playwright để điều khiển các trình duyệt thực.\nChúng có thể hoàn thành các tác vụ nhiều bước và tương tác với các ứng dụng theo cách giống như con người. Các tác tử này có thể thực thi JavaScript và xử lý các yếu tố UI phức tạp một cách hiệu quả.\nTương tác dựa trên VM: Các hệ thống như Computer Use của Anthropic hoạt động trong môi trường máy ảo (VM). Chúng tương tác với các ứng dụng theo cách giống con người hơn. Không giống như các trình duyệt tự động của Playwright, các hệ thống này sử dụng các cài đặt trình duyệt tiêu chuẩn, khiến cho hành vi của chúng gần như không thể phân biệt được với người dùng thực.\nXác định quy mô của hoạt động bot AI\rĐầu tiên, bạn cần hiểu bot AI ảnh hưởng đến ứng dụng của bạn như thế nào và ở quy mô nào.\nBắt đầu bằng cách thêm nhóm quy tắc AWS WAF Bot Control vào gói bảo vệ tài nguyên của bạn với Cấp độ Kiểm tra Chung (Common Inspection).\nSử dụng Chế độ Đếm (Count mode) ban đầu để giám sát các mẫu lưu lượng truy cập. Phương pháp tiếp cận này cho phép bạn phân tích hoạt động của bot trước khi thực hiện các thay đổi có thể ảnh hưởng đến lưu lượng truy cập sản xuất.\nNhóm quy tắc chung của Bot Control xác minh các bot tự nhận dạng thông qua xác thực chữ ký.\nNó bao gồm một quy tắc CategoryAI để phát hiện các bot AI đã được xác minh.\nHãy chắc chắn rằng bạn cấu hình nhóm quy tắc với phiên bản mới nhất, như được hiển thị trong Hình 1 sau đây.\nHình 1: Nhóm quy tắc AWS WAF Bot Control với mức kiểm tra “Common” và phiên bản 3.2\rSau khi chạy nhóm quy tắc được quản lý trong vài ngày, bạn có thể phân tích dữ liệu đã thu thập. Để xem các thông tin chi tiết, hãy mở bảng điều khiển AWS WAF và AWS Shield và chọn Khu vực AWS của bạn. Chọn gói bảo vệ của bạn và chọn xem bảng điều khiển. Điều hướng đến phần Tổng quan và chọn tùy chọn Bots để xem hoạt động, phát hiện, danh mục và tín hiệu của bot. Bảng điều khiển này cung cấp thông tin chi tiết về hoạt động của bot trên ứng dụng của bạn.\nHình 2 sau đây cho thấy một ví dụ về phần Các danh mục Bot. Nó hiển thị một khối lượng lớn các yêu cầu được đánh dấu là ai - AllowedRequests. Đây là các bot AI được quy tắc CategoryAI xác định, nhưng không bị chặn. Bạn cũng có thể nhận thấy các bot khác đang gửi khối lượng lớn yêu cầu.\nHình 2: Tổng quan lưu lượng truy cập cho quy tắc CategoryAI được AWS WAF phát hiện\rQuản lý các vấn đề do bot AI gây ra\rTrong các phần sau, chúng ta sẽ xem xét các phương pháp khác nhau để quản lý các vấn đề do bot AI gây ra.\nChặn bot AI sớm bằng robots.txt\rKịch bản 1: Chặn sớm các bot AI tuân thủ quy tắc\rMột tệp robots.txt giúp kiểm soát quyền truy cập của bot vào ứng dụng của bạn. Tệp văn bản đơn giản này được đặt tại thư mục gốc của ứng dụng (/robots.txt). Nó sử dụng một định dạng tiêu chuẩn để hướng dẫn các bot tuân thủ về những phần nào của ứng dụng mà chúng có thể và không thể truy cập. Mặc dù không phải tất cả các bot đều tuân theo các quy tắc này, nhưng các nhà điều hành bot có uy tín đều tôn trọng các tệp robots.txt được cấu hình đúng cách.\nCác dự án nguồn mở như ai.robots.txt cung cấp một tệp robots.txt với các trình thu thập dữ liệu (crawlers) liên quan đến AI mới nhất mà bạn có thể sử dụng để chặn các bot này trước khi chúng bắt đầu thu thập dữ liệu ứng dụng của bạn.\nNếu AWS WAF hiển thị khối lượng yêu cầu cao từ các bot cụ thể, thì bạn có thể sử dụng robots.txt để chặn các bot cào dữ liệu (scraping bots) quá mức nhưng tuân thủ quy tắc. Điều này giúp ngăn chúng ảnh hưởng đến DTO và hiệu suất ứng dụng của bạn.\nSau đây là một ví dụ cho phép Amazonbot thu thập dữ liệu các URL /public nhưng không thu thập dữ liệu các URL /private:\nUser-agent: Amazonbot Disallow: /private/ Allow: /public/\rKịch bản 2: Quản lý cách bot AI sử dụng dữ liệu của bạn\rCác bot từ các công ty công nghệ lớn phục vụ mục đích kép: chúng cào dữ liệu ứng dụng của bạn một lần và sử dụng dữ liệu đó để lập chỉ mục tìm kiếm và huấn luyện các mô hình AI. Bạn có thể cho phép các bot này thu thập dữ liệu ứng dụng của bạn để lập chỉ mục tìm kiếm, trong khi vẫn yêu cầu chúng không sử dụng dữ liệu này để huấn luyện các LLM.\nSau đây là ba ví dụ chỉ ra cách ngăn chặn các nhà điều hành bot lớn sử dụng dữ liệu của bạn để huấn luyện các LLM:\n1. Amazonbot: Nó sử dụng tiêu đề phản hồi HTTP X-Robots-Tag: noarchive để báo hiệu rằng bạn không muốn phản hồi này được sử dụng để huấn luyện các LLM.\nBạn có thể triển khai điều này bằng cách sử dụng chính sách tiêu đề phản hồi của CloudFront để thêm tiêu đề này vào mọi phản hồi từ ứng dụng của bạn.\nHTTP/1.1 200 OK Date: Tue, 15 Oct 2024 08:09:00 GMT X-Robots-Tag: noarchive\r2. Applebot: Bạn có thể yêu cầu Apple không sử dụng dữ liệu từ ứng dụng của bạn để huấn luyện các mô hình học máy (ML) của họ bằng cách thêm một mục User-agent Applebot-Extended vào tệp robots.txt của bạn.\nĐiều này vẫn cho phép Apple lập chỉ mục nội dung của bạn để tìm kiếm. Sau đây là một mục ví dụ để không cho phép Applebot-Extended (truy cập) trên toàn bộ ứng dụng của bạn:\nUser-agent: Applebot-Extended Disallow: /\rChỉ thị User-agent trong robots.txt phục vụ một mục đích cụ thể. Nó so khớp các mẫu với danh tính được khai báo của bot, vốn khác với tiêu đề HTTP User-Agent.\n3. Googlebot: Tương tự, Google cho phép bạn không cho phép huấn luyện các mô hình ML của Google bằng cách thêm User Agent Google-Extended vào tệp robots.txt của bạn:\nUser-agent: Google-Extended Disallow: /\rMột số nhà điều hành bot có thể không tôn trọng tệp robots.txt, vì vậy bạn phải cần quản lý chúng bằng AWS WAF.\nSử dụng AWS WAF\rKịch bản 3: Quản lý các bot AI đang gây ra hiệu suất kém và chi phí cao\rCác bot AI cào dữ liệu ráo riết từ ứng dụng của bạn có thể làm suy giảm hiệu suất ứng dụng và phát sinh chi phí DTO cũng như tính toán cao. Bạn có thể sử dụng các kỹ thuật sau đây bằng AWS WAF để bảo vệ ứng dụng của mình khỏi các bot không tôn trọng robots.txt:\nQuản lý các bot AI tự nhận dạng bằng nhóm quy tắc AWS WAF Bot Control với Cấp độ Kiểm tra Chung: Bạn có thể chặn các yêu cầu bot AI có khối lượng lớn bằng cách xóa hành động Ghi đè tất cả quy tắc mà trước đó bạn đã đặt thành Đếm trong nhóm quy tắc AWS WAF Bot Control với Cấp độ Kiểm tra Chung.\nQuy tắc CategoryAI giờ đây sẽ chặn các yêu cầu bot AI này theo mặc định. Ngoài các bot AI thuộc quy tắc CategoryAI, AWS WAF không chặn các bot phổ biến và có thể xác minh được.\nNếu bạn xác định một bot đã được xác minh, hoặc một danh mục bot, vẫn đang tạo ra khối lượng truy cập cao, thì bạn phải thêm một quy tắc một cách rõ ràng sau nhóm quy tắc AWS WAF Bot Control.\nQuy tắc này sẽ chặn bot cụ thể (hoặc nhóm bot được đại diện bởi một không gian tên nhãn), như được hiển thị trong hình 3 sau đây.\nHình 3: Quy tắc tùy chỉnh AWS WAF để chặn yandexbot bằng cách sử dụng nhãn (labels)\rLàm chậm các bot cào dữ liệu lẩn tránh: Các bot giả mạo tiêu đề HTTP user agent của chúng để giả vờ là các bot nổi tiếng, hoặc các máy khách hợp lệ của người dùng. Bạn có thể ngăn chặn các bot này làm quá tải ứng dụng của mình bằng cách sử dụng tính năng bảo vệ DDoS lớp ứng dụng (L7) nâng cao của AWS WAF và cả quy tắc dựa trên tỷ lệ của AWS WAF. Các quy tắc DDoS và quy tắc giới hạn tỷ lệ bảo vệ ứng dụng của bạn khỏi bất kỳ nguồn nào tạo ra khối lượng yêu cầu cao, bao gồm cả bot. Để tìm hiểu cách xác định ngưỡng quy tắc dựa trên tỷ lệ và các thông lệ tốt nhất về việc tạo quy tắc dựa trên tỷ lệ, hãy tham khảo bài viết \"Ba quy tắc dựa trên tỷ lệ quan trọng nhất của AWS WAF\".\nBắt các bot cào dữ liệu lẩn tránh phải làm việc: Hành động Thử thách (Challenge) của AWS WAF chạy một thử thách ngầm trong môi trường máy khách mà không yêu cầu người dùng tương tác và được thiết kế để không có tác động rõ rệt đến trải nghiệm của người dùng. Thử thách yêu cầu máy khách hoàn thành một tác vụ tốn kém về mặt tính toán (bằng chứng công việc). Phương pháp tiếp cận này nhằm cung cấp cho người dùng hợp lệ một cơ chế liền mạch để xác thực môi trường của họ, đồng thời tăng chi phí cho các nhà điều hành bot đang cố gắng tương tác với ứng dụng của bạn.\nHình 4 sau đây chỉ ra cách thêm một quy tắc tùy chỉnh sau nhóm quy tắc AWS WAF Bot Control. Quy tắc này yêu cầu người dùng phải hoàn thành một thử thách trước khi tiếp tục, trừ khi họ là các bot được phép/đã xác minh. Các bot đã xác minh được xác định bằng cách có một nhãn trong không gian tên awswaf:managed:aws:bot-control:bot.\nHình 4: Quy tắc AWS WAF buộc thực hiện thử thách đối với toàn bộ lưu lượng bot chưa được xác minh\r4. Sử dụng honeypot để bẫy các bot lẩn tránh\nGiải pháp Tự động hóa Bảo mật cho AWS WAF bao gồm một điểm cuối honeypot dụ các bot đang cào dữ liệu ứng dụng của bạn đến một điểm cuối mà không người dùng hợp lệ hoặc bot tuân thủ quy tắc nào truy cập.\nĐiểm cuối này giúp phát hiện và chặn các IP độc hại, hạn chế hiệu quả tác động của các bot cào dữ liệu ứng dụng của bạn.\n-\u003e Xem video sau để hiểu rõ hơn\nKịch bản 4: Quản lý các bot AI tự hành/tự động không mong muốn\nBạn có thể sử dụng các kỹ thuật sau để quản lý các bot AI tự hành:\nNhóm quy tắc AWS WAF Bot Control với Cấp độ Kiểm tra Chung: Quy tắc CategoryAI bao gồm các quy tắc cho các tác tử AI được xác định rõ ràng như Amazon Nova Act. Hơn nữa, các quy tắc SignalNonBrowserUserAgent và SignalAutomatedBrowser sẽ chặn các tác tử tự động hóa trình duyệt kiểu Playwright.\nNhóm quy tắc AWS WAF Bot Control với Cấp độ Kiểm tra Mục tiêu: Cấp độ kiểm tra này tạo ra một đường cơ sở thông minh về các mẫu lưu lượng truy cập. Cấp độ này sử dụng các kỹ thuật lấy dấu vân tay để giúp bảo vệ ứng dụng của bạn khỏi các bot tự hành bắt chước con người. Tham khảo bài viết phát hiện và chặn lưu lượng truy cập bot nâng cao để được hướng dẫn thiết lập tính năng này.\nHành động CAPTCHA của AWS WAF: Các LLM từ các nhà cung cấp lớn được huấn luyện để không giải CAPTCHA. Điều này sẽ ngăn chặn nhiều tác tử hoàn thành tương tác được yêu cầu. Tương tự như kỹ thuật Thử thách trước đó, bạn có thể cấu hình một quy tắc với hành động CAPTCHA để yêu cầu (hoàn thành) đối với một số yêu cầu nhất định. Tham khảo bài viết Sử dụng AWS WAF CAPTCHA để bảo vệ ứng dụng của bạn khỏi lưu lượng truy cập bot phổ biến để được hướng dẫn thiết lập tính năng này.\nXác thực (bao gồm cả sinh trắc học): Cuối cùng, các bot sẽ tiếp tục cải thiện và lẩn tránh các biện pháp giảm thiểu. Nếu bạn có yêu cầu cao về tương tác của con người, thì hãy cân nhắc sửdụng xác thực, bao gồm cả sinh trắc học, trước khi tiếp tục tương tác. Tham khảo bài viết Cách sử dụng AWS WAF Bot Control cho các tín hiệu bot được nhắm mục tiêu và giảm thiểu các bot lẩn tránh bằng trải nghiệm người dùng thích ứng để được hướng dẫn về cách thúc đẩy xác thực người dùng thích ứng khi các tương tác cho thấy có khả năng là lưu lượng truy cập bot.\nKết luận:\rCác bot AI tạo ra những thách thức đáng kể thông qua việc cào dữ liệu quá mức làm suy giảm hiệu suất và tăng chi phí, việc sử dụng nội dung trái phép để huấn luyện AI, và các tương tác tự động có thể từ gây phiền toái đến độc hại. Bằng cách triển khai các chiến lược đã được thảo luận trong bài viết này, bắt đầu từ các cấu hình robots.txt cơ bản đến các quy tắc AWS WAF Bot Control nâng cao, giới hạn tỷ lệ, và các thử thách CAPTCHA, bạn có thể bảo vệ khỏi việc cào dữ liệu trái phép, ngăn chặn suy giảm hiệu suất, và duy trì quyền kiểm soát đối với cách nội dung của bạn được các bot AI sử dụng.\nHơn nữa, để luôn được cập nhật về AWS WAF, hãy tham khảo Blog Bảo mật AWS WAF và Có gì mới với Bảo mật, Danh tính và Tuân thủ của AWS.\nCảm ơn bạn đã đọc bài viết này. Nếu bạn có phản hồi về bài viết này, hãy gửi bình luận trong phần bình luận. Nếu bạn có câu hỏi về bài viết này, hãy bắt đầu một chủ đề mới trên AWS WAF re:Post hoặc liên hệ Hỗ trợ của AWS.",
    "description": "Làm thế nào để quản lý Bot AI bằng AWS WAF và tăng cường bảo mật\rTác giả: Kartik Bheemisetty, và David MacDonald\nNgày: 01 tháng 8 năm 2025\nThể loại: AWS WAF, Thông lệ tốt nhất, Bảo mật, Danh tính, \u0026 Tuân thủ, Hướng dẫn kỹ thuật, Lãnh đạo tư tưởng\nGiới thiệu\rTrình thu thập dữ liệu web (web crawler) đầu tiên được tạo ra vào năm 1993 để đo lường kích thước của web, và giờ đây chúng đã phát triển thành các bot hiện đại được hỗ trợ bởi AI tự hành. Internet ngày nay ngày càng có nhiều và bị thống trị bởi các bot AI tự động tương tác với các ứng dụng để hỗ trợ các tác vụ liên quan đến AI.",
    "tags": [],
    "title": "Blog 11",
    "uri": "/en/3-translated_blogs/blog_11/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Worklog",
    "content": "Week 11 Objectives\rFind bugs and deploy Complete the solution architecture Tasks to be carried out this week\rDay Task Start Date Completion Date Reference Material 1 - Team meeting to discuss about the development process 17/11/2025 17/11/2025 - Adjust the solution architecture - Code back end: Fix bugs in User Service 2 - Code back end: Fix bugs in User Service 3 - Adjust solution architecture 19/11/2025 19/11/2025 - Team meeting to review existing flaws in Frontend and Backend 4 - Adjust solution architecture 20/11/2025 20/11/2025 5 - Team meeting 21/11/2025 21/11/2025 - Experimental deploy VPC, Subnets",
    "description": "Week 11 Objectives\rFind bugs and deploy Complete the solution architecture Tasks to be carried out this week\rDay Task Start Date Completion Date Reference Material 1 - Team meeting to discuss about the development process 17/11/2025 17/11/2025 - Adjust the solution architecture - Code back end: Fix bugs in User Service 2 - Code back end: Fix bugs in User Service 3 - Adjust solution architecture 19/11/2025 19/11/2025 - Team meeting to review existing flaws in Frontend and Backend 4 - Adjust solution architecture 20/11/2025 20/11/2025 5 - Team meeting 21/11/2025 21/11/2025 - Experimental deploy VPC, Subnets",
    "tags": [],
    "title": "Week 11 Worklog",
    "uri": "/en/1-worklog/1.11-week_11/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Translated Blogs",
    "content": "Hỗ trợ tùy chỉnh trên quy mô lớn: Biến một KB (Cơ sở kiến thức) Salesforce hợp nhất thành các tác tử AI tập trung vào LOB (Ngành kinh doanh)\rBởi: Bhaskar Rao, Saqib M, Dipkumar Mehta, và Murtuza Kainan\nNgày: 01 tháng 8 năm 2025\nThể loại: Nâng cao (300), Amazon AppFlow, Amazon Connect, Amazon Q, Tương tác khách hàng, Giải pháp khách hàng, Hướng dẫn kỹ thuật\nTrong thế giới siêu kết nối ngày nay, các nhóm hỗ trợ khách hàng phải xoay xở với một danh mục sản phẩm và dịch vụ ngày càng tăng, tất cả trong một CRM duy nhất như Salesforce. Các nhân viên cần truy cập ngay lập tức vào thông tin đúng, nhưng họ quá thường xuyên thấy mình phải lội qua một cơ sở kiến thức nguyên khối, hợp nhất mà bao trùm mọi ngành kinh doanh (LOB) – từ thanh toán viễn thông đến bồi thường bảo hiểm đến trả hàng bán lẻ. Những giây quý giá biến thành phút khi nhân viên nhấp chuột, cuộn và lọc, và những sự chậm trễ đó chuyển trực tiếp thành các cuộc gọi dài hơn, tỷ lệ giải quyết cuộc gọi đầu tiên thấp hơn, và khách hàng bực bội.\nThách thức kinh doanh \u0026 Giải pháp\rKhi Salesforce Knowledge đồng bộ vào Amazon Q in Connect, theo truyền thống, nó nhập tất cả các bài viết vào một kho lưu trữ hợp nhất. Mặc dù phương pháp tiếp cận hợp nhất này hoạt động tốt cho các nhóm nhỏ hơn hoặc các doanh nghiệp một ngành, các tổ chức quản lý nhiều ngành kinh doanh (LOB) phải đối mặt với những thách thức riêng biệt ở quy mô lớn.\nTrong những môi trường phức tạp này, bạn có thể tận dụng các tác tử AI trong Amazon Q in Connect để tự động phân đoạn Salesforce Knowledge của bạn thành nhiều cơ sở kiến thức cụ thể theo ngành kinh doanh (LOB). Mỗi LOB KB (Cơ sở kiến thức LOB) duy trì sự tích hợp Salesforce liền mạch trong khi tồn tại như một kho lưu trữ logic riêng của nó với các câu lệnh (prompts) AI cụ thể theo lĩnh vực.\nPhương pháp tiếp cận này mang lại các lợi ích chính bao gồm:\nKết quả tìm kiếm siêu liên quan Hiệu suất AI được tối ưu hóa Thời gian giải quyết nhanh hơn Điều này cho phép nhân viên tập trung chính xác vào điều quan trọng cho mỗi tương tác của khách hàng thay vì lội qua nội dung không liên quan.\nNhững gì bạn sẽ học được trong blog này\rBài viết này sẽ hướng dẫn bạn qua quy trình nâng cao cơ sở hạ tầng hỗ trợ khách hàng của bạn bằng cách sử dụng Amazon Q in Connect và Salesforce Knowledge.\nChúng ta sẽ bắt đầu bằng cách khám phá kiến trúc giải pháp, chỉ cho bạn cách tích hợp liền mạch Salesforce Knowledge với Amazon Q in Connect. Bạn sẽ học cách định nghĩa nhiều cơ sở kiến thức cụ thể theo LOB và liên kết chúng với các Tác tử AI tùy chỉnh và các câu lệnh AI, điều chỉnh việc quản lý kiến thức của bạn cho từng lĩnh vực riêng biệt của doanh nghiệp.\nSau đó, chúng ta sẽ đi sâu vào hướng dẫn triển khai từng bước để cấu hình trình kết nối (connector) Salesforce để đảm bảo luồng dữ liệu mượt mà. Chúng ta sẽ hướng dẫn bạn qua quy trình tự động hóa việc nhập dữ liệu và phân loại LOB, đảm bảo rằng kiến thức của bạn được tổ chức hiệu quả và chính xác.\nCuối cùng, chúng ta sẽ chỉ cho bạn cách tạo và triển khai các câu lệnh tác tử AI cho mỗi cơ sở kiến thức, tối ưu hóa khả năng truy xuất và trình bày thông tin liên quan cho các nhân viên hỗ trợ của bạn.\nĐến cuối bài blog này, bạn sẽ có một sự hiểu biết toàn diện về cách biến đổi hệ thống quản lý kiến thức của bạn thành một công cụ mạnh mẽ, được điều khiển bởi AI, giúp nâng cao hiệu suất của nhân viên và sự hài lòng của khách hàng trên tất cả các ngành kinh doanh của bạn.\nĐo lường thành công\rCác chỉ số chính:\nGiảm thời gian tìm kiếm của nhân viên Cải thiện tỷ lệ giải quyết cuộc gọi đầu tiên Sự hài lòng của khách hàng cao hơn Các bảng điều khiển và thông lệ tốt nhất về báo cáo Đến cuối bài viết này, bạn sẽ có một kế hoạch chi tiết rõ ràng để biến đổi việc nhập Salesforce Knowledge nguyên khối của bạn thành một khuôn khổ được phân đoạn, điều khiển bởi AI trong Amazon Q in Connect, giúp trao quyền cho các nhân viên của bạn giải quyết vấn đề nhanh hơn, duy trì luồng hội thoại, và nâng cao trải nghiệm khách hàng trên mọi ngành kinh doanh.\nTổng quan giải pháp\rSơ đồ kiến trúc tổng quan\rHướng dẫn từng bước\rA: Tích hợp Salesforce với AWS AppFlow\rDữ liệu Salesforce Knowledge được chuyển bằng AWS AppFlow.\nCác luồng riêng biệt xử lý các đơn vị kinh doanh (BU) khác nhau:\nBU Ô tô (auto_kb) BU Tín dụng (credit_kb) BU Thanh toán (payments_kb) AppFlow xử lý cả luồng theo yêu cầu (on-demand) và luồng gia tăng (incremental) với các bộ lọc thích hợp để chỉ lấy Kiến thức (Knowledge) liên quan đến BU cụ thể từ Salesforce, và dữ liệu được lưu trữ trong Amazon S3 với các tiền tố tương ứng.\nLuồng Theo yêu cầu (On Demand Flow): lấy tất cả KB liên quan cho một BU cụ thể Luồng Gia tăng (Incremental Flow): định kỳ thăm dò (poll) Salesforce xem có bất kỳ cập nhật nào đối với Kiến thức trong Salesforce cho BU đó hay không B: Phân tách dữ liệu trong Amazon S3\rDữ liệu từ Salesforce được lưu trữ trong một S3 bucket với các tiền tố cụ thể theo BU:\nauto_kb credit_kb payments_kb Thông báo sự kiện S3 (S3 Event Notifications) được cấu hình cho mỗi tiền tố, điều này sẽ kích hoạt giai đoạn tiếp theo.\nC: Thông báo sự kiện S3 đến Amazon SQS\rBất cứ khi nào dữ liệu mới được thêm vào S3 bucket, một Thông báo sự kiện S3 sẽ gửi một tin nhắn đến hàng đợi Amazon SQS.\nHàng đợi SQS đảm bảo việc tách rời (decoupling) và phân phối tin nhắn đáng tin cậy cho các bước xử lý sau (downstream).\nD: Xử lý bằng AWS Lambda\rTin nhắn SQS kích hoạt một hàm AWS Lambda, hàm này sẽ xử lý dữ liệu đến.\nLambda thực hiện làm sạch (sanitization) nội dung HTML trong mỗi bài viết kiến thức và ghi chúng dưới dạng các tệp HTML riêng biệt vào các S3 bucket cụ thể:\nKiến thức Ô tô (qic-auto) Kiến thức Tín dụng (qic-credit) Kiến thức Thanh toán (qic-payments) Lambda cũng được kích hoạt khi tìm nạp định kỳ từ Salesforce khi các bài viết kiến thức:\nĐược cập nhật và xuất bản – Lambda làm sạch nội dung HTML và ghi đè tệp trong S3 bucket cụ thể Được lưu trữ (Archived) – Lambda xóa tệp HTML khỏi S3 bucket tương ứng E: Các Cơ sở kiến thức đã cập nhật trong S3\rHàm Lambda xuất dữ liệu đã được làm sạch và phân loại vào các S3 bucket thích hợp:\nqic-auto qic-credit qic-payments F: Tích hợp Amazon Connect\rMỗi S3 bucket được liên kết với Cơ sở kiến thức Q in Connect (QiC KB):\nqic-auto-kb qic-credit-kb qic-payments-kb Các KB trong QiC có loại EXTERNAL, điều này có nghĩa là QiC tự động đồng bộ hóa với S3 bucket. Khi nội dung được thêm vào hoặc xóa khỏi S3 bucket, QiC sẽ tự động cập nhật KB của mình.\nCác cơ sở kiến thức hỗ trợ nhân viên trong các tương tác với khách hàng - Các Tác tử AI (AI Agents) cho cả Tìm kiếm thủ công và Đề xuất câu trả lời được tạo bằng các Câu lệnh AI (AI Prompts) tùy chỉnh và được liên kết với trợ lý cũng như các KB – sẽ được gọi (invoked) trong quá trình tương tác với khách hàng Quy trình Tương tác Khách hàng:\nKhách hàng liên hệ với doanh nghiệp thông qua Amazon Connect 2. Quyết định của Luồng IVR: Xác định loại cuộc gọi/truy vấn thông qua đầu vào IVR hoặc tra cứu của bên thứ 3: Ô tô Tín dụng Thanh toán Loại liên hệ được chuyển đến AWS Lambda để xử lý Lambda cập nhật Phiên làm việc của Nhân viên (Agent Session) bằng cách gọi API QiC updateSession() với cấu hình nhân viên dựa trên loại liên hệ: Kiến thức Ô tô Kiến thức Tín dụng Kiến thức Thanh toán G: Liên kết Trợ lý (Assistant Association)\rCơ sở kiến thức (KB) QIC thích hợp được liên kết với phiên làm việc bằng cách sử dụng liên kết trợ lý.\nH: Hỗ trợ Nhân viên\rCác Nhân viên Amazon Connect (Tác tử AI QIC) truy cập cơ sở kiến thức liên quan thông qua Trợ lý QIC:\nNhân viên bán hàng → Cơ sở kiến thức Ô tô Nhân viên hỗ trợ kỹ thuật → Cơ sở kiến thức Tín dụng Nhân viên nhân sự (HR) → Cơ sở kiến thức Thanh toán Các điều kiện tiên quyết\rMôi trường AWS\rTài khoản AWS đang hoạt động với quyền truy cập thích hợp AWS CLI đã được cài đặt và cấu hình AWS CDK CLI đã được cài đặt Quyền truy cập và các quyền hạn tại khu vực (region) mục tiêu Amazon Connect\rMột instance Amazon Connect đang hoạt động Hàng đợi (queue) Connect đã được cấu hình Quyền truy cập quản trị viên Connect Salesforce\rMột tổ chức (org) Salesforce có quyền truy cập API (Đã) triển khai cơ sở kiến thức Quyền truy cập Salesforce thích hợp Quyền thiết lập Ứng dụng được Kết nối (Connected App) – Salesforce Connected App AppFlow\rTrình kết nối (Connector) Salesforce cho AppFlow – AppFlow Salesforce Connector Môi trường phát triển\rPython 3.x Git Trình soạn thảo mã (Code editor) Kinh nghiệm phát triển AWS CDK Quyền truy cập bảo mật\rQuyền IAM thích hợp Quyền truy cập Salesforce thích hợp Quyền truy cập Dịch vụ Amazon AppFlow Cấu hình \u0026 triển khai\r1. Clone (Sao chép) dự án\rRepository URL: https://github.com/aws-samples/sample-sf-qic-multi-lob-intgr\n# Clone (Sao chép) kho lưu trữ git clone \u003crepository-url\u003e cd \u003cproject folder\u003e\r2. Cài đặt các gói phụ thuộc (Dependencies)\rpip install -r requirements.txt pip install -r requirements-dev.txt\r3. Cấu hình Salesforce AppFlow\r3.1 Tạo kết nối AppFlow\rĐể tạo kết nối AppFlow với Salesforce, hãy tham khảo các bước sau: Hướng dẫn kết nối AppFlow\nĐể biết thêm chi tiết về các tùy chọn cấu hình khi tạo kết nối, hãy tham khảo phần: Kết nối Amazon AppFlow với Salesforce trong tài liệu tham khảo bổ sung này: Trình kết nối Salesforce cho Amazon AppFlow 3.2 Các quyền bắt buộc cho Salesforce:** Với tư cách là quản trị viên Salesforce, hãy xem lại tài liệu Salesforce thích hợp cho các mục sau:\rQuyền đọc đối tượng Knowledge Quyền truy cập API đã được bật Phạm vi (scopes) OAuth đã được cấu hình 4. Cấu hình AWS\rCung cấp ID tài khoản AWS, khu vực (region) và tên cho môi trường. Với mục đích của blog này, chúng ta sẽ giả định “dev” là môi trường của mình. Do đó, trong tệp config.dev.json trong thư mục config/, hãy đảm bảo rằng các giá trị sau được cập nhật cho mỗi thuộc tính:\n{ \"account\": \"ID-Tài-khoản-AWS-của-bạn\", \"region\": \"Khu-vực-Mục-tiêu-của-bạn\", \"env_name\": \"Tên-Môi-trường\" //dev }\r5. Thiết lập Amazon Connect\rCung cấp ID instance Connect và ID hàng đợi (queue) của bạn. CDK sẽ triển khai một Luồng liên hệ (Contact Flow), và cần biết triển khai luồng này đến instance nào và bao gồm hàng đợi nào trong Luồng liên hệ.\n{ \"connect\": { \"instance_id\": \"ID-Instance-Connect-của-bạn\", \"queue_id\": \"ID-Hàng-đợi-Connect-của-bạn\" } }\r6. Ánh xạ Cơ sở Kiến thức (Knowledge Base)\rĐịnh nghĩa các Ngành kinh doanh (LOBs) của bạn\n\"LOBs\": [ \"LOB1\", \"LOB2\", \"LOB3\" ]\rCần có thông tin sau đây để chỉ định trường định danh duy nhất (unique identifier field) của Nội dung trong Salesforce cho mỗi LOB. Ví dụ: Trong Salesforce, bạn có thể có một trường như “ProgramId__c” hoặc “LOB__c” để phân biệt các cơ sở kiến thức.\n\"businessUnitFilters\": { \"YourLOB1\": { //e.g. Credit \"field\": \"Your-Classification-Field\", //e.g ProgramId__c \"value\": \"LOB1-Value\" //e.g. Credit Dept } // Add more LOBs as defined in the \"LOBs\" section above }\r7. Cấu hình AppFlow-Salesforce\rTừ Bước 1 ở trên, sao chép tên kết nối đã được tạo và cập nhật cấu hình sau:\n{ \"connection_name\": \"dev-sf-connection\", // Replace with your AppFlow connection name \"object_name\": \"Knowledge__kav\", }\r8. Các trường của Bài viết Kiến thức (Knowledge Article)\rPhần Trường Tùy chỉnh (Custom Field) trong đoạn mã dưới đây cần được cập nhật ở đây. Đây là các trường bổ sung mà bạn sẽ nhập từ Salesforce và cần được liệt kê trong cấu hình. KHÔNG THAY ĐỔI Các trường Bắt buộc của Hệ thống (System Required Fields)\n\"projections\": [ // System Required Fields {\"field\": \"Id\", \"data_type\": \"id\"}, {\"field\": \"LastModifiedDate\", \"data_type\": \"datetime\"}, {\"field\": \"ArticleNumber\", \"data_type\": \"string\"}, {\"field\": \"PublishStatus\", \"data_type\": \"picklist\"}, {\"field\": \"UrlName\", \"data_type\": \"string\"} // Custom Fields - Replace with your actual fields {\"field\": \"Your-Title-Field\", \"data_type\": \"string\"}, {\"field\": \"Your-Content-Field\", \"data_type\": \"textarea\"}, // Add additional fields as needed ]\r9. Bộ lọc Salesforce Knowledge\rĐể nguyên như sau. Nếu có các bộ lọc bổ sung bạn muốn chỉ định, hãy thêm chúng vào đây. Tài liệu tham khảo bổ sung:\nChange Data Capture Salesforce Cấu hình Salesforce AppFlow \"filters\": [ { \"field\": \"PublishStatus\", \"operator\": \"EQUAL_TO\", \"values\": [\"Online\", \"Archived\"] } // Add additional filters as needed ]\r10. Quy tắc Xác thực (Validation Rules) Salesforce Knowledge\rCập nhật thuộc tính field với trường Salesforce chứa nội dung của kiến thức. Trường này phải có trong danh sách Projections ở Bước 6. Cấu hình này là cần thiết để đảm bảo chúng ta không nhập nội dung trống vào QiC.\n\"validations\": [ { \"field\": \"Your-Content-Field\", // Replace with your content field. Must be in the list of Projections above \"operator\": \"VALIDATE_NON_NULL\", \"action\": \"DropRecord\" } // Add additional validations as needed ]\rLưu ý quan trọng:\nconnection_name phải khớp chính xác với tên kết nối Amazon AppFlow-Salesforce của bạn object_name phải là Knowledge__kav Đảm bảo kết nối AppFlow có các quyền thích hợp để truy cập đối tượng được chỉ định Kiểm tra kết nối và quyền truy cập đối tượng trước khi triển khai Ghi lại bất kỳ tên đối tượng tùy chỉnh hoặc tên kết nối nào đã sử dụng Giữ cấu hình nhất quán giữa các môi trường Xác thực quyền đối tượng và khả năng truy cập trường 11. Bootstrap Môi trường CDK\rNếu đây là lần đầu tiên bạn sử dụng CDK trong tài khoản/khu vực này:\ncdk bootstrap aws://ACCOUNT-NUMBER/REGION\r12. Xem lại (Review) CDK Diff\rcdk diff\r13. Triển khai Stack\rcdk deploy\rCác tài nguyên được tạo khi triển khai:\nLuồng (flow) Amazon AppFlow để tích hợp Salesforce Các hàm Lambda để xử lý dữ liệu Các vai trò (roles) và chính sách (policies) IAM Các cơ sở kiến thức (knowledge bases) Amazon Q Các thành phần tích hợp Amazon Connect Xác minh việc triển khai\rSau khi việc triển khai hoàn tất:\nKiểm tra bảng điều khiển (console) AWS CloudFormation để xem trạng thái của stack Xác minh việc tạo luồng (flow) AppFlow Xác nhận việc triển khai các hàm Lambda Kiểm tra việc thiết lập các cơ sở kiến thức Amazon Q Xác thực (Validate) việc tích hợp Amazon Connect Các bước sau triển khai\rLiên kết các Cơ sở Kiến thức Q in Connect với Instance Connect của bạn bằng các lệnh CLI sau Đầu tiên, liệt kê các Cơ sở Kiến thức của bạn aws qconnect list-knowledge-bases --region \u003cKHU_VỰC_AWS_CỦA_BẠN\u003e\rTiếp theo, đối với mỗi Cơ sở Kiến thức, hãy tạo một Liên kết Tích hợp Connect (Connect Integration Association). Lưu ý rằng Cơ sở Kiến thức đầu tiên do CloudFormation tạo ra có thể đã được liên kết và AWS CLI có thể báo lỗi do điều này. Bạn có thể đơn giản bỏ qua các bước này cho bất kỳ Cơ sở Kiến Kến thức nào như vậy aws connect create-integration-association --region \u003cKHU_VỰC_AWS_CỦA_BẠN\u003e \\ --instance-id \u003cID_INSTANCE_CONNECT_CỦA_BẠN\u003e \\ --integration-arn \u003cARN_CƠ_SỞ_KIẾN_THỨC_CỦA_BẠN\u003e \\ --integration-type WISDOM_KNOWLEDGE_BASE\rCuối cùng, gắn thẻ (tag) cho mỗi Cơ sở Kiến thức là AmazonConnectEnabled=True aws qconnect tag-resource --region \u003cKHU_VỰC_AWS_CỦA_BẠN\u003e \\ --resource-arn \u003cARN_CƠ_SỞ_KIẾN_THỨC_CỦA_BẠN\u003e \\ --tags AmazonConnectEnabled=True\rTrong Bảng điều khiển (Console) Amazon Connect, điều hướng đến Amazon Q, và nhấp vào Thêm Miền (Add Domain): Trên trang Thêm Miền (Add Domain), Chọn Sử dụng một miền hiện có (Use an existing domain), và chọn miền đã được tạo bởi quá trình triển khai từ danh sách thả xuống và nhấp vào Thêm Miền (Add Domain): Sau khi Miền (Domain) được thêm vào, bạn sẽ thấy một trang hiển thị Miền và các Cơ sở Kiến thức (Knowledge bases) được Liên kết như hình bên dưới: Chạy các Luồng AppFlow (Run the AppFlows) Đăng nhập vào Bảng điều khiển Quản lý AWS (AWS Management Console) và mở bảng điều khiển Amazon AppFlow tại https://console.aws.amazon.com/appflow/ Trong khung điều hướng (navigation pane) bên trái, chọn Luồng (Flows). Bảng điều khiển sẽ hiển thị trang Luồng (Flows). Trang này chứa một bảng tóm tắt các luồng đã được tạo. Để khởi tạo một luồng, bạn kích hoạt (activate) hoặc chạy (run) nó. Chúng ta có 2 loại luồng được tạo: Theo yêu cầu (OnDemand) và Theo lịch (Scheduled) Đối với mỗi LOB, Chọn luồng Theo yêu cầu (OnDemand) và chọn Xem Chi tiết (View Details). Chọn Chạy luồng (Run flow) để chạy luồng. Đối với mỗi LOB, Chọn luồng Theo lịch (Scheduled) và chọn Xem Chi tiết (View Details) Chọn Kích hoạt (Activate) để kích hoạt luồng. Xem xét và cập nhật luồng liên hệ (contact flow) Amazon Connect Đăng nhập vào instance Amazon Connect của bạn. Dưới mục Định tuyến (Routing), chọn Luồng liên hệ (Contact Flows). Chọn luồng có tên: qic-sf-contact-flow Điều hướng đến Khối (Block) Lấy thông tin đầu vào của khách hàng (Get customer input). Cập nhật Lời nhắc (Prompts) để bao gồm các BU hoặc LOB của bạn. Cập nhật khối Đặt thuộc tính liên hệ (Set contact attributes) cho mỗi tùy chọn. Thuộc tính LOB là bắt buộc, và giá trị phải giống hệt như các giá trị đã cung cấp trong cấu hình CDK tại thời điểm triển khai. - Nhấp vào **Lưu** (Save) để lưu luồng.\r- Nhấp vào **Xuất bản** (Publish) để xuất bản luồng.\rXác minh trạng thái luồng AppFlow Kích hoạt Luồng Theo yêu cầu (OnDemand Flow) trước tiên – để truy xuất nội dung kiến thức hiện có từ Salesforce Bắt đầu Luồng Theo lịch (Scheduled Flow) – để định kỳ thăm dò (poll) Salesforce Knowledge nhằm nhập bất kỳ nội dung bổ sung/cập nhật nào cho Salesforce Knowledge. Xác minh các S3 bucket mục tiêu để kiểm tra việc đồng bộ hóa dữ liệu Salesforce Giám sát (Monitor) CloudWatch logs Mẹo khắc phục sự cố\rCác sự cố thường gặp và giải pháp\rSự cố kết nối AppFlow - Xác minh connection_name trong tệp cấu hình (config).\nKiểm tra thông tin đăng nhập (credentials) Salesforce. Xác thực (Validate) token OAuth. Lỗi quyền (Permission Errors) - Xem lại các vai trò (roles) IAM.\nXác minh quyền truy cập API Salesforce. Sự cố đồng bộ hóa cơ sở kiến thức (Knowledge Base Sync Issues) - Xác thực cấu hình object_name.\nKiểm tra các ánh xạ trường (field mappings). Dọn dẹp\rĐể tránh phát sinh chi phí trong tương lai, hãy xóa các tài nguyên bằng các bước sau:\nĐảm bảo hủy ánh xạ (unmap) Luồng liên hệ (Contact Flow) do quá trình triển khai tạo ra khỏi số điện thoại. Trong terminal của bạn, đảm bảo bạn đang ở trong thư mục dự án. Chạy lệnh: cdk destroy Kết luận\rTóm lại, việc tích hợp Salesforce Knowledge với Amazon Q in Connect giúp tăng cường hỗ trợ nhân viên và giải quyết một số thách thức cốt lõi mà các trung tâm liên hệ phải đối mặt. Sự tích hợp này trao quyền cho các tổ chức nâng cao hoạt động của họ thông qua việc triển khai các tác tử AI cụ thể theo LOB (ngành kinh doanh), đảm bảo sự hỗ trợ theo thời gian thực, theo ngữ cảnh được thiết kế riêng cho các nhu cầu riêng biệt của các đơn vị kinh doanh khác nhau.",
    "description": "Hỗ trợ tùy chỉnh trên quy mô lớn: Biến một KB (Cơ sở kiến thức) Salesforce hợp nhất thành các tác tử AI tập trung vào LOB (Ngành kinh doanh)\rBởi: Bhaskar Rao, Saqib M, Dipkumar Mehta, và Murtuza Kainan\nNgày: 01 tháng 8 năm 2025\nThể loại: Nâng cao (300), Amazon AppFlow, Amazon Connect, Amazon Q, Tương tác khách hàng, Giải pháp khách hàng, Hướng dẫn kỹ thuật",
    "tags": [],
    "title": "Blog 12",
    "uri": "/en/3-translated_blogs/blog_12/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Worklog",
    "content": "Week 12 Objectives\rSummarize and resolve remaining issues in the system Conduct experimental deployment of S3 in a Single Region Practice and get familiar with Amazon RDS MySQL and ElastiCache for Redis Configure Google OAuth for the Authentication Service Adjust the solution architecture to be suitable for a Single Region (considering cost limitations for Free accounts) Successfully deploy frontend resources to S3 Complete the worklog and update the workshop Tasks to be carried out this week\rDay Task Start Date Completion Date Reference Material 1 - Team meeting to to review issues needed to be solve 24/11/2025 24/11/2025 - Experimental S3 single region deployment 2 - Complete part 4. Events participated of Workshop 25/11/2025 25/11/2025 - Experimental S3 single region deployment 3 - Experimental VPC deployment 26/11/2025 26/11/2025 - Practice using RDS MySQL, ElastiCacheForRedis 4 - Config Google OAuth 27/11/2025 27/11/2025 - Practice using RDS MySQL, ElastiCacheForRedis 5 - Adjust solution architecture to single-region (as for Free account and cost limitations) 28/11/2025 28/11/2025 - Deploy frontend resources to S3 successfully - Update worklog, workshop",
    "description": "Week 12 Objectives\rSummarize and resolve remaining issues in the system Conduct experimental deployment of S3 in a Single Region Practice and get familiar with Amazon RDS MySQL and ElastiCache for Redis Configure Google OAuth for the Authentication Service Adjust the solution architecture to be suitable for a Single Region (considering cost limitations for Free accounts) Successfully deploy frontend resources to S3 Complete the worklog and update the workshop Tasks to be carried out this week\rDay Task Start Date Completion Date Reference Material 1 - Team meeting to to review issues needed to be solve 24/11/2025 24/11/2025 - Experimental S3 single region deployment 2 - Complete part 4. Events participated of Workshop 25/11/2025 25/11/2025 - Experimental S3 single region deployment 3 - Experimental VPC deployment 26/11/2025 26/11/2025 - Practice using RDS MySQL, ElastiCacheForRedis 4 - Config Google OAuth 27/11/2025 27/11/2025 - Practice using RDS MySQL, ElastiCacheForRedis 5 - Adjust solution architecture to single-region (as for Free account and cost limitations) 28/11/2025 28/11/2025 - Deploy frontend resources to S3 successfully - Update worklog, workshop",
    "tags": [],
    "title": "Week 12 Worklog",
    "uri": "/en/1-worklog/1.12-week_12/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Worklog",
    "content": "Week 13 Objectives\rReview and resolve remaining setup issues from previous deployments Set up VPC, Subnets, Internet Gateway, and Security Groups Configure Application Load Balancer (ALB) Deploy backend and frontend images to ECR and ECS Fargate Set up Amazon RDS MySQL Configure Google OAuth redirect for Authentication Service Organize and update worklog categories based on team members Tasks to be carried out this week\rDay Task Start Date Completion Date Reference Material 1 - Team meeting to to review issues needed to be solve 01/11/2025 01/11/2025 - Setup VPC, Subnets, Internet Gateway, Security groups - Update worklog: create worklog categories base on team members 2 - Setup VPC, Subnets, Internet Gateway 02/11/2025 02/11/2025 - Setup ALB, Security groups, RDS MySQL 3 - Setup VPC, Subnets, Internet Gateway, Security groups 03/11/2025 03/11/2025 - Deploy images to ECR, ECS Fargate - Setup ALB, Security groups, RDS MySQL 4 - Deploy images to ECR, ECS Fargate, ALB, RDS 04/11/2025 04/11/2025 - Setup ALB, Security groups, RDS MySQL 5 - Deploy images to ECR, ECS Fargate, ALB, RDS 05/11/2025 05/11/2025 - Setup ALB, Security groups, RDS MySQL - Config Google OAuth Redirect",
    "description": "Week 13 Objectives\rReview and resolve remaining setup issues from previous deployments Set up VPC, Subnets, Internet Gateway, and Security Groups Configure Application Load Balancer (ALB) Deploy backend and frontend images to ECR and ECS Fargate Set up Amazon RDS MySQL Configure Google OAuth redirect for Authentication Service Organize and update worklog categories based on team members Tasks to be carried out this week\rDay Task Start Date Completion Date Reference Material 1 - Team meeting to to review issues needed to be solve 01/11/2025 01/11/2025 - Setup VPC, Subnets, Internet Gateway, Security groups - Update worklog: create worklog categories base on team members 2 - Setup VPC, Subnets, Internet Gateway 02/11/2025 02/11/2025 - Setup ALB, Security groups, RDS MySQL 3 - Setup VPC, Subnets, Internet Gateway, Security groups 03/11/2025 03/11/2025 - Deploy images to ECR, ECS Fargate - Setup ALB, Security groups, RDS MySQL 4 - Deploy images to ECR, ECS Fargate, ALB, RDS 04/11/2025 04/11/2025 - Setup ALB, Security groups, RDS MySQL 5 - Deploy images to ECR, ECS Fargate, ALB, RDS 05/11/2025 05/11/2025 - Setup ALB, Security groups, RDS MySQL - Config Google OAuth Redirect",
    "tags": [],
    "title": "Week 13 Worklog",
    "uri": "/en/1-worklog/1.13-week_13/index.html"
  },
  {
    "breadcrumb": "Internship Report \u003e Worklog",
    "content": "Week 14 Objectives\rComplete deployment Complete workshop Run test Tasks to be carried out this week\rDay Task Start Date Completion Date Reference Material 1 - Complete deploying ECS Fargate, run test 08/11/2025 08/11/2025 - Complete worklog \u0026 Workshop 2 09/11/2025 09/11/2025 3 10/11/2025 10/11/2025 4 11/11/2025 11/11/2025 5 12/11/2025 12/11/2025",
    "description": "Week 14 Objectives\rComplete deployment Complete workshop Run test Tasks to be carried out this week\rDay Task Start Date Completion Date Reference Material 1 - Complete deploying ECS Fargate, run test 08/11/2025 08/11/2025 - Complete worklog \u0026 Workshop 2 09/11/2025 09/11/2025 3 10/11/2025 10/11/2025 4 11/11/2025 11/11/2025 5 12/11/2025 12/11/2025",
    "tags": [],
    "title": "Week 14 Worklog",
    "uri": "/en/1-worklog/1.14-week_14/index.html"
  },
  {
    "breadcrumb": "Internship Report",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/en/categories/index.html"
  },
  {
    "breadcrumb": "Internship Report",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/en/tags/index.html"
  }
]
